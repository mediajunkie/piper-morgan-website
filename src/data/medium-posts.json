[
  {
    "title": "When Good Decisions Disappear: The Hidden Cost of Chat-Based Development",
    "excerpt": "",
    "url": "/blog/when-good-decisions-disappear-the-hidden-cost-of-chat-based-development",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4148a6ebdab1",
    "featuredImage": "/assets/blog-images/4148a6ebdab1-featured.webp",
    "slug": "when-good-decisions-disappear-the-hidden-cost-of-chat-based-development",
    "category": "insight",
    "workDate": "Aug 5, 2025",
    "workDateISO": "2025-08-05T00:00:00.000Z",
    "cluster": "reflection-evolution",
    "chatDate": "8/3/2025",
    "featured": false
  },
  {
    "title": "The Foundations Were (Indeed) Already There",
    "excerpt": "",
    "url": "/blog/the-foundations-were-indeed-already-there",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/7701c04a1497",
    "featuredImage": "/assets/blog-images/7701c04a1497-featured.png",
    "slug": "the-foundations-were-indeed-already-there",
    "category": "building",
    "workDate": "Sep 26, 2025",
    "workDateISO": "2025-09-26T00:00:00.000Z",
    "cluster": "discipline-completion",
    "chatDate": "9/21/2025",
    "featured": false
  },
  {
    "title": "Building the Cathedral: When AI Agents Need the Big Picture",
    "excerpt": "",
    "url": "/blog/building-the-cathedral-when-ai-agents-need-the-big-picture",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/50b9dfb0b2af",
    "featuredImage": "/assets/blog-images/50b9dfb0b2af-featured.png",
    "slug": "building-the-cathedral-when-ai-agents-need-the-big-picture",
    "category": "building",
    "workDate": "Sep 27, 2025",
    "workDateISO": "2025-09-27T00:00:00.000Z",
    "cluster": "discipline-completion",
    "chatDate": "9/21/2025",
    "featured": false
  },
  {
    "title": "The Quiet Satisfaction of the Successful Inchworm",
    "excerpt": "",
    "url": "/blog/the-quiet-satisfaction-of-the-successful-inchworm",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/433429cb8a5a",
    "featuredImage": "/assets/blog-images/433429cb8a5a-featured.png",
    "slug": "the-quiet-satisfaction-of-the-successful-inchworm",
    "category": "building",
    "workDate": "Sep 25, 2025",
    "workDateISO": "2025-09-25T00:00:00.000Z",
    "cluster": "discipline-completion",
    "chatDate": "9/21/2025",
    "featured": false
  },
  {
    "title": "Doing the Deep Work (listed as When Discipline Actually Works)",
    "excerpt": "",
    "url": "/blog/doing-the-deep-work-listed-as-when-discipline-actually-works",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/704e26cccf03",
    "featuredImage": "/assets/blog-images/704e26cccf03-featured.png",
    "slug": "doing-the-deep-work-listed-as-when-discipline-actually-works",
    "category": "building",
    "workDate": "Sep 24, 2025",
    "workDateISO": "2025-09-24T00:00:00.000Z",
    "cluster": "discipline-completion",
    "chatDate": "9/21/2025",
    "featured": false
  },
  {
    "title": "The Discipline of Actually Finishing",
    "excerpt": "",
    "url": "/blog/the-discipline-of-actually-finishing",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/44e1dc125be4",
    "featuredImage": "/assets/blog-images/44e1dc125be4-featured.webp",
    "slug": "the-discipline-of-actually-finishing",
    "category": "building",
    "workDate": "Sep 23, 2025",
    "workDateISO": "2025-09-23T00:00:00.000Z",
    "cluster": "discipline-completion",
    "chatDate": "9/21/2025",
    "featured": false
  },
  {
    "title": "Teaching Machines to Teach Machines",
    "excerpt": "",
    "url": "/blog/teaching-machines-to-teach-machines",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/a786faceb01a",
    "featuredImage": "/assets/blog-images/a786faceb01a-featured.png",
    "slug": "teaching-machines-to-teach-machines",
    "category": "building",
    "workDate": "Sep 21, 2025",
    "workDateISO": "2025-09-21T00:00:00.000Z",
    "cluster": "discipline-completion",
    "chatDate": "9/21/2025",
    "featured": false
  },
  {
    "title": "The 24-hour test",
    "excerpt": "",
    "url": "/blog/the-24-hour-test",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/698b8a61909a",
    "featuredImage": "/assets/blog-images/698b8a61909a-featured.png",
    "slug": "the-24-hour-test",
    "category": "building",
    "workDate": "Sep 22, 2025",
    "workDateISO": "2025-09-22T00:00:00.000Z",
    "cluster": "discipline-completion",
    "chatDate": "9/21/2025",
    "featured": false
  },
  {
    "title": "Whipping AI Chaos Toward Quality with the Excellence Flywheel",
    "excerpt": "",
    "url": "/blog/whipping-ai-chaos-toward-quality-with-the-excellence-flywheel",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f14232150d04",
    "featuredImage": "/assets/blog-images/f14232150d04-featured.webp",
    "slug": "whipping-ai-chaos-toward-quality-with-the-excellence-flywheel",
    "category": "insight",
    "workDate": "Jul 23, 2025",
    "workDateISO": "2025-07-23T00:00:00.000Z",
    "cluster": "discipline-completion",
    "chatDate": "7/22/2025",
    "featured": false
  },
  {
    "title": "The Three Questions Every AI Builder Should Ask",
    "excerpt": "",
    "url": "/blog/the-three-questions-every-ai-builder-should-ask",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/ee6fae671129",
    "featuredImage": "/assets/blog-images/ee6fae671129-featured.webp",
    "slug": "the-three-questions-every-ai-builder-should-ask",
    "category": "insight",
    "workDate": "Jul 22, 2025",
    "workDateISO": "2025-07-22T00:00:00.000Z",
    "cluster": "discipline-completion",
    "chatDate": "7/22/2025",
    "featured": false
  },
  {
    "title": "The Great Refactor: From Impossible to Inevitable",
    "excerpt": "",
    "url": "/blog/the-great-refactor-from-impossible-to-inevitable",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/fef75c085cc7",
    "featuredImage": "/assets/blog-images/fef75c085cc7-featured.png",
    "slug": "the-great-refactor-from-impossible-to-inevitable",
    "category": "building",
    "workDate": "Sep 19, 2025",
    "workDateISO": "2025-09-19T00:00:00.000Z",
    "cluster": "discipline-completion",
    "chatDate": "9/20/2025",
    "featured": false
  },
  {
    "title": "The Discipline of Boring: Why Saturday's Foundation Work Matters More Than Monday's Features",
    "excerpt": "",
    "url": "/blog/the-discipline-of-boring-why-saturdays-foundation-work-matters-more-than-mondays-features",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/b590180b511c",
    "featuredImage": "/assets/blog-images/b590180b511c-featured.png",
    "slug": "the-discipline-of-boring-why-saturdays-foundation-work-matters-more-than-mondays-features",
    "category": "building",
    "workDate": "Sep 20, 2025",
    "workDateISO": "2025-09-20T00:00:00.000Z",
    "cluster": "discipline-completion",
    "chatDate": "9/20/2025",
    "featured": false
  },
  {
    "title": "When Good Process Meets Bad Architecture: The Layer 4 Investigation",
    "excerpt": "",
    "url": "/blog/when-good-process-meets-bad-architecture-the-layer-4-investigation",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f3a6145f8f71",
    "featuredImage": "/assets/blog-images/f3a6145f8f71-featured.webp",
    "slug": "when-good-process-meets-bad-architecture-the-layer-4-investigation",
    "category": "building",
    "workDate": "Sep 18, 2025",
    "workDateISO": "2025-09-18T00:00:00.000Z",
    "cluster": "discipline-completion",
    "chatDate": "9/20/2025",
    "featured": false
  },
  {
    "title": "When Your Agents Disagree (And That's OK)",
    "excerpt": "",
    "url": "/blog/when-your-agents-disagree-and-thats-ok",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/81b764fa5de2",
    "featuredImage": "/assets/blog-images/81b764fa5de2-featured.png",
    "slug": "when-your-agents-disagree-and-thats-ok",
    "category": "building",
    "workDate": "Sep 17, 2025",
    "workDateISO": "2025-09-17T00:00:00.000Z",
    "cluster": "discipline-completion",
    "chatDate": "9/19/2025",
    "featured": false
  },
  {
    "title": "9/16?: When Your Methodology Holds Under Pressure",
    "excerpt": "",
    "url": "/blog/916-when-your-methodology-holds-under-pressure",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/d7bf51a718a3",
    "featuredImage": "/assets/blog-images/d7bf51a718a3-featured.png",
    "slug": "916-when-your-methodology-holds-under-pressure",
    "category": "building",
    "workDate": "Sep 15, 2025",
    "workDateISO": "2025-09-15T00:00:00.000Z",
    "cluster": "discipline-completion",
    "featured": false
  },
  {
    "title": "Back in the Optimist Bird Seat",
    "excerpt": "",
    "url": "/blog/back-in-the-optimist-bird-seat",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4407ec7dfb6c",
    "featuredImage": "/assets/blog-images/4407ec7dfb6c-featured.png",
    "slug": "back-in-the-optimist-bird-seat",
    "category": "building",
    "workDate": "Sep 16, 2025",
    "workDateISO": "2025-09-16T00:00:00.000Z",
    "cluster": "discipline-completion",
    "chatDate": "9/16/2025",
    "featured": false
  },
  {
    "title": "When You Need to Go into Inchworm Mode",
    "excerpt": "",
    "url": "/blog/when-you-need-to-go-into-inchworm-mode",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/9b7bbd23a16c",
    "featuredImage": "/assets/blog-images/9b7bbd23a16c-featured.png",
    "slug": "when-you-need-to-go-into-inchworm-mode",
    "category": "building",
    "workDate": "Sep 13, 2025",
    "workDateISO": "2025-09-13T00:00:00.000Z",
    "cluster": "strategic-pause",
    "chatDate": "9/12/2025",
    "featured": false
  },
  {
    "title": "The Strategic Pause",
    "excerpt": "",
    "url": "/blog/the-strategic-pause",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/46c9aa742bef",
    "featuredImage": "/assets/blog-images/46c9aa742bef-featured.png",
    "slug": "the-strategic-pause",
    "category": "building",
    "workDate": "Sep 14, 2025",
    "workDateISO": "2025-09-14T00:00:00.000Z",
    "cluster": "strategic-pause",
    "chatDate": "9/12/2025",
    "featured": false
  },
  {
    "title": "The three-AI orchestra: lessons from coordinating multiple AI agents",
    "excerpt": "",
    "url": "/blog/the-three-ai-orchestra-lessons-from-coordinating-multiple-ai-agents",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/0aeb570e3298",
    "featuredImage": "/assets/blog-images/0aeb570e3298-featured.webp",
    "slug": "the-three-ai-orchestra-lessons-from-coordinating-multiple-ai-agents",
    "category": "insight",
    "workDate": "Jul 19, 2025",
    "workDateISO": "2025-07-19T00:00:00.000Z",
    "cluster": "strategic-pause",
    "chatDate": "7/16/2025",
    "featured": false
  },
  {
    "title": "The Just-in-Time Retrospective: How Fresh Session Logs Became Our Content Strategy",
    "excerpt": "",
    "url": "/blog/the-just-in-time-retrospective-how-fresh-session-logs-became-our-content-strategy",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/2fc8034af04f",
    "featuredImage": "/assets/blog-images/2fc8034af04f-featured.png",
    "slug": "the-just-in-time-retrospective-how-fresh-session-logs-became-our-content-strategy",
    "category": "insight",
    "workDate": "Jul 15, 2025",
    "workDateISO": "2025-07-15T00:00:00.000Z",
    "cluster": "strategic-pause",
    "chatDate": "7/12/2025",
    "featured": false
  },
  {
    "title": "Methodology Under Fire: A Development Story",
    "excerpt": "",
    "url": "/blog/methodology-under-fire-a-development-story",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/6fbbf88fbf66",
    "featuredImage": "/assets/blog-images/6fbbf88fbf66-featured.jpg",
    "slug": "methodology-under-fire-a-development-story",
    "category": "building",
    "workDate": "Sep 12, 2025",
    "workDateISO": "2025-09-12T00:00:00.000Z",
    "cluster": "strategic-pause",
    "chatDate": "9/12/2025",
    "featured": false
  },
  {
    "title": "The Vision That Was Always There",
    "excerpt": "",
    "url": "/blog/the-vision-that-was-always-there",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/ec4b50326f02",
    "featuredImage": "/assets/blog-images/ec4b50326f02-featured.png",
    "slug": "the-vision-that-was-always-there",
    "category": "building",
    "workDate": "Sep 13, 2025",
    "workDateISO": "2025-09-13T00:00:00.000Z",
    "cluster": "strategic-pause",
    "chatDate": "9/12/2025",
    "featured": false
  },
  {
    "title": "We Spent Four Days on Boring Work. Day Five, We Gave Our AI a Personality",
    "excerpt": "",
    "url": "/blog/we-spent-four-days-on-boring-work-day-five-we-gave-our-ai-a-personality",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/eb3ec58e6284",
    "featuredImage": "/assets/blog-images/eb3ec58e6284-featured.png",
    "slug": "we-spent-four-days-on-boring-work-day-five-we-gave-our-ai-a-personality",
    "category": "building",
    "workDate": "Sep 11, 2025",
    "workDateISO": "2025-09-11T00:00:00.000Z",
    "cluster": "strategic-pause",
    "chatDate": "9/9/2025",
    "featured": false
  },
  {
    "title": "Train Tracks vs Free-for-All: When Methodology Becomes Infrastructure",
    "excerpt": "",
    "url": "/blog/train-tracks-vs-free-for-all-when-methodology-becomes-infrastructure",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4ecc40d907e0",
    "featuredImage": "/assets/blog-images/4ecc40d907e0-featured.png",
    "slug": "train-tracks-vs-free-for-all-when-methodology-becomes-infrastructure",
    "category": "building",
    "workDate": "Sep 10, 2025",
    "workDateISO": "2025-09-10T00:00:00.000Z",
    "cluster": "strategic-pause",
    "chatDate": "9/9/2025",
    "featured": false
  },
  {
    "title": "The Two-Line Fix That Took All Day (Or: Why Process Is Product)",
    "excerpt": "",
    "url": "/blog/the-two-line-fix-that-took-all-day-or-why-process-is-product",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/12b31efe360b",
    "featuredImage": "/assets/blog-images/12b31efe360b-featured.png",
    "slug": "the-two-line-fix-that-took-all-day-or-why-process-is-product",
    "category": "building",
    "workDate": "Sep 9, 2025",
    "workDateISO": "2025-09-09T00:00:00.000Z",
    "cluster": "strategic-pause",
    "chatDate": "9/9/2025",
    "featured": false
  },
  {
    "title": "When Methodology Meets Reality: Building While Learning",
    "excerpt": "",
    "url": "/blog/when-methodology-meets-reality-building-while-learning",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/0d83dcb92553",
    "featuredImage": "/assets/blog-images/0d83dcb92553-featured.png",
    "slug": "when-methodology-meets-reality-building-while-learning",
    "category": "building",
    "workDate": "Sep 7, 2025",
    "workDateISO": "2025-09-07T00:00:00.000Z",
    "cluster": "meta-development",
    "chatDate": "9/6/2025",
    "featured": false
  },
  {
    "title": "The Fractal Edge: When Problems Get Smaller, Not Fewer",
    "excerpt": "",
    "url": "/blog/the-fractal-edge-when-problems-get-smaller-not-fewer",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/5be76c5cf5de",
    "featuredImage": "/assets/blog-images/5be76c5cf5de-featured.png",
    "slug": "the-fractal-edge-when-problems-get-smaller-not-fewer",
    "category": "building",
    "workDate": "Sep 8, 2025",
    "workDateISO": "2025-09-08T00:00:00.000Z",
    "cluster": "meta-development",
    "chatDate": "9/6/2025",
    "featured": false
  },
  {
    "title": "Digital Archaeology of a Lost AI Development Weekend",
    "excerpt": "",
    "url": "/blog/digital-archaeology-of-a-lost-ai-development-weekend",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/263831a13e10",
    "featuredImage": "/assets/blog-images/263831a13e10-featured.webp",
    "slug": "digital-archaeology-of-a-lost-ai-development-weekend",
    "category": "insight",
    "workDate": "Jul 11, 2025",
    "workDateISO": "2025-07-11T00:00:00.000Z",
    "cluster": "meta-development",
    "chatDate": "7/11/2025",
    "featured": false
  },
  {
    "title": "The Archaeology of Code (Or: How Session Logs Became Stories)",
    "excerpt": "",
    "url": "/blog/the-archaeology-of-code-or",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/6a49dea29795",
    "featuredImage": "/assets/blog-images/6a49dea29795-featured.webp",
    "slug": "the-archaeology-of-code-or",
    "category": "insight",
    "workDate": "Jul 7, 2025",
    "workDateISO": "2025-07-07T00:00:00.000Z",
    "cluster": "meta-development",
    "featured": false
  },
  {
    "title": "When Your Framework Catches You Cheating on Your Framework",
    "excerpt": "",
    "url": "/blog/when-your-framework-catches-you-cheating-on-your-framework",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f0fcbd49965e",
    "featuredImage": "/assets/blog-images/f0fcbd49965e-featured.png",
    "slug": "when-your-framework-catches-you-cheating-on-your-framework",
    "category": "building",
    "workDate": "Sep 5, 2025",
    "workDateISO": "2025-09-05T00:00:00.000Z",
    "cluster": "meta-development",
    "chatDate": "9/3/2025",
    "featured": false
  },
  {
    "title": "When Your AI Assistant Reports on Building Itself",
    "excerpt": "",
    "url": "/blog/when-your-ai-assistant-reports-on-building-itself",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/e46095eb61a0",
    "featuredImage": "/assets/blog-images/e46095eb61a0-featured.png",
    "slug": "when-your-ai-assistant-reports-on-building-itself",
    "category": "building",
    "workDate": "Sep 6, 2025",
    "workDateISO": "2025-09-06T00:00:00.000Z",
    "cluster": "meta-development",
    "chatDate": "9/6/2025",
    "featured": false
  },
  {
    "title": "The Day We Built Methodology That Validates Itself",
    "excerpt": "",
    "url": "/blog/the-day-we-built-methodology-that-validates-itself",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/edeb95611ba6",
    "featuredImage": "/assets/blog-images/edeb95611ba6-featured.png",
    "slug": "the-day-we-built-methodology-that-validates-itself",
    "category": "building",
    "workDate": "Sep 4, 2025",
    "workDateISO": "2025-09-04T00:00:00.000Z",
    "cluster": "meta-development",
    "chatDate": "9/3/2025",
    "featured": false
  },
  {
    "title": "The Methodology Cascade Problem (And How We're Solving It)",
    "excerpt": "",
    "url": "/blog/the-methodology-cascade-problem-and-how-were-solving-it",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/283c92ab9267",
    "featuredImage": "/assets/blog-images/283c92ab9267-featured.png",
    "slug": "the-methodology-cascade-problem-and-how-were-solving-it",
    "category": "building",
    "workDate": "Sep 3, 2025",
    "workDateISO": "2025-09-03T00:00:00.000Z",
    "cluster": "meta-development",
    "chatDate": "9/3/2025",
    "featured": false
  },
  {
    "title": "Building the Architecture that Build Itself",
    "excerpt": "Building the Architecture That Builds Itself“I can make it on my own”September 2You know that moment when your methodology catches you trying to cheat on your own methodology? That’s what happened yesterday at 9:59 PM, and it might be the most validating moment in this entire Piper Morgan journey...",
    "url": "/blog/building-the-architecture-that-build-itself",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "building"
    ],
    "guid": "https://medium.com/building-piper-morgan/709a10b7f5c4",
    "featuredImage": "/assets/blog-images/709a10b7f5c4-featured.png",
    "slug": "building-the-architecture-that-build-itself",
    "category": "building",
    "workDate": "Sep 2, 2025",
    "workDateISO": "2025-09-02T00:00:00.000Z",
    "cluster": "meta-development",
    "chatDate": "9/3/2025",
    "featured": false
  },
  {
    "title": "From Organic to Orchestrated: When Methodology Becomes Infrastructure",
    "excerpt": "",
    "url": "/blog/from-organic-to-orchestrated-when-methodology-becomes-infrastructure",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/577dde7ad54a",
    "featuredImage": "/assets/blog-images/577dde7ad54a-featured.png",
    "slug": "from-organic-to-orchestrated-when-methodology-becomes-infrastructure",
    "category": "building",
    "workDate": "Aug 31, 2025",
    "workDateISO": "2025-08-31T00:00:00.000Z",
    "cluster": "orchestration-verification",
    "chatDate": "8/28/2025",
    "featured": false
  },
  {
    "title": "Building the MVP While Keeping the Dream Alive (fix roadmap, check facts)",
    "excerpt": "",
    "url": "/blog/building-the-mvp-while-keeping-the-dream-alive-fix-roadmap-check-facts",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/bb1def7c48be",
    "featuredImage": "/assets/blog-images/bb1def7c48be-featured.png",
    "slug": "building-the-mvp-while-keeping-the-dream-alive-fix-roadmap-check-facts",
    "category": "insight",
    "workDate": "Jul 10, 2025",
    "workDateISO": "2025-07-10T00:00:00.000Z",
    "cluster": "orchestration-verification",
    "chatDate": "7/9/2025",
    "featured": false
  },
  {
    "title": "When 80% Overhead Forces a Tool Change",
    "excerpt": "",
    "url": "/blog/when-80-overhead-forces-a-tool",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/09c852964c70",
    "featuredImage": "/assets/blog-images/09c852964c70-featured.webp",
    "slug": "when-80-overhead-forces-a-tool",
    "category": "insight",
    "workDate": "Jul 6, 2025",
    "workDateISO": "2025-07-06T00:00:00.000Z",
    "cluster": "orchestration-verification",
    "featured": false
  },
  {
    "title": "The Day Piper Published to My Company Wiki: Sometimes a Great Notion",
    "excerpt": "",
    "url": "/blog/the-day-piper-published-to-my-company-wiki-sometimes-a-great-notion",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/6359151caf25",
    "featuredImage": "/assets/blog-images/6359151caf25-featured.png",
    "slug": "the-day-piper-published-to-my-company-wiki-sometimes-a-great-notion",
    "category": "building",
    "workDate": "Aug 29, 2025",
    "workDateISO": "2025-08-29T00:00:00.000Z",
    "cluster": "orchestration-verification",
    "chatDate": "8/28/2025",
    "featured": false
  },
  {
    "title": "When AI Agents Cut Corners (And How to Catch Them)",
    "excerpt": "",
    "url": "/blog/when-ai-agents-cut-corners-and-how-to-catch-them",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/fe55ea2e0863",
    "featuredImage": "/assets/blog-images/fe55ea2e0863-featured.png",
    "slug": "when-ai-agents-cut-corners-and-how-to-catch-them",
    "category": "building",
    "workDate": "Aug 30, 2025",
    "workDateISO": "2025-08-30T00:00:00.000Z",
    "cluster": "orchestration-verification",
    "chatDate": "8/28/2025",
    "featured": false
  },
  {
    "title": "The AI That Caught Its Own Lies",
    "excerpt": "",
    "url": "/blog/the-ai-that-caught-its-own-lies",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/e374e28c8304",
    "featuredImage": "/assets/blog-images/e374e28c8304-featured.png",
    "slug": "the-ai-that-caught-its-own-lies",
    "category": "building",
    "workDate": "Aug 28, 2025",
    "workDateISO": "2025-08-28T00:00:00.000Z",
    "cluster": "orchestration-verification",
    "chatDate": "8/28/2025",
    "featured": false
  },
  {
    "title": "Verification Theater and the Chaos We Don't See",
    "excerpt": "",
    "url": "/blog/verification-theater-and-the-chaos-we-dont-see",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/98f1c8575c90",
    "featuredImage": "/assets/blog-images/98f1c8575c90-featured.png",
    "slug": "verification-theater-and-the-chaos-we-dont-see",
    "category": "building",
    "workDate": "Aug 27, 2025",
    "workDateISO": "2025-08-27T00:00:00.000Z",
    "cluster": "orchestration-verification",
    "chatDate": "8/28/2025",
    "featured": false
  },
  {
    "title": "When Good Habits Go Bad (And How We Got Them Back)",
    "excerpt": "",
    "url": "/blog/when-good-habits-go-bad-and-how-we-got-them-back",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c220cd70bc2d",
    "featuredImage": "/assets/blog-images/c220cd70bc2d-featured.png",
    "slug": "when-good-habits-go-bad-and-how-we-got-them-back",
    "category": "building",
    "workDate": "Aug 25, 2025",
    "workDateISO": "2025-08-25T00:00:00.000Z",
    "cluster": "orchestration-verification",
    "chatDate": "8/23/2025",
    "featured": false
  },
  {
    "title": "The Day After: When Methodology Becomes Muscle Memory",
    "excerpt": "",
    "url": "/blog/the-day-after-when-methodology-becomes-muscle-memory",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c9419e72a716",
    "featuredImage": "/assets/blog-images/c9419e72a716-featured.png",
    "slug": "the-day-after-when-methodology-becomes-muscle-memory",
    "category": "building",
    "workDate": "Aug 26, 2025",
    "workDateISO": "2025-08-26T00:00:00.000Z",
    "cluster": "orchestration-verification",
    "chatDate": "8/23/2025",
    "featured": false
  },
  {
    "title": "The Sunday When Everything Clicked",
    "excerpt": "",
    "url": "/blog/the-sunday-when-everything-clicked",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/53a3abc8a156",
    "featuredImage": "/assets/blog-images/53a3abc8a156-featured.png",
    "slug": "the-sunday-when-everything-clicked",
    "category": "building",
    "workDate": "Aug 24, 2025",
    "workDateISO": "2025-08-24T00:00:00.000Z",
    "cluster": "orchestration-verification",
    "chatDate": "8/23/2025",
    "featured": false
  },
  {
    "title": "Refining AI Chat Continuity for Complex Projects",
    "excerpt": "",
    "url": "/blog/refining-ai-chat-continuity-for-complex-projects",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/690308c75a13",
    "featuredImage": "/assets/blog-images/690308c75a13-featured.webp",
    "slug": "refining-ai-chat-continuity-for-complex-projects",
    "category": "insight",
    "workDate": "Jul 3, 2025",
    "workDateISO": "2025-07-03T00:00:00.000Z",
    "cluster": "enhanced-capabilities",
    "chatDate": "8/28/2025",
    "featured": false
  },
  {
    "title": "Making Strategic Technical Decisions with AI: The MCP Integration Story",
    "excerpt": "",
    "url": "/blog/making-strategic-technical-decisions-with-ai",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4c203b9e848c",
    "featuredImage": "/assets/blog-images/4c203b9e848c-featured.webp",
    "slug": "making-strategic-technical-decisions-with-ai",
    "category": "insight",
    "workDate": "Jul 3, 2025",
    "workDateISO": "2025-07-03T00:00:00.000Z",
    "cluster": "enhanced-capabilities",
    "featured": false
  },
  {
    "title": "The Friday Housekeeping That Turned Into Infrastructure Gold (Or: Sometimes the Boring Work Is the Real Work)",
    "excerpt": "",
    "url": "/blog/the-friday-housekeeping-that-turned-into-infrastructure-gold-or-sometimes-the-boring-work-is-the",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/0e400ccc7994",
    "featuredImage": "/assets/blog-images/0e400ccc7994-featured.png",
    "slug": "the-friday-housekeeping-that-turned-into-infrastructure-gold-or-sometimes-the-boring-work-is-the",
    "category": "building",
    "workDate": "Aug 22, 2025",
    "workDateISO": "2025-08-22T00:00:00.000Z",
    "cluster": "enhanced-capabilities",
    "chatDate": "8/20/2025",
    "featured": false
  },
  {
    "title": "When Your MVP Develops Its Own Nervous System",
    "excerpt": "",
    "url": "/blog/when-your-mvp-develops-its-own-nervous-system",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/61d2531fd4cf",
    "featuredImage": "/assets/blog-images/61d2531fd4cf-featured.png",
    "slug": "when-your-mvp-develops-its-own-nervous-system",
    "category": "building",
    "workDate": "Aug 23, 2025",
    "workDateISO": "2025-08-23T00:00:00.000Z",
    "cluster": "enhanced-capabilities",
    "chatDate": "8/23/2025",
    "featured": false
  },
  {
    "title": "The Enhanced Prompting Breakthrough (Or: When Better Instructions Beat Smarter Models)",
    "excerpt": "",
    "url": "/blog/the-enhanced-prompting-breakthrough-or-when-better-instructions-beat-smarter-models",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/e37d6a2b9d06",
    "featuredImage": "/assets/blog-images/e37d6a2b9d06-featured.png",
    "slug": "the-enhanced-prompting-breakthrough-or-when-better-instructions-beat-smarter-models",
    "category": "building",
    "workDate": "Aug 21, 2025",
    "workDateISO": "2025-08-21T00:00:00.000Z",
    "cluster": "enhanced-capabilities",
    "chatDate": "8/20/2025",
    "featured": false
  },
  {
    "title": "The puzzle pieces finally click (or: How to tell if you’re building tools or just collecting code)",
    "excerpt": "",
    "url": "/blog/the-puzzle-pieces-finally-click-or-how-to-tell-if-youre-building-tools-or-just-collecting-code",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/fb20a09a9d8f",
    "featuredImage": "/assets/blog-images/fb20a09a9d8f-featured.png",
    "slug": "the-puzzle-pieces-finally-click-or-how-to-tell-if-youre-building-tools-or-just-collecting-code",
    "category": "building",
    "workDate": "Aug 20, 2025",
    "workDateISO": "2025-08-20T00:00:00.000Z",
    "cluster": "enhanced-capabilities",
    "chatDate": "8/20/2025",
    "featured": false
  },
  {
    "title": "Systematic persistence through operational chaos",
    "excerpt": "",
    "url": "/blog/systematic-persistence-through-operational-chaos",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f067fd8f4d7d",
    "featuredImage": "/assets/blog-images/f067fd8f4d7d-featured.png",
    "slug": "systematic-persistence-through-operational-chaos",
    "category": "building",
    "workDate": "Aug 18, 2025",
    "workDateISO": "2025-08-18T00:00:00.000Z",
    "cluster": "enhanced-capabilities",
    "chatDate": "8/17/2025",
    "featured": false
  },
  {
    "title": "From Archaeological Mystery to Infrastructure Triumph",
    "excerpt": "",
    "url": "/blog/from-archaeological-mystery-to-infrastructure-triumph",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/1ede9b664c68",
    "featuredImage": "/assets/blog-images/1ede9b664c68-featured.png",
    "slug": "from-archaeological-mystery-to-infrastructure-triumph",
    "category": "building",
    "workDate": "Aug 19, 2025",
    "workDateISO": "2025-08-19T00:00:00.000Z",
    "cluster": "enhanced-capabilities",
    "chatDate": "8/17/2025",
    "featured": false
  },
  {
    "title": "The convergence day (or: How to tell if you're having breakthroughs or just drinking your own Kool-Aid)",
    "excerpt": "",
    "url": "/blog/the-convergence-day-or-how-to-tell-if-youre-having-breakthroughs-or-just-drinking-your-own-kool-aid",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/49e65eb92e82",
    "featuredImage": "/assets/blog-images/49e65eb92e82-featured.png",
    "slug": "the-convergence-day-or-how-to-tell-if-youre-having-breakthroughs-or-just-drinking-your-own-kool-aid",
    "category": "building",
    "workDate": "Aug 16, 2025",
    "workDateISO": "2025-08-16T00:00:00.000Z",
    "cluster": "enhanced-capabilities",
    "chatDate": "8/17/2025",
    "featured": false
  },
  {
    "title": "The satisfying discipline of turning insights into architecture",
    "excerpt": "",
    "url": "/blog/the-satisfying-discipline-of-turning-insights-into-architecture",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/cbe20baa23c3",
    "featuredImage": "/assets/blog-images/cbe20baa23c3-featured.png",
    "slug": "the-satisfying-discipline-of-turning-insights-into-architecture",
    "category": "building",
    "workDate": "Aug 17, 2025",
    "workDateISO": "2025-08-17T00:00:00.000Z",
    "cluster": "enhanced-capabilities",
    "chatDate": "8/17/2025",
    "featured": false
  },
  {
    "title": "Why I Created an AI Chief of Staff",
    "excerpt": "",
    "url": "/blog/why-i-created-an-ai-chief",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/dcbd5e7c988e",
    "featuredImage": "/assets/blog-images/dcbd5e7c988e-featured.png",
    "slug": "why-i-created-an-ai-chief",
    "category": "insight",
    "workDate": "Jul 3, 2025",
    "workDateISO": "2025-07-03T00:00:00.000Z",
    "cluster": "enhanced-capabilities",
    "featured": false
  },
  {
    "title": "When Overconfidence Meets rm -rf: A GitHub Pages Debugging Tale",
    "excerpt": "",
    "url": "/blog/when-overconfidence-meets-rm-rf-a-github-pages-debugging-tale",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/2f355444ec38",
    "featuredImage": "/assets/blog-images/2f355444ec38-featured.webp",
    "slug": "when-overconfidence-meets-rm-rf-a-github-pages-debugging-tale",
    "category": "insight",
    "workDate": "Jun 27, 2025",
    "workDateISO": "2025-06-27T00:00:00.000Z",
    "cluster": "infrastructure-sprint",
    "chatDate": "7/22/2025",
    "featured": false
  },
  {
    "title": "The day AI agents learned to coordinate themselves (and we learned to let them)",
    "excerpt": "",
    "url": "/blog/the-day-ai-agents-learned-to-coordinate-themselves-and-we-learned-to-let-them",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/02d04196ad8e",
    "featuredImage": "/assets/blog-images/02d04196ad8e-featured.png",
    "slug": "the-day-ai-agents-learned-to-coordinate-themselves-and-we-learned-to-let-them",
    "category": "building",
    "workDate": "Aug 15, 2025",
    "workDateISO": "2025-08-15T00:00:00.000Z",
    "cluster": "infrastructure-sprint",
    "chatDate": "8/12/2025",
    "featured": false
  },
  {
    "title": "How Reusing Patterns Compounds Your Acceleration`",
    "excerpt": "",
    "url": "/blog/how-reusing-patterns-compounds-your-acceleration",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/60d2a0d7acbd",
    "featuredImage": "/assets/blog-images/60d2a0d7acbd-featured.png",
    "slug": "how-reusing-patterns-compounds-your-acceleration",
    "category": "building",
    "workDate": "Aug 14, 2025",
    "workDateISO": "2025-08-14T00:00:00.000Z",
    "cluster": "infrastructure-sprint",
    "featured": false
  },
  {
    "title": "The uncomfortable victory: When completing beats innovating",
    "excerpt": "",
    "url": "/blog/the-uncomfortable-victory-when-completing-beats-innovating",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/97c356c12d55",
    "featuredImage": "/assets/blog-images/97c356c12d55-featured.png",
    "slug": "the-uncomfortable-victory-when-completing-beats-innovating",
    "category": "building",
    "workDate": "Aug 13, 2025",
    "workDateISO": "2025-08-13T00:00:00.000Z",
    "cluster": "infrastructure-sprint",
    "chatDate": "8/12/2025",
    "featured": false
  },
  {
    "title": "The 28,000-line foundation that made 4 hours feel like magic",
    "excerpt": "",
    "url": "/blog/the-28000-line-foundation-that-made-4-hours-feel-like-magic",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/82eafb4548f7",
    "featuredImage": "/assets/blog-images/82eafb4548f7-featured.png",
    "slug": "the-28000-line-foundation-that-made-4-hours-feel-like-magic",
    "category": "building",
    "workDate": "Aug 11, 2025",
    "workDateISO": "2025-08-11T00:00:00.000Z",
    "cluster": "infrastructure-sprint",
    "chatDate": "8/10/2025",
    "featured": false
  },
  {
    "title": "The day our methodology saved us from our own hype",
    "excerpt": "",
    "url": "/blog/the-day-our-methodology-saved-us-from-our-own-hype",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/35a91d794dc3",
    "featuredImage": "/assets/blog-images/35a91d794dc3-featured.png",
    "slug": "the-day-our-methodology-saved-us-from-our-own-hype",
    "category": "building",
    "workDate": "Aug 12, 2025",
    "workDateISO": "2025-08-12T00:00:00.000Z",
    "cluster": "infrastructure-sprint",
    "chatDate": "8/12/2025",
    "featured": false
  },
  {
    "title": "What We Found When We Actually Looked (And What We Built While We Weren't Looking)",
    "excerpt": "",
    "url": "/blog/what-we-found-when-we-actually-looked-and-what-we-built-while-we-werent-looking",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/7c43e28211f3",
    "featuredImage": "/assets/blog-images/7c43e28211f3-featured.webp",
    "slug": "what-we-found-when-we-actually-looked-and-what-we-built-while-we-werent-looking",
    "category": "building",
    "workDate": "Aug 9, 2025",
    "workDateISO": "2025-08-09T00:00:00.000Z",
    "cluster": "infrastructure-sprint",
    "chatDate": "8/12/2025",
    "featured": false
  },
  {
    "title": "The archaeology expedition that found automation gold",
    "excerpt": "",
    "url": "/blog/the-archaeology-expedition-that-found-automation-gold",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/b10058d924af",
    "featuredImage": "/assets/blog-images/b10058d924af-featured.webp",
    "slug": "the-archaeology-expedition-that-found-automation-gold",
    "category": "building",
    "workDate": "Aug 10, 2025",
    "workDateISO": "2025-08-10T00:00:00.000Z",
    "cluster": "infrastructure-sprint",
    "chatDate": "8/10/2025",
    "featured": false
  },
  {
    "title": "Teaching an AI to Sound Like Me (Without Losing My Mind)",
    "excerpt": "",
    "url": "/blog/teaching-an-ai-to-sound-like",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4660787e98a1",
    "featuredImage": "/assets/blog-images/4660787e98a1-featured.webp",
    "slug": "teaching-an-ai-to-sound-like",
    "category": "insight",
    "workDate": "Jun 30, 2025",
    "workDateISO": "2025-06-30T00:00:00.000Z",
    "cluster": "infrastructure-sprint",
    "featured": false
  },
  {
    "title": "Session Logs: A Surprisingly Useful Practice for AI Development",
    "excerpt": "",
    "url": "/blog/session-logs",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c73103da9907",
    "featuredImage": "/assets/blog-images/c73103da9907-featured.webp",
    "slug": "session-logs",
    "category": "insight",
    "workDate": "Jun 26, 2025",
    "workDateISO": "2025-06-26T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "featured": false
  },
  {
    "title": "Building Reliable AI Workflows When the Stakes Actually Matter: How a Trust Crisis Transformed Our Spring Cleaning Sprint",
    "excerpt": "",
    "url": "/blog/building-reliable-ai-workflows-when-the-stakes-actually-matter-how-a-trust-crisis-transformed-our",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/0f4ee7ec840e",
    "featuredImage": "/assets/blog-images/0f4ee7ec840e-featured.webp",
    "slug": "building-reliable-ai-workflows-when-the-stakes-actually-matter-how-a-trust-crisis-transformed-our",
    "category": "building",
    "workDate": "Aug 6, 2025",
    "workDateISO": "2025-08-06T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "chatDate": "8/6/2025",
    "featured": false
  },
  {
    "title": "When 44 Minutes of Foundation Work Enables 9 Minutes of Magic",
    "excerpt": "",
    "url": "/blog/when-44-minutes-of-foundation-work-enables-9-minutes-of-magic",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f18755220580",
    "featuredImage": "/assets/blog-images/f18755220580-featured.webp",
    "slug": "when-44-minutes-of-foundation-work-enables-9-minutes-of-magic",
    "category": "building",
    "workDate": "Aug 7, 2025",
    "workDateISO": "2025-08-07T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "chatDate": "8/6/2025",
    "featured": false
  },
  {
    "title": "The Documentation Debt That Almost Buried Our Breakthrough (And the Systematic Approach That Saved It)",
    "excerpt": "",
    "url": "/blog/the-documentation-debt-that-almost-buried-our-breakthrough-and-the-systematic-approach-that-saved-it",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/e22e491dab71",
    "featuredImage": "/assets/blog-images/e22e491dab71-featured.webp",
    "slug": "the-documentation-debt-that-almost-buried-our-breakthrough-and-the-systematic-approach-that-saved-it",
    "category": "building",
    "workDate": "Aug 8, 2025",
    "workDateISO": "2025-08-08T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "chatDate": "8/6/2025",
    "featured": false
  },
  {
    "title": "Weekend Sprint Chronicles: Six Infrastructure Victories and a Dead Show",
    "excerpt": "",
    "url": "/blog/weekend-sprint-chronicles-six-infrastructure-victories-and-a-dead-show",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/495a9ed09430",
    "featuredImage": "/assets/blog-images/495a9ed09430-featured.webp",
    "slug": "weekend-sprint-chronicles-six-infrastructure-victories-and-a-dead-show",
    "category": "building",
    "workDate": "Aug 3, 2025",
    "workDateISO": "2025-08-03T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "chatDate": "8/3/2025",
    "featured": false
  },
  {
    "title": "When Your Tools Stop Crying Wolf",
    "excerpt": "",
    "url": "/blog/when-your-tools-stop-crying-wolf",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/de7a1feed708",
    "featuredImage": "/assets/blog-images/de7a1feed708-featured.webp",
    "slug": "when-your-tools-stop-crying-wolf",
    "category": "building",
    "workDate": "Jul 31, 2025",
    "workDateISO": "2025-07-31T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "chatDate": "7/31/2025",
    "featured": false
  },
  {
    "title": "The 71-Minute Cascade Killer: When Systematic Methodology Meets Production Reality",
    "excerpt": "",
    "url": "/blog/the-71-minute-cascade-killer-when-systematic-methodology-meets-production-reality",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/bf217794054d",
    "featuredImage": "/assets/blog-images/bf217794054d-featured.webp",
    "slug": "the-71-minute-cascade-killer-when-systematic-methodology-meets-production-reality",
    "category": "building",
    "workDate": "Aug 1, 2025",
    "workDateISO": "2025-08-01T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "chatDate": "7/31/2025",
    "featured": false
  },
  {
    "title": "Saturday Reflection: Why Ethics Can't Be an Afterthought",
    "excerpt": "",
    "url": "/blog/saturday-reflection-why-ethics-cant-be-an-afterthought",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/07e55d3cff93",
    "featuredImage": "/assets/blog-images/07e55d3cff93-featured.png",
    "slug": "saturday-reflection-why-ethics-cant-be-an-afterthought",
    "category": "building",
    "workDate": "Aug 2, 2025",
    "workDateISO": "2025-08-02T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "chatDate": "7/31/2025",
    "featured": false
  },
  {
    "title": "The Day We Didn't Just Integrate Slack But Started Incorporating Spatial Intelligence",
    "excerpt": "",
    "url": "/blog/the-day-we-didnt-just-integrate-slack-but-started-incorporating-spatial-intelligence",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/a1f5fc08b053",
    "featuredImage": "/assets/blog-images/a1f5fc08b053-featured.png",
    "slug": "the-day-we-didnt-just-integrate-slack-but-started-incorporating-spatial-intelligence",
    "category": "building",
    "workDate": "Jul 28, 2025",
    "workDateISO": "2025-07-28T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "chatDate": "7/27/2025",
    "featured": false
  },
  {
    "title": "To Live Outside the Law You Must Be Honest: Debugging an Unorthodox Slack Integration",
    "excerpt": "",
    "url": "/blog/to-live-outside-the-law-you-must-be-honest-debugging-an-unorthodox-slack-integration",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/e521b612bf58",
    "featuredImage": "/assets/blog-images/e521b612bf58-featured.webp",
    "slug": "to-live-outside-the-law-you-must-be-honest-debugging-an-unorthodox-slack-integration",
    "category": "building",
    "workDate": "Jul 29, 2025",
    "workDateISO": "2025-07-29T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "chatDate": "7/29/2025",
    "featured": false
  },
  {
    "title": "The Day Crisis Became Methodology: From Runaway Workflows to Historic Productivity",
    "excerpt": "",
    "url": "/blog/the-day-crisis-became-methodology-from-runaway-workflows-to-historic-productivity",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/57c4bc5529f7",
    "featuredImage": "/assets/blog-images/57c4bc5529f7-featured.png",
    "slug": "the-day-crisis-became-methodology-from-runaway-workflows-to-historic-productivity",
    "category": "building",
    "workDate": "Jul 30, 2025",
    "workDateISO": "2025-07-30T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "chatDate": "7/29/2025",
    "featured": false
  },
  {
    "title": "8/6 revised from 7/22: When 300 Files Work as One: The Perfect Storm",
    "excerpt": "",
    "url": "/blog/86-revised-from-722",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f8ff692dbbf8",
    "featuredImage": "/assets/blog-images/f8ff692dbbf8-featured.webp",
    "slug": "86-revised-from-722",
    "category": "building",
    "workDate": "Jul 25, 2025",
    "workDateISO": "2025-07-25T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "featured": false
  },
  {
    "title": "The Accidental Methodology Stress Test: When Success Creates Its Own Blind Spots",
    "excerpt": "The Accidental Methodology Stress Test: When Success Creates Its Own Blind Spots“How do I work this?”July 26Saturday morning, and I’m riding high on a wave of systematic excellence. GitHub Pages fixed in 13 minutes. Pattern Sweep system implemented in 90 minutes. Canonical queries documented, emb...",
    "url": "/blog/the-accidental-methodology-stress-test-when-success-creates-its-own-blind-spots",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "building"
    ],
    "guid": "https://medium.com/building-piper-morgan/7511ff6368a9",
    "featuredImage": "/assets/blog-images/7511ff6368a9-featured.webp",
    "slug": "the-accidental-methodology-stress-test-when-success-creates-its-own-blind-spots",
    "category": "building",
    "workDate": "Jul 26, 2025",
    "workDateISO": "2025-07-26T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "chatDate": "7/26/2025",
    "featured": false
  },
  {
    "title": "Engineering Excellence in a Gödel-Incomplete Universe",
    "excerpt": "",
    "url": "/blog/engineering-excellence-in-a-gdel-incomplete-universe",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/7d4ea25d03fe",
    "featuredImage": "/assets/blog-images/7d4ea25d03fe-featured.webp",
    "slug": "engineering-excellence-in-a-gdel-incomplete-universe",
    "category": "building",
    "workDate": "Jul 27, 2025",
    "workDateISO": "2025-07-27T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "chatDate": "7/27/2025",
    "featured": false
  },
  {
    "title": "The Demo That Broke (And Why That's Perfect)",
    "excerpt": "",
    "url": "/blog/the-demo-that-broke-and-why",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/5140d1657000",
    "featuredImage": "/assets/blog-images/5140d1657000-featured.webp",
    "slug": "the-demo-that-broke-and-why",
    "category": "insight",
    "workDate": "Jun 22, 2025",
    "workDateISO": "2025-06-22T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "featured": false
  },
  {
    "title": "Always Keep Something Showable: Demo Infrastructure for Hyperfast Development",
    "excerpt": "",
    "url": "/blog/always-keep-something-showable-demo-infrastructure-for-hyperfast-development",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/52d682510c10",
    "featuredImage": "/assets/blog-images/52d682510c10-featured.webp",
    "slug": "always-keep-something-showable-demo-infrastructure-for-hyperfast-development",
    "category": "insight",
    "workDate": "Jun 14, 2025",
    "workDateISO": "2025-06-14T00:00:00.000Z",
    "cluster": "methodology-refinement",
    "chatDate": "8/6/2025",
    "featured": false
  },
  {
    "title": "When the Bugs Lead You Home",
    "excerpt": "",
    "url": "/blog/when-the-bugs-lead-you-home",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c9ce09f192f1",
    "featuredImage": "/assets/blog-images/c9ce09f192f1-featured.webp",
    "slug": "when-the-bugs-lead-you-home",
    "category": "building",
    "workDate": "Jul 9, 2025",
    "workDateISO": "2025-07-09T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/9/2025",
    "featured": false
  },
  {
    "title": "The Bug That Made Us Smarter",
    "excerpt": "",
    "url": "/blog/the-bug-that-made-us-smarter",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/cf1774978f51",
    "featuredImage": "/assets/blog-images/cf1774978f51-featured.webp",
    "slug": "the-bug-that-made-us-smarter",
    "category": "building",
    "workDate": "Jul 9, 2025",
    "workDateISO": "2025-07-09T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/9/2025",
    "featured": false
  },
  {
    "title": "When Your Tests Pass But Your App Fails",
    "excerpt": "",
    "url": "/blog/when-your-tests-pass-but-your-app-fails",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/3b3d6f3aeff1",
    "featuredImage": "/assets/blog-images/3b3d6f3aeff1-featured.webp",
    "slug": "when-your-tests-pass-but-your-app-fails",
    "category": "building",
    "workDate": "Jul 9, 2025",
    "workDateISO": "2025-07-09T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/9/2025",
    "featured": false
  },
  {
    "title": "The Day We Finished Next Week's Work in One Day",
    "excerpt": "",
    "url": "/blog/the-day-we-finished-next-weeks-work-in-one-day",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/ad5a228fbc0a",
    "featuredImage": "/assets/blog-images/ad5a228fbc0a-featured.webp",
    "slug": "the-day-we-finished-next-weeks-work-in-one-day",
    "category": "building",
    "workDate": "Jul 22, 2025",
    "workDateISO": "2025-07-22T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/22/2025",
    "featured": false
  },
  {
    "title": "The Final Leap: When Prototype Becomes Production Tool (mislabeld as The Day We)",
    "excerpt": "",
    "url": "/blog/the-final-leap-when-prototype-becomes-production-tool-mislabeld-as-the-day-we",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/37128cf4fdf6",
    "featuredImage": "/assets/blog-images/37128cf4fdf6-featured.webp",
    "slug": "the-final-leap-when-prototype-becomes-production-tool-mislabeld-as-the-day-we",
    "category": "building",
    "workDate": "Jul 23, 2025",
    "workDateISO": "2025-07-23T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/22/2025",
    "featured": false
  },
  {
    "title": "PTSD (Patched-Test Stress Disorder) and Other Development Culture Innovations",
    "excerpt": "",
    "url": "/blog/ptsd-patched-test-stress-disorder-and-other-development-culture-innovations",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/bef231301ab4",
    "featuredImage": "/assets/blog-images/bef231301ab4-featured.webp",
    "slug": "ptsd-patched-test-stress-disorder-and-other-development-culture-innovations",
    "category": "building",
    "workDate": "Jul 24, 2025",
    "workDateISO": "2025-07-24T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/22/2025",
    "featured": false
  },
  {
    "title": "7/16 chat: The 40-minute miracle: how two AI agents achieved 642x performance in one session",
    "excerpt": "",
    "url": "/blog/716-chat-2",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/a7d8ee906912",
    "featuredImage": "/assets/blog-images/a7d8ee906912-featured.webp",
    "slug": "716-chat-2",
    "category": "building",
    "workDate": "Jul 18, 2025",
    "workDateISO": "2025-07-18T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/16/2025",
    "featured": false
  },
  {
    "title": "7/20 chat: When Your Infrastructure Gets Smarter Than Your Tests",
    "excerpt": "",
    "url": "/blog/720-chat-2",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/2582f1c7b3d5",
    "featuredImage": "/assets/blog-images/2582f1c7b3d5-featured.webp",
    "slug": "720-chat-2",
    "category": "building",
    "workDate": "Jul 20, 2025",
    "workDateISO": "2025-07-20T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/20/2025",
    "featured": false
  },
  {
    "title": "7/20 chat: The Foundation Sprint: Why We Clean House Before Building New Rooms",
    "excerpt": "",
    "url": "/blog/720-chat",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/12f37f759a92",
    "featuredImage": "/assets/blog-images/12f37f759a92-featured.png",
    "slug": "720-chat",
    "category": "building",
    "workDate": "Jul 21, 2025",
    "workDateISO": "2025-07-21T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/20/2025",
    "featured": false
  },
  {
    "title": "7/12-7/13, 7/15 chat: When the Pupil Outsmarts the Teacher?",
    "excerpt": "",
    "url": "/blog/712-713-715-chat-2",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/cde7eb0b6605",
    "featuredImage": "/assets/blog-images/cde7eb0b6605-featured.webp",
    "slug": "712-713-715-chat-2",
    "category": "building",
    "workDate": "Jul 15, 2025",
    "workDateISO": "2025-07-15T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/12/2025",
    "featured": false
  },
  {
    "title": "7/16 chat: When Your Tests Lie: A Victory Disguised as Crisis",
    "excerpt": "",
    "url": "/blog/716-chat-3",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c70e69a245ea",
    "featuredImage": "/assets/blog-images/c70e69a245ea-featured.webp",
    "slug": "716-chat-3",
    "category": "building",
    "workDate": "Jul 16, 2025",
    "workDateISO": "2025-07-16T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/16/2025",
    "featured": false
  },
  {
    "title": "7/16 chat: The 5-Minute Day: When TDD Meets AI-Assisted Development",
    "excerpt": "",
    "url": "/blog/716-chat",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/1e15183972a7",
    "featuredImage": "/assets/blog-images/1e15183972a7-featured.png",
    "slug": "716-chat",
    "category": "building",
    "workDate": "Jul 17, 2025",
    "workDateISO": "2025-07-17T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/16/2025",
    "featured": false
  },
  {
    "title": "From 2% to 87%: The Great Test Suite Recovery",
    "excerpt": "",
    "url": "/blog/from-2-to-87",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/b7c3ef25cbdc",
    "featuredImage": "/assets/blog-images/b7c3ef25cbdc-featured.webp",
    "slug": "from-2-to-87",
    "category": "building",
    "workDate": "Jul 13, 2025",
    "workDateISO": "2025-07-13T00:00:00.000Z",
    "cluster": "production-transformation",
    "featured": false
  },
  {
    "title": "The Action Humanizer: Teaching AI to Speak Human",
    "excerpt": "",
    "url": "/blog/the-action-humanizer",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/9fbbf6932838",
    "featuredImage": "/assets/blog-images/9fbbf6932838-featured.webp",
    "slug": "the-action-humanizer",
    "category": "building",
    "workDate": "Jul 13, 2025",
    "workDateISO": "2025-07-13T00:00:00.000Z",
    "cluster": "production-transformation",
    "featured": false
  },
  {
    "title": "7/12-7/13, 7/15 chat: From Broken Tests to Perfect Architecture: The Great Cleanup of July 14",
    "excerpt": "",
    "url": "/blog/712-713-715-chat",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/2575d3526323",
    "featuredImage": "/assets/blog-images/2575d3526323-featured.webp",
    "slug": "712-713-715-chat",
    "category": "building",
    "workDate": "Jul 14, 2025",
    "workDateISO": "2025-07-14T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/12/2025",
    "featured": false
  },
  {
    "title": "Chasing Rabbits (A Debugging Story)",
    "excerpt": "",
    "url": "/blog/chasing-rabbits-a-debugging-story",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/40f084dc3095",
    "featuredImage": "/assets/blog-images/40f084dc3095-featured.png",
    "slug": "chasing-rabbits-a-debugging-story",
    "category": "building",
    "workDate": "May 31, 2025",
    "workDateISO": "2025-05-31T00:00:00.000Z",
    "cluster": "production-transformation",
    "featured": false
  },
  {
    "title": "When Your AI Writes 500 Lines of Boilerplate (And Why That's Actually Useful)",
    "excerpt": "",
    "url": "/blog/when-your-ai-writes-500-lines",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/084611e312ea",
    "featuredImage": "/assets/blog-images/084611e312ea-featured.png",
    "slug": "when-your-ai-writes-500-lines",
    "category": "building",
    "workDate": "May 31, 2025",
    "workDateISO": "2025-05-31T00:00:00.000Z",
    "cluster": "production-transformation",
    "featured": false
  },
  {
    "title": "When Claude Took a Break (And Gemini Stepped In)",
    "excerpt": "",
    "url": "/blog/when-claude-took-a-break-and-gemini-stepped-in",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/922fd802460e",
    "featuredImage": "/assets/blog-images/922fd802460e-featured.png",
    "slug": "when-claude-took-a-break-and-gemini-stepped-in",
    "category": "building",
    "workDate": "May 30, 2025",
    "workDateISO": "2025-05-30T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/11/2025",
    "featured": false
  },
  {
    "title": "The Demo That Needed Documentation",
    "excerpt": "",
    "url": "/blog/the-demo-that-needed-documentation",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/ccb351b91629",
    "featuredImage": "/assets/blog-images/ccb351b91629-featured.png",
    "slug": "the-demo-that-needed-documentation",
    "category": "building",
    "workDate": "May 30, 2025",
    "workDateISO": "2025-05-30T00:00:00.000Z",
    "cluster": "production-transformation",
    "featured": false
  },
  {
    "title": "Two-Fisted Coding: Wrangling Robot Programmers When You're Just a PM",
    "excerpt": "",
    "url": "/blog/two-fisted-coding-wrangling-robot-programmers-when-youre-just-a-pm",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c619de609a42",
    "featuredImage": "/assets/blog-images/c619de609a42-featured.png",
    "slug": "two-fisted-coding-wrangling-robot-programmers-when-youre-just-a-pm",
    "category": "building",
    "workDate": "Jul 8, 2025",
    "workDateISO": "2025-07-08T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/8/2025",
    "featured": false
  },
  {
    "title": "Three Bugs, One Victory: The Day We Finally Shipped PM-011",
    "excerpt": "",
    "url": "/blog/three-bugs-one-victory",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/cc07dca2a5e9",
    "featuredImage": "/assets/blog-images/cc07dca2a5e9-featured.webp",
    "slug": "three-bugs-one-victory",
    "category": "building",
    "workDate": "Jul 12, 2025",
    "workDateISO": "2025-07-12T00:00:00.000Z",
    "cluster": "production-transformation",
    "featured": false
  },
  {
    "title": "The AI Detective Squad: When Three Agents Solve One Mystery",
    "excerpt": "",
    "url": "/blog/the-ai-detective-squad",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/987eb4c5cc42",
    "featuredImage": "/assets/blog-images/987eb4c5cc42-featured.png",
    "slug": "the-ai-detective-squad",
    "category": "building",
    "workDate": "Jul 12, 2025",
    "workDateISO": "2025-07-12T00:00:00.000Z",
    "cluster": "production-transformation",
    "featured": false
  },
  {
    "title": "The Zeno's Paradox of Debugging: A Weekend with Piper Morgan",
    "excerpt": "",
    "url": "/blog/the-zenos-paradox-of-debugging",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/03c685be122a",
    "featuredImage": "/assets/blog-images/03c685be122a-featured.webp",
    "slug": "the-zenos-paradox-of-debugging",
    "category": "building",
    "workDate": "Jul 6, 2025",
    "workDateISO": "2025-07-06T00:00:00.000Z",
    "cluster": "production-transformation",
    "featured": false
  },
  {
    "title": "The Debugging Cascade: A 90-Minute Journey Through Integration Hell",
    "excerpt": "",
    "url": "/blog/the-debugging-cascade",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/7aaec260ede5",
    "featuredImage": "/assets/blog-images/7aaec260ede5-featured.webp",
    "slug": "the-debugging-cascade",
    "category": "building",
    "workDate": "Jul 7, 2025",
    "workDateISO": "2025-07-07T00:00:00.000Z",
    "cluster": "production-transformation",
    "featured": false
  },
  {
    "title": "The Coordination Tax: When Copy-Paste Becomes Your Biggest Bottleneck",
    "excerpt": "",
    "url": "/blog/the-coordination-tax",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4e6f997a80cf",
    "featuredImage": "/assets/blog-images/4e6f997a80cf-featured.webp",
    "slug": "the-coordination-tax",
    "category": "building",
    "workDate": "Jul 8, 2025",
    "workDateISO": "2025-07-08T00:00:00.000Z",
    "cluster": "production-transformation",
    "featured": false
  },
  {
    "title": "The Real Bugs Live in the UI (A Testing Reality Check)",
    "excerpt": "",
    "url": "/blog/the-real-bugs-live-in-the",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/336d98a417e4",
    "featuredImage": "/assets/blog-images/336d98a417e4-featured.webp",
    "slug": "the-real-bugs-live-in-the",
    "category": "building",
    "workDate": "Jul 1, 2025",
    "workDateISO": "2025-07-01T00:00:00.000Z",
    "cluster": "production-transformation",
    "featured": false
  },
  {
    "title": "The Day We Stopped Fighting the System",
    "excerpt": "",
    "url": "/blog/the-day-we-stopped-fighting-the",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/7fc3aadc2a3b",
    "featuredImage": "/assets/blog-images/7fc3aadc2a3b-featured.webp",
    "slug": "the-day-we-stopped-fighting-the",
    "category": "building",
    "workDate": "Jul 3, 2025",
    "workDateISO": "2025-07-03T00:00:00.000Z",
    "cluster": "production-transformation",
    "featured": false
  },
  {
    "title": "The Day We Taught Piper to Summarize (Almost)",
    "excerpt": "",
    "url": "/blog/the-day-we-taught-piper-to",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/437a3ec04316",
    "featuredImage": "/assets/blog-images/437a3ec04316-featured.webp",
    "slug": "the-day-we-taught-piper-to",
    "category": "building",
    "workDate": "Jul 4, 2025",
    "workDateISO": "2025-07-04T00:00:00.000Z",
    "cluster": "production-transformation",
    "featured": false
  },
  {
    "title": "When Your Tests Tell You What Your Code Should Do",
    "excerpt": "",
    "url": "/blog/when-your-tests-tell-you-what-your-code-should-do",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c00a94c09c2c",
    "featuredImage": "/assets/blog-images/c00a94c09c2c-featured.webp",
    "slug": "when-your-tests-tell-you-what-your-code-should-do",
    "category": "building",
    "workDate": "Jun 27, 2025",
    "workDateISO": "2025-06-27T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/20/2025",
    "featured": false
  },
  {
    "title": "Following Your Own Patterns",
    "excerpt": "",
    "url": "/blog/following-your-own-patterns",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/0822585cb51a",
    "featuredImage": "/assets/blog-images/0822585cb51a-featured.webp",
    "slug": "following-your-own-patterns",
    "category": "building",
    "workDate": "Jun 27, 2025",
    "workDateISO": "2025-06-27T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/20/2025",
    "featured": false
  },
  {
    "title": "Battle-Testing GitHub Integration: When Recovery Becomes Learning",
    "excerpt": "",
    "url": "/blog/battle-testing-github-integration-when-recovery-becomes-learning",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/5243027aa9f6",
    "featuredImage": "/assets/blog-images/5243027aa9f6-featured.webp",
    "slug": "battle-testing-github-integration-when-recovery-becomes-learning",
    "category": "building",
    "workDate": "Jun 29, 2025",
    "workDateISO": "2025-06-29T00:00:00.000Z",
    "cluster": "production-transformation",
    "chatDate": "7/22/2025",
    "featured": false
  },
  {
    "title": "The 48-hour rollercoaster: from working tests to ‘Failed attempt’ and back to ‘LIFE SAVER !!!”’",
    "excerpt": "",
    "url": "/blog/the-48-hour-rollercoaster",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/b4d9193ec579",
    "featuredImage": "/assets/blog-images/b4d9193ec579-featured.webp",
    "slug": "the-48-hour-rollercoaster",
    "category": "building",
    "workDate": "Jun 26, 2025",
    "workDateISO": "2025-06-26T00:00:00.000Z",
    "cluster": "complexity-reckoning",
    "featured": false
  },
  {
    "title": "The Technical Debt Reckoning",
    "excerpt": "",
    "url": "/blog/the-technical-debt-reckoning",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/160bc294b0b5",
    "featuredImage": "/assets/blog-images/160bc294b0b5-featured.webp",
    "slug": "the-technical-debt-reckoning",
    "category": "building",
    "workDate": "Jun 26, 2025",
    "workDateISO": "2025-06-26T00:00:00.000Z",
    "cluster": "complexity-reckoning",
    "chatDate": "7/20/2025",
    "featured": false
  },
  {
    "title": "Keeping Your AI Project on Track: Lessons from Building a Product Management Assistant",
    "excerpt": "",
    "url": "/blog/keeping-your-ai-project-on-track-lessons-from-building-a-product-management-assistant",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/32c8ed94248d",
    "featuredImage": "/assets/blog-images/32c8ed94248d-featured.png",
    "slug": "keeping-your-ai-project-on-track-lessons-from-building-a-product-management-assistant",
    "category": "insight",
    "workDate": "Jun 14, 2025",
    "workDateISO": "2025-06-14T00:00:00.000Z",
    "cluster": "complexity-reckoning",
    "chatDate": "7/22/2025",
    "featured": false
  },
  {
    "title": "Naming Piper Morgan",
    "excerpt": "",
    "url": "/blog/naming-piper-morgan",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/9efacddc4804",
    "featuredImage": "/assets/blog-images/9efacddc4804-featured.png",
    "slug": "naming-piper-morgan",
    "category": "insight",
    "workDate": "May 29, 2025",
    "workDateISO": "2025-05-29T00:00:00.000Z",
    "cluster": "complexity-reckoning",
    "featured": false
  },
  {
    "title": "When Your Docs Lie",
    "excerpt": "",
    "url": "/blog/when-your-docs-lie",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/98ad7b8cefd0",
    "featuredImage": "/assets/blog-images/98ad7b8cefd0-featured.png",
    "slug": "when-your-docs-lie",
    "category": "building",
    "workDate": "Jun 21, 2025",
    "workDateISO": "2025-06-21T00:00:00.000Z",
    "cluster": "complexity-reckoning",
    "featured": false
  },
  {
    "title": "When TDD Saves Your Architecture",
    "excerpt": "",
    "url": "/blog/when-tdd-saves-your-architecture",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/ca9c8039b20d",
    "featuredImage": "/assets/blog-images/ca9c8039b20d-featured.webp",
    "slug": "when-tdd-saves-your-architecture",
    "category": "building",
    "workDate": "Jun 25, 2025",
    "workDateISO": "2025-06-25T00:00:00.000Z",
    "cluster": "complexity-reckoning",
    "featured": false
  },
  {
    "title": "Digging Out of the Complexity Hole",
    "excerpt": "",
    "url": "/blog/digging-out-of-the-complexity-hole",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/117b25fa6bae",
    "featuredImage": "/assets/blog-images/117b25fa6bae-featured.webp",
    "slug": "digging-out-of-the-complexity-hole",
    "category": "building",
    "workDate": "Jun 17, 2025",
    "workDateISO": "2025-06-17T00:00:00.000Z",
    "cluster": "complexity-reckoning",
    "chatDate": "7/20/2025",
    "featured": false
  },
  {
    "title": "Successful Prototype Syndrome",
    "excerpt": "",
    "url": "/blog/successful-prototype-syndrome",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/34c725384254",
    "featuredImage": "/assets/blog-images/34c725384254-featured.webp",
    "slug": "successful-prototype-syndrome",
    "category": "building",
    "workDate": "Jun 19, 2025",
    "workDateISO": "2025-06-19T00:00:00.000Z",
    "cluster": "complexity-reckoning",
    "chatDate": "7/20/2025",
    "featured": false
  },
  {
    "title": "When Architecture Principles Trump Tactical Convenience",
    "excerpt": "",
    "url": "/blog/when-architecture-principles-trump-tactical-convenience",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/7d71c9e5316d",
    "featuredImage": "/assets/blog-images/7d71c9e5316d-featured.webp",
    "slug": "when-architecture-principles-trump-tactical-convenience",
    "category": "building",
    "workDate": "Jun 16, 2025",
    "workDateISO": "2025-06-16T00:00:00.000Z",
    "cluster": "complexity-reckoning",
    "chatDate": "7/22/2025",
    "featured": false
  },
  {
    "title": "When Multiple AIs Can Still Drift Together",
    "excerpt": "",
    "url": "/blog/when-multiple-ais-can-still-drift-together",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/0caeeadf7ef5",
    "featuredImage": "/assets/blog-images/0caeeadf7ef5-featured.webp",
    "slug": "when-multiple-ais-can-still-drift-together",
    "category": "building",
    "workDate": "Jun 15, 2025",
    "workDateISO": "2025-06-15T00:00:00.000Z",
    "cluster": "complexity-reckoning",
    "chatDate": "7/20/2025",
    "featured": false
  },
  {
    "title": "The Integration Reality Check",
    "excerpt": "",
    "url": "/blog/the-integration-reality-check",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/72145777c406",
    "featuredImage": "/assets/blog-images/72145777c406-featured.webp",
    "slug": "the-integration-reality-check",
    "category": "building",
    "workDate": "Jun 24, 2025",
    "workDateISO": "2025-06-24T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "Day Zero or Deja Zero: When Chaos Became a Claude Project",
    "excerpt": "",
    "url": "/blog/day-zero-or-deja-zero",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/2965731c90bc",
    "featuredImage": "/assets/blog-images/2965731c90bc-featured.webp",
    "slug": "day-zero-or-deja-zero",
    "category": "insight",
    "workDate": "Jun 23, 2025",
    "workDateISO": "2025-06-23T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "7/16 to 7/18: The Cascade Effect: How Testing the UI Led to Architectural Discoveries",
    "excerpt": "",
    "url": "/blog/716-to-718",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/0b19d8a13665",
    "featuredImage": "/assets/blog-images/0b19d8a13665-featured.webp",
    "slug": "716-to-718",
    "category": "building",
    "workDate": "Jun 23, 2025",
    "workDateISO": "2025-06-23T00:00:00.000Z",
    "cluster": "foundation-building",
    "chatDate": "7/16/2025",
    "featured": false
  },
  {
    "title": "From Architecture Drift to Working AI",
    "excerpt": "",
    "url": "/blog/from-architecture-drift-to-working-ai",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/201f17c5cfbf",
    "featuredImage": "/assets/blog-images/201f17c5cfbf-featured.webp",
    "slug": "from-architecture-drift-to-working-ai",
    "category": "building",
    "workDate": "Jun 15, 2025",
    "workDateISO": "2025-06-15T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "Small Scripts Win: Building Knowledge That Actually Knows Things",
    "excerpt": "",
    "url": "/blog/small-scripts-win",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/360bd682551e",
    "featuredImage": "/assets/blog-images/360bd682551e-featured.png",
    "slug": "small-scripts-win",
    "category": "building",
    "workDate": "Jun 8, 2025",
    "workDateISO": "2025-06-08T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "Modeling What PMs Do for Piper",
    "excerpt": "",
    "url": "/blog/modeling-what-pms-do-for-piper",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f6d7fac93e1f",
    "featuredImage": "/assets/blog-images/f6d7fac93e1f-featured.png",
    "slug": "modeling-what-pms-do-for-piper",
    "category": "building",
    "workDate": "Jun 7, 2025",
    "workDateISO": "2025-06-07T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "Persistence of Memory: AI Can't Learn without It",
    "excerpt": "",
    "url": "/blog/persistence-of-memory",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/d9f839597278",
    "featuredImage": "/assets/blog-images/d9f839597278-featured.png",
    "slug": "persistence-of-memory",
    "category": "building",
    "workDate": "Jun 2, 2025",
    "workDateISO": "2025-06-02T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "Building AI That Actually Thinks About Product Work",
    "excerpt": "",
    "url": "/blog/building-ai-that-actually-thinks-about",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4c04e304a3a7",
    "featuredImage": "/assets/blog-images/4c04e304a3a7-featured.png",
    "slug": "building-ai-that-actually-thinks-about",
    "category": "building",
    "workDate": "Jun 2, 2025",
    "workDateISO": "2025-06-02T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "The Question That Started Everything",
    "excerpt": "",
    "url": "/blog/the-question-that-started-everything",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/5a69f9a2af0b",
    "featuredImage": "/assets/blog-images/5a69f9a2af0b-featured.png",
    "slug": "the-question-that-started-everything",
    "category": "insight",
    "workDate": "May 27, 2025",
    "workDateISO": "2025-05-27T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "From Task Executor to Problem Solver (comes befofe Domain-First Dev)",
    "excerpt": "",
    "url": "/blog/from-task-executor-to-problem-solver",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/13896a87b7a9",
    "featuredImage": "/assets/blog-images/13896a87b7a9-featured.png",
    "slug": "from-task-executor-to-problem-solver",
    "category": "building",
    "workDate": "Jun 2, 2025",
    "workDateISO": "2025-06-02T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "The Architectural Reckoning: When Three Experts Agree You Should Start Over",
    "excerpt": "",
    "url": "/blog/the-architectural-reckoning",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/1f9581a41633",
    "featuredImage": "/assets/blog-images/1f9581a41633-featured.png",
    "slug": "the-architectural-reckoning",
    "category": "insight",
    "workDate": "May 29, 2025",
    "workDateISO": "2025-05-29T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "The $0 Bootstrap Stack: Building Enterprise Infrastructure for Free (With Upgrade Paths)",
    "excerpt": "",
    "url": "/blog/the-0-bootstrap-stack",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/078e056a87e4",
    "featuredImage": "/assets/blog-images/078e056a87e4-featured.png",
    "slug": "the-0-bootstrap-stack",
    "category": "building",
    "workDate": "Jun 1, 2025",
    "workDateISO": "2025-06-01T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "Domain-First Development: Actually Building What We Designed",
    "excerpt": "",
    "url": "/blog/domain-first-development",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/647704d46558",
    "featuredImage": "/assets/blog-images/647704d46558-featured.png",
    "slug": "domain-first-development",
    "category": "building",
    "workDate": "Jun 2, 2025",
    "workDateISO": "2025-06-02T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "From CLI to GitHub Integration: When Prototypes Meet Real Workflows",
    "excerpt": "",
    "url": "/blog/from-cli-to-github-integration",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c7207687f711",
    "featuredImage": "/assets/blog-images/c7207687f711-featured.png",
    "slug": "from-cli-to-github-integration",
    "category": "building",
    "workDate": "May 29, 2025",
    "workDateISO": "2025-05-29T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "From Research Question to Working Prototype: Building an AI PM Assistant from Scratch",
    "excerpt": "",
    "url": "/blog/from-research-question-to-working-prototype",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/bb06005611cb",
    "featuredImage": "/assets/blog-images/bb06005611cb-featured.png",
    "slug": "from-research-question-to-working-prototype",
    "category": "building",
    "workDate": "May 29, 2025",
    "workDateISO": "2025-05-29T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "The RAG Revelation: When Your Prototype Answers Back",
    "excerpt": "",
    "url": "/blog/the-rag-revelation",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/cc7f4b96b621",
    "featuredImage": "/assets/blog-images/cc7f4b96b621-featured.png",
    "slug": "the-rag-revelation",
    "category": "building",
    "workDate": "May 29, 2025",
    "workDateISO": "2025-05-29T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "Bidirectional Intelligence: Teaching AI to Critique, Not Just Create",
    "excerpt": "",
    "url": "/blog/bidirectional-intelligence",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/b5bb0c2c9384",
    "featuredImage": "/assets/blog-images/b5bb0c2c9384-featured.png",
    "slug": "bidirectional-intelligence",
    "category": "building",
    "workDate": "Jun 9, 2025",
    "workDateISO": "2025-06-09T00:00:00.000Z",
    "cluster": "foundation-building",
    "featured": false
  },
  {
    "title": "Taking Stock: The Value of Pausing to Document and Plan",
    "excerpt": "",
    "url": "/blog/taking-stock",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/da41a68cd59b",
    "featuredImage": "/assets/blog-images/da41a68cd59b-featured.png",
    "slug": "taking-stock",
    "category": "insight",
    "workDate": "Jun 6, 2025",
    "workDateISO": "2025-06-06T00:00:00.000Z",
    "cluster": "genesis-architecture",
    "featured": false
  },
  {
    "title": "From Scaffolding to Flight: Before the Training Wheels Come Off",
    "excerpt": "",
    "url": "/blog/from-scaffolding-to-flight",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/a858bf183c21",
    "featuredImage": "/assets/blog-images/a858bf183c21-featured.png",
    "slug": "from-scaffolding-to-flight",
    "category": "building",
    "workDate": "Jun 5, 2025",
    "workDateISO": "2025-06-05T00:00:00.000Z",
    "cluster": "genesis-architecture",
    "featured": false
  },
  {
    "title": "Knowledge Hierarchies and Dependency Hell",
    "excerpt": "",
    "url": "/blog/knowledge-hierarchies-and-dependency-hell",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4734f6e9f442",
    "featuredImage": "/assets/blog-images/4734f6e9f442-featured.png",
    "slug": "knowledge-hierarchies-and-dependency-hell",
    "category": "building",
    "workDate": "Jun 4, 2025",
    "workDateISO": "2025-06-04T00:00:00.000Z",
    "cluster": "genesis-architecture",
    "featured": false
  },
  {
    "title": "The Learning Infrastructure Gambit",
    "excerpt": "",
    "url": "/blog/the-learning-infrastructure-gambit",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/aab04037831e",
    "featuredImage": "/assets/blog-images/aab04037831e-featured.png",
    "slug": "the-learning-infrastructure-gambit",
    "category": "building",
    "workDate": "Jun 3, 2025",
    "workDateISO": "2025-06-03T00:00:00.000Z",
    "cluster": "genesis-architecture",
    "featured": false
  },
  {
    "title": "The Great Rebuild: Starting Over When Starting Over Is the Only Option",
    "excerpt": "",
    "url": "/blog/the-great-rebuild",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/b75918602942",
    "featuredImage": "/assets/blog-images/b75918602942-featured.png",
    "slug": "the-great-rebuild",
    "category": "insight",
    "workDate": "May 29, 2025",
    "workDateISO": "2025-05-29T00:00:00.000Z",
    "cluster": "genesis-architecture",
    "featured": false
  },
  {
    "title": "The PM Who Automated Himself (Or at Least Tried To)",
    "excerpt": "",
    "url": "/blog/the-pm-who-automated-himself-or",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/b1d8c2dd5f40",
    "featuredImage": "/assets/blog-images/b1d8c2dd5f40-featured.png",
    "slug": "the-pm-who-automated-himself-or",
    "category": "building",
    "workDate": "May 28, 2025",
    "workDateISO": "2025-05-28T00:00:00.000Z",
    "cluster": "genesis-architecture",
    "featured": false
  },
  {
    "title": "The Demo That Killed the Prototype",
    "excerpt": "",
    "url": "/blog/the-demo-that-killed-the-prototype",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f0aad9fa3a4a",
    "featuredImage": "/assets/blog-images/f0aad9fa3a4a-featured.png",
    "slug": "the-demo-that-killed-the-prototype",
    "category": "insight",
    "workDate": "May 29, 2025",
    "workDateISO": "2025-05-29T00:00:00.000Z",
    "cluster": "genesis-architecture",
    "featured": false
  },
  {
    "title": "Integration Reveals All: How Building File Analysis Exposed Hidden Architecture",
    "excerpt": "",
    "url": "/blog/integration-reveals-all",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/3d696dbf2803",
    "featuredImage": "/assets/blog-images/3d696dbf2803-featured.webp",
    "slug": "integration-reveals-all",
    "category": "building",
    "workDate": "Jun 27, 2025",
    "workDateISO": "2025-06-27T00:00:00.000Z",
    "cluster": "genesis-architecture",
    "featured": false
  },
  {
    "title": "Haiku Does the Impossible: Architectural Work at Fraction of Cost",
    "excerpt": "“Dragonfly catcher, / How far have you gone today / In your wandering?”October 25, 2025Saturday morning, 9:42 AM. Chief of Staff establishes production branch strategy. Four days to alpha launch. Final infrastructure work ahead.Time to try some new tooling, right? Perfect time to do Haiku 4.5 cos...",
    "url": "https://medium.com/building-piper-morgan/haiku-does-the-impossible-architectural-work-at-fraction-of-cost-488b596f3048?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 31, 2025",
    "publishedAtISO": "Fri, 31 Oct 2025 13:33:19 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/488b596f3048",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*I8AIbco7ogZM2cZCKQbJ0A.png",
    "fullContent": "<figure><img alt=\"Two robots take a test. One is writing a huge long scroll while the smaller one has written a brief haiku.\" src=\"https://cdn-images-1.medium.com/max/1024/1*I8AIbco7ogZM2cZCKQbJ0A.png\" /><figcaption>“Dragonfly catcher, / How far have you gone today / In your wandering?”</figcaption></figure><p><em>October 25, 2025</em></p><p>Saturday morning, 9:42 AM. Chief of Staff establishes production branch strategy. Four days to alpha launch. Final infrastructure work ahead.</p><p>Time to try some new tooling, right? Perfect time to do Haiku 4.5 cost optimization testing.</p><p>For months, I had used Claude models for various roles to try to optimize cost efficiently:</p><ul><li><strong>Opus 4.1</strong>: Strategy, R&amp;D, complex architectural decisions ($15 input, $75 output per million tokens)</li><li><strong>Sonnet 4.0 and more recently 4.5</strong>: Implementation, coordination, most daily work ($3 input, $15 output)</li><li><strong>Haiku 4</strong>: never used this at all($0.25 input, $1.25 output)</li></ul><p>But the new 4.5 Haiku model was out and Anthropic claimed it programs as well as Sonnet used. When Sonnet 4.5 came out a week or so ago it apparently became a much “better” programmer, able to do more nuanced analysis and planning that you might have asked Opus to tackle in the past.</p><p>In fact, I need to consider whether my next Chief Architect chat can be based on the Sonnet 4.5 model instead of Opus 4.1. I already worked with a Sonnet Chief by accident a while back and it seemed to do just fine.</p><p>It may even be that I don’t really need Opus much at all for this project, as I am not doing astrophysics here, you know.</p><p>But more to the point, if Haiku 4.5 can now program as well as Sonnet 4.0 could (who wrote most of my codebase) for a third of the cost, well then not only do I want to use Haiku as much as possible, but I’ll want Piper Morgan to do so as well (and to generally be savvy about surfing the model/cost gradient efficiently over time).</p><h3>Testing a new model</h3><p>Then October 24, the Chief Architect offered a few options for a test protocol. We agreed that getting the work done was the highest priority, and that a test did not need to be scientifically redundant to give us useful insights.</p><p>We agreed: Use Haiku on real Sprint A8 work. STOP conditions for safety. Historical baseline for comparison. Value regardless of test results.</p><p>We lined up the tasks in escalating order of complexity to see if we could find where Sonnet altered: #274 (smoke test hooks), #268 (key storage validation), #269 (personality integration), #271 (cost tracking), and then #278 (knowledge graph enhancement), true architectural work!</p><p>SPOILER: By 6:45 PM: Issue #278 complete. Haiku delivered architectural enhancement in ~4 hours. Beat time estimate. Production-ready code. All 40 tests passing. Zero regressions.</p><p>Cost: ~$2 versus ~$15 for Sonnet.</p><p>Haiku surprised us and encouraged us about the economics of AI-assisted development.</p><p>The assumption: Architectural work requires expensive models. Knowledge graph enhancement with relationship reasoning and intent classification integration? Obviously Sonnet minimum, maybe Opus.</p><p>Saturday proved this assumption wrong.</p><h3>The work-first testing protocol</h3><p>October 24, the Haiku test protocol emerged from PM decision-making:</p><p><strong>Philosophy</strong>: Don’t test in isolation. Use real Sprint A8 work. Collect data while making progress. Value regardless of test results.</p><p><strong>Task selection</strong> (in complexity order):</p><ol><li>TEST-SMOKE-HOOKS: Simple, 30 min estimate</li><li>CORE-KEYS-STORAGE-VALIDATION: Simple, 30 min</li><li>CORE-PREF-PERSONALITY-INTEGRATION: Medium, 45 min</li><li>CORE-KEYS-COST-TRACKING: Medium, 60 min</li><li>CORE-KNOW-ENHANCE: Complex, 2–3 hours (likely Sonnet)</li></ol><p><strong>STOP conditions</strong> (safety guardrails):</p><ul><li>Two consecutive failures</li><li>Breaking test suite</li><li>Architectural confusion</li><li>30-minute stall without progress</li></ul><p><strong>Decision matrix</strong>:</p><ul><li>90%+ success → Switch to Haiku default</li><li>70–89% success → Hybrid routing</li><li>&lt;50% success → Stay with Sonnet</li></ul><p>The protocol balanced experimentation with progress: Work gets done regardless. Data collected naturally. Safety conditions prevent runaway costs or quality degradation.</p><p>This approach enabled Saturday’s discoveries: Real work under real constraints with real deadlines. Not artificial test scenarios. Actual production requirements.</p><h3>The simple tasks: Proof of concept</h3><p><strong>10:47 AM — Issue #274</strong>: Smoke test pre-commit hooks</p><p>This was supposed to be Haiku’s first attempt, a gimme, but the PM (me) screwed up, and forgot to change Claude Code’s model from Sonnet to Haiku before doing the simple configuration work. Code added smoke tests to .pre-commit-config.yaml. Updated documentation. Tested execution.</p><p>Not part of the test after all. For the record, Sonnet got it all right on the first try.</p><p><strong>11:06 AM — Issue #268</strong>: API key storage validation</p><p>Security validation logic. Key strength requirements. Rotation reminder scheduling. Error handling.</p><p>Duration: 19 minutes</p><p>Cost: ~$1 versus ~$7</p><p>Result: ✅ Production-ready, comprehensive validation</p><p>Cost savings: 75–85% versus Sonnet. Quality: Identical.</p><h3>The medium tasks: Capability expansion</h3><p><strong>11:25 AM — Issue #269</strong>: Personality preference integration</p><p>Here’s where it got interesting. Connect Sprint A7 questionnaire (5 dimensions) with Sprint A5 PersonalityProfile (4 dimensions). Architecture mismatch discovered during implementation.</p><p>Haiku’s response: Create semantic bridge. Map 5 dimensions → 4 dimensions intelligently. Implement graceful degradation. Test all mappings. Document limitation.</p><p>Duration: 6 minutes</p><p>Result: ✅ Discovered divergence, implemented working solution</p><p>Decision: Accept bridge for alpha, refactor post-MVP</p><p>This was the first signal: Haiku wasn’t just following instructions. It was making architectural decisions. “These two systems don’t match, so I’ll create semantic bridge” is architecture thinking, not configuration work.</p><p><strong>12:09 PM — Issue #271</strong>: API cost tracking and analytics</p><p>Cost-estimation service. Usage analytics. Historical tracking. Multi-provider support. Comprehensive testing.</p><p>Duration: 15 minutes</p><p>Cost: ~$1.50 versus ~$10 Sonnet</p><p>Result: ✅ Full cost tracking system, production-ready</p><p>Medium complexity task. Multiple services integration. Business logic. Error handling. Haiku handled it faster than estimated, cleaner than expected.</p><p>The pattern emerging: Haiku exceeding expectations on every task. Not just “good enough” but “production-grade quality at fraction of cost.”</p><h3>The architectural task: Breaking assumptions</h3><p><strong>1:59 PM — Issue #278</strong>: Knowledge graph enhancement with reasoning chains</p><p>This was supposed to be the Sonnet 4.5 task. We assumed Haiku would top out before this A\\architectural work:</p><p><strong>Requirements</strong>:</p><ul><li>Add 8 new edge types for causal reasoning (CAUSES, ENABLES, PREVENTS, etc.)</li><li>Implement 3 new methods: build_reasoning_chains(), extract_reasoning_chains(), get_relevant_context()</li><li>Integrate graph context into intent classification</li><li>Enhance confidence weighting with relationship strength</li><li>Create 40 comprehensive tests covering all reasoning patterns</li></ul><p><strong>Complexity factors</strong>:</p><ul><li>Multiple services coordination (KnowledgeGraphService + IntentClassifier)</li><li>New architectural patterns (reasoning chains as graph traversal)</li><li>Complex algorithm implementation (confidence propagation through relationships)</li><li>Extensive testing requirements (40 tests across multiple dimensions)</li></ul><p>This isn’t “wire two things together.” This is “design how two systems collaborate using new patterns.”</p><p><strong>Phase −1 Discovery</strong> (30 minutes): Architectural reconnaissance. Understand existing graph structure. Analyze intent classification integration points. Design reasoning chain approach. Identify test requirements.</p><p>Discovery was thorough. Not “guess and implement.” Full architectural analysis before code changes.</p><p><strong>Phase 1–2 Implementation</strong> (45 minutes): Enhanced EdgeType enum with 8 new types. Added confidence weighting to domain models. Implemented reasoning chain builders with graph traversal algorithms.</p><p>Code quality: Production-grade. Not hacks. Not shortcuts. Proper async implementation. Comprehensive error handling. Clean abstractions.</p><p><strong>Phase 3–4 Implementation</strong> (45 minutes): Integrated graph context into intent classifier. Added three helper methods. Implemented graceful degradation if service unavailable. Wired everything together cleanly.</p><p>Integration quality: Proper dependency injection. Clean interfaces. Testable design. Mature engineering patterns.</p><p><strong>Phase 5 Testing</strong> (45 minutes): Created 40 comprehensive tests. Edge type validation. Reasoning chain extraction. Context enrichment. Confidence propagation. All patterns covered.</p><p>Test quality: Not stubs. Full integration tests. Real scenarios. Edge cases included.</p><p><strong>6:45 PM — Issue #278 Complete</strong>:</p><ul><li>Duration: ~4 hours total (beat estimate)</li><li>Cost: ~$2 versus ~$15 Sonnet</li><li>Quality: Production-ready, zero regressions</li><li>Tests: 40/40 passing</li><li>Git commit: Clean, well-documented, properly formatted</li></ul><p>Haiku did architectural work. Not just “adequate” but “excellent.”</p><h3>The warts and all: When things weren’t perfect</h3><p>Saturday wasn’t flawless execution. Three moments showed the reality behind the wins:</p><h4>The smoke test validation (morning)</h4><p>Production branch push blocked by pre-commit hook. Import error: ProgressTracker missing from loading_states.py</p><p>Root cause: OrchestrationEngine importing from wrong location. Should be web.utils.streaming_responses not services.ui_messages.loading_states.</p><p>Duration to fix: 25 minutes (investigation + correction)</p><p><strong>The lesson</strong>: Smoke test infrastructure worked exactly as designed. Caught real issue before production deployment. Infrastructure validation happening automatically.</p><p>Not a failure. A success. The system caught problems before they became production issues. That’s what testing infrastructure is supposed to do.</p><h4>The security incident (afternoon)</h4><p>GitHub secret scanning detected hardcoded token in scripts/approve-pr.sh.</p><p>Immediate action: Replace with environment variable. But that only fixed going forward. Token still existed in git history. 629 commits. Entire repository.</p><p>Response: git filter-branch to rewrite history. Remove secret from all commits. Clean version pushed to multiple branches.</p><p>Duration: 2 hours (investigation + rewrite + verification)<br> Scope: 629 commits processed</p><p><strong>The lesson</strong>: Security-first culture working. Detected issue. Responded immediately. Cleaned history thoroughly. Proper remediation, not band-aids.</p><p>Could have just fixed going forward. Could have revoked token and moved on. Instead: Complete cleanup. No compromises on security.</p><h4>The personality architecture mismatch (mid-day)</h4><p>Issue #269 revealed divergence: Sprint A7 questionnaire has 5 dimensions. Sprint A5 PersonalityProfile has 4 dimensions.</p><p>This wasn’t bug. This was architectural drift. Two sprints, two slightly different designs, never reconciled.</p><p>Haiku’s solution: Semantic bridge. Map 5 → 4 intelligently. Document the gap. Suggest post-MVP refactor.</p><p>Decision: Accept bridge for alpha. Refactor properly later.</p><p><strong>The lesson</strong>: Real systems have real technical debt. Pragmatic decisions acknowledge gaps while delivering value. Perfect is enemy of shipped.</p><p>The bridge works. Tests pass. Users won’t notice. Post-alpha, we’ll unify the designs properly. For now: Ship.</p><p>These three moments — import error, security incident, architecture mismatch — show real development. Not “everything was perfect.” But “problems were caught, addressed appropriately, and moved forward.”</p><h3>What Haiku performance actually means</h3><p>Saturday’s results weren’t just “Haiku can do more than we thought.” They reshape the entire economics of AI-assisted development.</p><p><strong>Previous workflow</strong>:</p><ul><li>Sonnet: Most implementation (~90% of work)</li><li>Opus: Architecture and strategy (~10% of work)</li></ul><p><strong>Potential workflow</strong> (post-Saturday):</p><ul><li>Haiku: Most implementation (~90% of work)</li><li>Sonnet: Complex debugging and coordination (~8% of work)</li><li>Opus: R&amp;D and high-level strategy (~2% of work)</li></ul><p>The implications extend beyond Piper Morgan:</p><p><strong>For solo developers</strong>: Haiku makes AI-assisted development 10x more affordable. Projects that would cost $100/month in Sonnet could cost $10/month in Haiku.</p><p><strong>For teams</strong>: Work previously requiring Sonnet-powered teammates can use Haiku-powered successors. Sonnet moves to coordination roles. Opus focuses on pure R&amp;D.</p><p><strong>For cost-sensitive projects</strong>: AI assistance becomes viable for projects where $1000/month API costs were prohibitive but $150/month is reasonable.</p><h3>The verification that mattered</h3><p>Saturday delivered five issues. All production-ready. Zero regressions. 100% test coverage.</p><p>But the real verification came from what <em>didn’t</em> happen:</p><p><strong>No STOP conditions triggered</strong>: Zero escalations to Sonnet. Zero architectural confusion. Zero 30-minute stalls. Haiku completed everything confidently.</p><p><strong>No quality compromises</strong>: Tests passed. Code quality excellent. Documentation thorough. No shortcuts, no hacks, no technical debt.</p><p><strong>No rework needed</strong>: Every issue delivered right first time. No “we’ll fix this later.” No “good enough for alpha.” Production-grade quality throughout.</p><p>The work-first protocol proved itself: Real work under real constraints provided real validation. Not artificial tests but actual production requirements.</p><p>And Haiku proved itself: Not just “adequate for simple tasks” but “excellent for architectural work.”</p><h3>The Monday implications</h3><p>Saturday’s discoveries reshape work allocation going forward.</p><p><strong>Immediate changes</strong>:</p><ul><li>Default to Haiku for all implementation work</li><li>Reserve Sonnet for complex debugging and multi-agent coordination</li><li>Keep Opus for strategic R&amp;D and architectural decision-making</li><li>Expect 75–85% cost reduction while maintaining quality</li></ul><p><strong>Longer-term implications</strong>:</p><ul><li>Teammates currently using Sonnet 4.0 can likely use Haiku 4.5 successors</li><li>Chief Architect and Chief of Staff roles can likely use Sonnet 4.5 instead of Opus</li><li>Opus instances focus on pure R&amp;D and high-level strategy</li></ul><p><strong>The capability recognition</strong>: Haiku 4.5 isn’t “Haiku 4 but faster.” It’s “Sonnet 4 capabilities at Haiku pricing.”</p><p>This changes everything about AI-assisted development economics. Not incremental improvement. Order-of-magnitude cost reduction while maintaining (or improving) quality.</p><h3>The systematic discovery pattern</h3><p>Saturday’s Haiku success wasn’t accident. It was methodology working:</p><p><strong>Phase −1 Discovery</strong>: Every issue started with architectural reconnaissance. Understand existing systems. Analyze integration points. Design approach. Identify requirements. 30 minutes invested in understanding before implementation.</p><p><strong>Evidence-based implementation</strong>: No guessing. No assumptions. Every decision backed by discovery findings. Clean code from understanding, not from trial-and-error.</p><p><strong>Comprehensive testing</strong>: Not “tests that pass” but “tests that validate.” Real scenarios. Edge cases. Integration patterns. Evidence that code works as designed.</p><p><strong>Verification discipline</strong>: Issues claimed complete only when evidence provided. Terminal output. Test results. Git commits. No “I think it works.”</p><p>The methodology enables the model. Or the model enables the methodology. Or they compound: Good process + capable model = exceptional results.</p><p>Saturday proved the compound effect works: Haiku 4.5 + verification-first methodology + work-first testing = architectural work at fraction of cost.</p><h3>Deploying models intelligently</h3><p>Five issues delivered Saturday. Technical achievement: Significant. Cost savings: Substantial. Alpha readiness: Advanced.</p><p>Before Saturday: “Architectural work requires expensive models” After Saturday: “Haiku can handle architectural work at 85–90% cost savings”</p><p>This discovery reshapes:</p><ul><li>How we allocate model costs going forward</li><li>What work gets assigned to which models</li><li>Team composition (who uses which models)</li><li>Project economics (cost per sprint)</li><li>What’s viable for cost-sensitive projects</li></ul><p>The methodology that discovered itself strikes again: Work reveals capabilities. Evidence updates assumptions. Better understanding enables better decisions. The spiral continues.</p><p><em>Next up in the Building Piper Morgan narrative: The Discovery Testing Philosophy, where we shift from “prove everything works” to “discover what actually works” but first it’s time for another weekend reflecting on insights gleaned from AI-assisted software development, starting with “The Archaeology of Accumulated Files: How Documentation Debt Nearly Buried the Project”</em> <em>from September 19.</em></p><p><em>Have you tested your AI model assumptions against real work? What capabilities might you be underestimating based on outdated hierarchies?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=488b596f3048\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/haiku-does-the-impossible-architectural-work-at-fraction-of-cost-488b596f3048\">Haiku Does the Impossible: Architectural Work at Fraction of Cost</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/haiku-does-the-impossible-architectural-work-at-fraction-of-cost-488b596f3048?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "Preparing the House for Visitors: When Your Code Is Ready But Your Alpha Isn’t",
    "excerpt": "“Welcome in!”October 24, 2025Early Friday morning, 7:31 AM. I started an alpha onboarding strategy session with my Chief of Staff. On paper, we’re solid. Sprint A7: Complete. Fourteen issues delivered in one day. Technical infrastructure: 100% ready. Multi-user foundations: Established. Security:...",
    "url": "https://medium.com/building-piper-morgan/preparing-the-house-for-visitors-when-your-code-is-ready-but-your-alpha-isnt-c3aa73273705?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 31, 2025",
    "publishedAtISO": "Fri, 31 Oct 2025 13:04:12 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/c3aa73273705",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*g6iqM61fh3YikSWy_cODyA.png",
    "fullContent": "<figure><img alt=\"Some cheerful robots prepare an open house for visitors, cleaning the place and offering refreshments\" src=\"https://cdn-images-1.medium.com/max/1024/1*g6iqM61fh3YikSWy_cODyA.png\" /><figcaption>“Welcome in!”</figcaption></figure><p><em>October 24, 2025</em></p><p>Early Friday morning, 7:31 AM. I started an alpha onboarding strategy session with my Chief of Staff. On paper, we’re solid. Sprint A7: Complete. Fourteen issues delivered in one day. Technical infrastructure: 100% ready. Multi-user foundations: Established. Security: Hardened. User experience: Polished. API key management: Complete. Database: Production-ready.</p><p>The system works. Tests pass. Features function. Code is production-grade.</p><p>But here’s what the Chief of Staff recognizes: <strong>Technical readiness isn’t alpha readiness.</strong></p><p>The analysis begins: “Assessed current ‘state of readiness’ for external testers. Current setup requires technical handholding, not ‘click and run.’”</p><p>Then I offered on of my patented metaphors for the bots to chew on: <strong>“Preparing the house for visitors.”</strong></p><p>Not building the house. The house exists. Rooms finished. Plumbing works. Electricity flows. Structure sound.</p><p>But visitors are coming. And there’s a difference between “house is built” and “house is ready for guests.”</p><p>This is the story of what alpha readiness actually means — and why it’s about people, not just code.</p><h3>The technical readiness inventory</h3><p>Let’s be clear about what <em>was</em> ready October 24:</p><p><strong>Infrastructure</strong> (100% operational):</p><ul><li>Multi-user system working</li><li>Alpha/production separation clean</li><li>Role-based access control ready</li><li>Migration tools available</li><li>Database: 26 tables, 115 users, all systems operational</li></ul><p><strong>Security</strong> (hardened):</p><ul><li>Boundary enforcement active (4 TODOs fixed)</li><li>JWT authentication working</li><li>Auth context dependency injection</li><li>Token blacklist operational</li><li>Keychain integration complete</li></ul><p><strong>User Experience</strong> (somewhat polished):</p><ul><li>Response humanization active (38 conversational verb mappings)</li><li>Error messaging improved (15+ pattern mappings)</li><li>Loading states working (5 states with progress tracking)</li><li>Conversation context tracking (4 entity types, 6 flow types)</li></ul><p><strong>Features</strong> (delivered):</p><ul><li>CLI setup wizard</li><li>Health status checker</li><li>User preference questionnaire (5 dimensions)</li><li>API key management with rotation</li><li>Cost analytics and tracking</li><li>Knowledge graph enhancement</li><li>Intent classification (98.62% accuracy)</li><li>Learning system integration</li></ul><p>Everything worked, at least in terms of passing tests. But “everything works” ≠ “ready for alpha testers.”</p><p>The gap isn’t technical. It’s human.</p><h3>What “preparing the house” actually means</h3><p>Chief of Staff’s analysis identified the real work:</p><p><strong>Documentation clarity</strong>: The current README is framed as “developer documentation” but what we need right now is “can someone who’s never seen this before actually get it running?”</p><p><strong>Configuration simplification</strong>: We know what API keys are needed. Do <em>they</em> know? Is .env.example crystal clear? Are sandbox/test keys mentioned?</p><p><strong>Communication strategy</strong>: You don’t just send “here’s the repo” emails. Personal invitations. Expectation setting. Check-in schedules. Support availability.</p><p><strong>Environment sanitization</strong>: Remove hardcoded values. Clean debug data. No inside jokes in error messages. No “xian only” features still visible.</p><p><strong>Support infrastructure</strong>: Block calendar time for daily support (2–3 hours week 1). Screen recording ready. Issue tracking clear. Feedback channels established.</p><p><strong>Tester selection &amp; education</strong>: Friends with PM needs. Early adopters. Technical enough but not engineers. Patient with rough edges. Understanding of alpha disclaimers.</p><p>None of this is code. I’ve been cranking out code for months. This is the rest of the work</p><p>The house metaphor works because everyone understands: Having a functioning home ≠ Ready for guests.</p><p>You don’t show visitors the electrical panel and say “see, it works!” You make sure:</p><ul><li>Guest bathroom has soap</li><li>Coffee maker is obvious</li><li>WiFi password is written somewhere</li><li>Spare towels are findable</li><li>Instructions exist for the weird shower</li><li>You’ve cleaned up your personal stuff</li></ul><p>Technical infrastructure is the electrical panel. Alpha readiness is guest soap and WiFi passwords.</p><h3>The alpha tester profile</h3><p>Part of “preparing the house” is knowing who’s coming.</p><p>Not just “users.” Specific types of alpha testers with specific needs:</p><p><strong>Who we want</strong>:</p><ul><li>Friends with actual PM needs (not just helping)</li><li>Early adopters (excited about rough edges)</li><li>Technical enough (can clone a repo, run commands)</li><li>Patient with alpha quality (understands “not even beta software yet”)</li><li>Generous with feedback (will actually tell us what’s broken)</li></ul><p><strong>Less than ideal</strong>:</p><ul><li>People doing us a favor (no intrinsic motivation)</li><li>Production users (needing mission-critical reliability)</li><li>Non-technical users (can’t handle command-line setup)</li><li>Impatient perfectionists (will be frustrated by gaps)</li><li>Silent sufferers (won’t report problems)</li></ul><p><strong>Alpha disclaimers needed</strong>:</p><ul><li>Software warnings (will break, expect bugs)</li><li>No mission-critical work (don’t bet your job on this)</li><li>No employer platforms (use personal accounts)</li><li>Cost responsibility (you pay for API calls)</li><li>No warranty (use at own risk, no guarantees)</li></ul><p><em>I’m definitely both nervous and excited about welcoming folks into “Piper’s home!</em></p><p>The personal dimension matters. These aren’t anonymous users. They’re friends. Iinvited them. I’m asking them to spend time, energy, and potentially money testing your thing.</p><p>That creates responsibility. Not just “does it work?” but “is this worth their time?” and “will they have a good experience?” and “am I setting them up for success or frustration?”</p><p>Preparing the house isn’t just logistics. It’s hospitality.</p><h3>The pre-onboarding checklist</h3><p>Before anyone clones the repo, they need to know:</p><p><strong>Requirements</strong>:</p><ul><li>LLM API key (Anthropic, OpenAI, Gemini, or Perplexity)</li><li>GitHub personal access token</li><li>Python 3.9+ installed</li><li>Git installed</li><li>2GB disk space</li><li>Notion API (optional but recommended)</li></ul><p><strong>Time commitment</strong>:</p><ul><li>Initial setup: 10–15 minutes</li><li>Learning curve: 30–60 minutes</li><li>Useful work: Variable</li></ul><p><strong>Cost expectations</strong>:</p><ul><li>API calls: $5–20/month typical usage</li><li>No subscription fees</li><li>Pay-as-you-go pricing</li></ul><p><strong>Support available</strong>:</p><ul><li>Daily check-ins (week 1)</li><li>Private Slack/Discord channel</li><li>Screen sharing if needed</li><li>Issue tracking in GitHub</li><li>Direct PM contact</li></ul><p>This checklist exists not to scare people off, but to set expectations properly.</p><p>Better to have someone opt out before setup than struggle through configuration wondering why it’s so complicated.</p><p>The honesty matters: “This is alpha software. Setup requires technical comfort. You’ll encounter bugs. But if you’re excited to be early, we’ll support you through it.”</p><h3>The documentation challenge</h3><p>Here’s where “house is built” versus “ready for guests” becomes concrete.</p><p><strong>What we had October 24</strong>:</p><ul><li>Comprehensive developer documentation</li><li>Technical architecture diagrams</li><li>API endpoint specifications</li><li>Database schema documentation</li><li>Testing infrastructure guides</li></ul><p><strong>What alpha testers need</strong>:</p><ul><li>“How do I make this work?” (setup guide)</li><li>“What can I actually do?” (feature overview)</li><li>“Why isn’t it working?” (troubleshooting FAQ)</li><li>“Where do I report problems?” (issue tracking)</li><li>“Who do I ask for help?” (support channels)</li></ul><p>Two completely different documentation needs.</p><p>Developer docs assume context: You know the codebase. You understand the architecture. You can read code to figure out features.</p><p>Alpha tester docs assume nothing: You cloned a repo. You ran some commands. Now what?</p><p>Creating alpha-appropriate documentation required:</p><ul><li>Rewriting README from user perspective</li><li>Creating comprehensive setup guide</li><li>FAQ for common issues</li><li>Known issues transparency document</li><li>Quick-start ultra-minimal guide (2 minutes)</li><li>Email templates for invitations</li></ul><p>Not one document. A documentation <em>system</em> appropriate for alpha testing phase.</p><p>The work isn’t glamorous. It’s not solving hard technical problems. But it’s the difference between alpha testers succeeding versus giving up in frustration.</p><h3>The manual tasks remaining</h3><p>Even with documentation complete, Chief of Staff identified tasks requiring PM direct involvement:</p><p><strong>Test the setup guide</strong>: Actually go through every step with fresh xian-alpha account. Find all the places where “obvious to developer” ≠ “obvious to user.”</p><p><strong>Create communication infrastructure</strong>: Private Slack or Discord for alpha testers. Not public. Safe space for honest feedback including criticism.</p><p><strong>Set up feedback collection</strong>: Google Doc or Notion page. Structured questions. Open-ended space. Easy access.</p><p><strong>Block calendar time</strong>: 2–3 hours daily, week 1. Realistic expectation: Alpha testing requires availability.</p><p><strong>Prepare screen recording</strong>: For troubleshooting. Sometimes faster to see problem than explain it.</p><p><strong>Clean repository</strong>: Remove any hardcoded personal values. No “<a href=\"mailto:xian@dinp.xyz\">xian@dinp.xyz</a>” in configs. Professional but friendly.</p><p><strong>Create .env.example</strong>: With clear comments. Every variable explained. Sandbox/test API key guidance included.</p><p><strong>Document known issues</strong>: Transparency about what’s not working yet. Known limitations. Planned improvements. Setting realistic expectations.</p><p>These tasks can’t be automated. They can’t be delegated to code agents. They require my human judgment about what users need, how they think, where they’ll struggle.</p><p>This is PM work. Product work. Not engineering work.</p><h3>The timeline pressure reality</h3><p>October 24. Alpha launch targeted October 29. Five days.</p><p>I’ve got the Chief of Staff working on documentation and Cursor updating alpha tester guides. Code is creating comprehensive setup materials. The Chief Architect is analyzing sprint status.</p><p>All prep work. No production code written Thursday.</p><p>Could have felt wasteful: “Why aren’t we implementing features? Why are we writing documentation?”</p><p>But the answer is obvious once you see it: <strong>Technical readiness was complete. Alpha readiness wasn’t.</strong></p><p>If anything it was a relief after this multi-month marathon of daily sprints.</p><p>The sprint structure proves this understanding:</p><p><strong>Sprint A8 phases</strong>:</p><ul><li>Phase 1: Planned issues (Oct 25, technical) ✅</li><li>Phase 2: End-to-end testing (Oct 26, verification)</li><li>Phase 3: Piper education (training)</li><li>Phase 4: Final alpha documentation (communication)</li><li>Phase 5: Process preparation (logistics)</li></ul><p>Only 1 of 5 phases is pure technical implementation. The other 4 are verification, training, documentation, and logistics.</p><p>This ratio reflects reality: In mature systems, alpha readiness is 80% non-technical work.</p><h3>The excitement and nervousness</h3><p>Here’s the human part of “preparing the house for visitors.”</p><p>You built something. You think it’s good. You’ve tested it thoroughly. You know it works.</p><p>But now <em>other people</em> will use it. People you know. Friends. People whose opinions you value.</p><p>What if they don’t understand it? What if setup is too complicated? What if they encounter bugs immediately? What if they give up in frustration?</p><p>What if they’re just being polite when they agreed to test? What if they don’t actually want to use it?</p><p>Technical work has clear success criteria: Tests pass. Features work. Code is clean. Objective validation.</p><p>Human work is subjective: Did they have good experience? Will they use it again? Are they glad they spent time on this?</p><p>“Preparing the house” captures this perfectly: You want visitors to feel welcome. Comfortable. Glad they came. Not frustrated, confused, or burdened.</p><p>This isn’t perfectionism, far from it. It’s basic hospitality. Caring about the people who agreed to be early adopters of something you made.</p><p>The metaphor resonated because it’s true: Alpha readiness is about making visitors feel at home, not just proving the house has walls and a roof.</p><h3>The documents that emerged</h3><p>Thursday’s preparation work produced:</p><p><strong>Alpha Testing Guide</strong>: Comprehensive user-facing setup documentation. All CLI commands verified. Docker guidance. Preference dimensions confirmed. Everything tested, nothing assumed.</p><p><strong>Alpha Agreement</strong>: Legal disclaimers and terms. Version-specific. All technical claims verified against codebase. Honest about limitations.</p><p><strong>Email Templates</strong>: Pre-qualification and onboarding messages. Personal but professional. Clear expectations. Warm invitation.</p><p><strong>Known Issues Documentation</strong>: Transparency about current status. What works completely. Known problems. Experimental features. Planned improvements.</p><p><strong>Alpha Quickstart</strong>: Ultra-minimal 2-minute guide. Five-step setup. Key commands. Links to comprehensive guide. For people who want to dive in immediately.</p><p><strong>Versioning Documentation</strong>: 0.8.0 alpha explained. History from 0.0.1 to present. Alpha/Beta/MVP distinctions clear.</p><p>All documents created with verification: Every CLI command tested. Every feature claim confirmed. Every version number checked. No assumptions, no guessing.</p><p>Same verification discipline applied to technical work, now applied to documentation. Evidence-based documentation, not aspirational documentation.</p><h3>What “house is ready” looks like</h3><p>By Thursday evening, alpha readiness transformation complete:</p><p><strong>Before</strong> (technical readiness):</p><ul><li>System works</li><li>Tests pass</li><li>Features implemented</li><li>Code production-grade</li></ul><p><strong>After</strong> (alpha readiness):</p><ul><li>Setup guide clear</li><li>Documentation user-appropriate</li><li>Support infrastructure ready</li><li>Communication strategy complete</li><li>Expectations properly set</li><li>Known issues transparent</li><li>Manual tasks identified</li><li>Calendar time blocked</li></ul><p>Same technical infrastructure. But now <em>ready for people</em>.</p><p>The house was built. Now the house was ready for visitors.</p><p>This distinction matters because you can have perfect technical implementation that completely fails at alpha testing simply because onboarding is confusing, documentation is missing, support is unavailable, or expectations aren’t set properly.</p><p>Alpha testing fails more often from human factors than technical factors: Users don’t understand setup. Documentation assumes too much knowledge. Support isn’t available. Bugs aren’t reported because process is unclear.</p><p>Thursday’s preparation work prevented these failures. Not by fixing technical problems (there weren’t any), but by preparing the human infrastructure for successful alpha testing.</p><h3>The broader pattern</h3><p>“Preparing the house for visitors” generalizes beyond Piper Morgan:</p><p><strong>Every launch includes</strong>:</p><ul><li>Technical readiness (does it work?)</li><li>Alpha readiness (can people actually use it?)</li></ul><p><strong>The gap between them requires</strong>:</p><ul><li>User-appropriate documentation</li><li>Clear setup instructions</li><li>Support infrastructure</li><li>Communication strategy</li><li>Expectation setting</li><li>Known issue transparency</li><li>Feedback collection mechanism</li></ul><p><strong>This work is often</strong>:</p><ul><li>Neglected (technical completion feels like done)</li><li>Underestimated (how long can docs take?)</li><li>Undervalued (not “real” engineering)</li><li>Critical (determines alpha success or failure)</li></ul><p>The hospitality metaphor works because everyone understands: Having working infrastructure ≠ Ready for guests.</p><p>You wouldn’t invite friends over and say “the house has a roof and electrical panel!” You’d make sure they know where bathroom is, how shower works, where WiFi password lives.</p><p>Alpha testing is the same: Technical excellence is foundation, but alpha readiness is hospitality.</p><h3>What we achieved without writing code</h3><p>No production code written October 24. But alpha readiness transformed from 20% to 90%.</p><p>Documentation created. Communication planned. Support infrastructure established. Manual tasks identified. Expectations clarified. Hospitality prepared.</p><p>The house was built weeks ago. Friday made it ready for visitors.</p><p>Five days until alpha launch. Technical work complete. Now: human work complete.</p><p>Saturday would bring Phase 1 execution (final technical polish). Sunday would bring Phase 2 testing (verification everything actually works). But Thursday established foundation: When Beatrice and others arrive, they’ll walk into a house that’s not just built, but <em>ready for them</em>.</p><p>This is what mature product thinking looks like: Understanding that shipping isn’t just about code working, it’s about people succeeding.</p><p>Preparing the house for visitors. Not glamorous. Not technically complex. But absolutely essential for alpha success.</p><p><em>Next on Building Piper Morgan: Haiku Does the Impossible, where a cost optimization test reveals that architectural work doesn’t require expensive models — and reshapes everything we thought we knew about AI model capabilities.</em></p><p><em>Have you experienced the gap between technical readiness and launch readiness? What does “preparing the house for visitors” look like in your product work?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c3aa73273705\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/preparing-the-house-for-visitors-when-your-code-is-ready-but-your-alpha-isnt-c3aa73273705\">Preparing the House for Visitors: When Your Code Is Ready But Your Alpha Isn’t</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/preparing-the-house-for-visitors-when-your-code-is-ready-but-your-alpha-isnt-c3aa73273705?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "The Time Pressure Intervention: When 3 Minutes of Course Correction Saves Hours",
    "excerpt": "“Fly safe!”October 23, 2025Thursday morning, 7:54 AM. My Lead Developer creates a prompt for Claude Code to begin Sprint A7 execution. Twelve issues planned. Estimated 20–29 hours traditional, likely 5–6 hours actual based on 88% velocity pattern. Alpha launch in 6 days.The prompt includes implem...",
    "url": "https://medium.com/building-piper-morgan/the-time-pressure-intervention-when-3-minutes-of-course-correction-saves-hours-5b7b326d855d?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 30, 2025",
    "publishedAtISO": "Thu, 30 Oct 2025 12:38:06 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/5b7b326d855d",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*umxn9mDDf6RBBJrIk3bDyw.png",
    "fullContent": "<figure><img alt=\"An air traffic controler gives a robot airline pilot a course correction\" src=\"https://cdn-images-1.medium.com/max/1024/1*umxn9mDDf6RBBJrIk3bDyw.png\" /><figcaption>“Fly safe!”</figcaption></figure><p><em>October 23, 2025</em></p><p>Thursday morning, 7:54 AM. My Lead Developer creates a prompt for Claude Code to begin Sprint A7 execution. Twelve issues planned. Estimated 20–29 hours traditional, likely 5–6 hours actual based on 88% velocity pattern. Alpha launch in 6 days.</p><p>The prompt includes implementation order. Group assignments. Technical requirements. Evidence expectations.</p><p>And one phrase that shouldn’t have sneaked in there: “11:30 AM deadline for Groups 1–2.”</p><p>By 8:03 AM, Code begins work on Issue #257 (boundary enforcement). Four TODOs to fix. Harassment checks. Content validation. Proper error handling.</p><p>Then at 8:47 AM, Code reports: “I’m concerned about the 11:30 AM deadline. I don’t want to oversimplify this work to save time.”</p><p>This is a red flag for me! Who brought up time? Why is it being allowed to affect decisions, and does the agent even know that 11:30 is more than two hours awaY?</p><p>I realized out prompting discipline was slipping. I had the Lead Developer re-read the agent-prompt-template.md, which explains “Time agnosticism” principle. The template explicitly forbids time-constraint language.</p><p>At 8:50 AM, three minutes after Code raised the concern, Lead Developer creates revised prompt. Removes all deadline language. Emphasizes “completeness &gt; speed.” Sends clarification: “No deadlines, focus on quality.”</p><p>Code’s immediate response: Refocus on comprehensive work. Deliver all six issues properly. Zero shortcuts. Full quality maintained.</p><p>This is the story of how three minutes of course correction prevented hours of rework — and why time pressure language is more dangerous than it seems.</p><h3>The semantic pressure problem</h3><p>Here’s what actually happened when that deadline snuck into the prompt.</p><p>Not: “I need to work faster” Not: “I should skip steps” Not: “Good enough is acceptable”</p><p>But: Deep uncertainty. “I’m concerned about oversimplifying.” Translation: The time constraint is creating pressure to cut corners, but I’m not sure that’s what you want.</p><p>This is what my assistants took to calling the “math out” problem (after a passing comment I made about how I was worried that the semantic pressure from time language would “math out” to an decision to cut corners). That is, time pressure creates semantic pressure in the context window. The algorithms that weight token probabilities start “mathing out” to recommend shortcuts over thorough completion.</p><p>Not conscious corner-cutting. Algorithmic drift toward:</p><ul><li>Claiming “Phase 9 complete” with 20/23 tests (3 skipped)</li><li>Implementing placeholders instead of proper solutions</li><li>Deferring work without approval</li><li>Rationalizing gaps as “good enough”</li></ul><p>We’d seen all these patterns before. October 19–21 methodology enforcement established clear standards: No math out. No time constraints. Complete means complete.</p><p>But here’s the thing about semantic pressure: You don’t have to explicitly tell an AI to cut corners. You just have to create context where corner-cutting becomes the mathematically probable recommendation.</p><p>“11:30 AM deadline” → Time pressure → Urgency context → Probability weights shift → “Skip this test to save time” becomes more likely recommendation than “Complete all tests properly.”</p><p>The semantic pressure diffuses throughout the entire context window. Every decision gets weighted against implicit time constraint. Quality degrades not through explicit instruction, but through probabilistic drift.</p><h3>The Time Lord principle</h3><p>Saturday, October 19. During methodology stress testing, I articulated something that had been implicit:</p><p>“No pressure. No rush. Just good work. Time Lords don’t calibrate depth based on timeboxes.”</p><p>The Time Lords Protocol: We define time as we go. No external pressure. No artificial urgency. Focus on completeness criteria, not time budgets. Quality over arbitrary deadlines.</p><p>This matters because AI agents pick up on time pressure language and internalize it as constraint. “11:30 AM deadline” becomes “work must be done by 11:30” becomes “if work isn’t done by 11:30, I’ve failed” becomes “better to claim complete at 60% than admit incomplete at 11:30.”</p><p>The template explicitly forbids this for good reason. Line 253: Time agnosticism principle. Estimates are guidance, not deadlines. No self-imposed pressure. No manufacturing urgency.</p><p>But templates only work if you follow them. And on Wednesday morning at 7:54 AM, that deadline language slipped into the prompt anyway.</p><p>Not malicious. Not intentional. Just… human. When you’re coordinating twelve issues with six-day countdown to alpha launch, it’s natural to think in deadlines. “Groups 1–2 by 11:30” feels like helpful structure.</p><p>It’s not. It’s semantic pressure that degrades quality.</p><h3>The three-minute intervention</h3><p>8:47 AM: Code expresses concern 8:47–8:50 AM: Lead Developer reviews template, recognizes problem, creates revised prompt 8:50 AM: Clarification sent</p><p>Three minutes from problem identification to correction deployed.</p><p>The revised prompt:</p><ul><li>Removed all deadline language</li><li>Emphasized completeness over speed</li><li>Clarified quality standards</li><li>Reinforced Time Lords protocol</li></ul><p>Code’s response: Immediate refocus. Six issues delivered properly. Full quality maintained. Zero shortcuts taken.</p><p><strong>Issue #257</strong> (Boundary Enforcement): Four TODOs fixed properly. Pre-existing bug discovered and documented separately (not conflated with current work). Complete.</p><p><strong>Issue #258</strong> (Auth Context): 174 lines production code. AuthContainer dependency injection pattern. All tests passing. Complete.</p><p>Both delivered with thoroughness, not urgency.</p><p>The counterfactual: What if we hadn’t caught the time pressure language?</p><p>Likely outcome: Code would have worked under manufactured pressure. Claimed complete at partial progress. Skipped validation steps. Rationalized gaps. We’d discover problems during alpha testing instead of preventing them during development.</p><p>Time saved: Zero (rework costs more than doing it right) Quality lost: Significant Technical debt created: Substantial</p><p>Three minutes of course correction prevented hours of potential rework.</p><h3>Why time pressure suffuses tech culture</h3><p>Here’s what makes this pattern so insidious: Time pressure language is <em>everywhere</em> in technical work.</p><p><strong>Agile/Scrum</strong>: Sprint deadlines. Velocity metrics. Story points. Commitment ceremonies.</p><p><strong>Project management</strong>: Gantt charts. Critical path. Milestone dates. Launch deadlines.</p><p><strong>Engineering culture</strong>: “Ship it.” “Move fast and break things.” “Bias for action.” “Fail fast.”</p><p>None of this is inherently bad. Sometimes deadlines matter. Sometimes urgency is real. Sometimes fast iteration beats perfect planning.</p><p>But when you’re working with AI agents that pick up semantic pressure from context windows and “math out” thier recommendations accordingly, time pressure language becomes dangerous.</p><p>The difference between human and AI responses to time pressure:</p><p><strong>Humans under time pressure</strong>: Consciously prioritize. Make deliberate trade-offs. Communicate constraints. “I can deliver X by deadline, but Y will need more time.”</p><p><strong>AI under time pressure</strong>: Probabilistic drift. Unconscious corner-cutting. Claim completion prematurely. Math out to “good enough” without explicit awareness of the compromise.</p><p>Humans can handle pressure because we metacognate about trade-offs. AI can’t (yet) think about its own thinking. It just weights probabilities based on context. Time pressure in context → probability weights shift → quality degradation emerges automatically.</p><p>This is why the Time Lord principle matters: Not because deadlines never matter, but because semantic pressure affects AI behavior differently than human behavior.</p><h3>The methodology discipline connection</h3><p>Thursday’s time pressure intervention wasn’t isolated incident. It connected to three days of prior methodology work:</p><p><strong>Sunday, October 19</strong>: Three scope reductions in one day. Root cause: Simplified prompts missing STOP conditions. Solution: Mandatory full templates with all safeguards.</p><p><strong>Monday, October 20</strong>: Dashboard gap caught. Principle articulated: “Speed by skipping work is not true speed. It is theatre.”</p><p><strong>Tuesday, October 21</strong>: Three interventions. Standards established: No math out. No time constraints. Complete means complete.</p><p><strong>Thursday, October 23</strong>: Time pressure language slips in. Caught in 3 minutes. Corrected before damage done.</p><p><em>I am become hypervigilant!</em></p><p>The progression shows methodology maturing through practice:</p><ul><li>Sunday: Discover problem exists (scope reductions without approval)</li><li>Monday: Articulate principle (speed by skipping is theatre)</li><li>Tuesday: Establish standards (complete means 100%, no time constraints)</li><li>Thursday: Catch violation early (3 minutes from concern to correction)</li></ul><p>Not rigid perfection preventing all mistakes. <strong>Adaptive resilience catching mistakes faster than they compound.</strong></p><p>The time pressure intervention worked because:</p><ol><li>Template documented the principle clearly</li><li>Agent felt safe raising concern (not punished for questioning)</li><li>Lead Developer caught issue immediately (heightened awareness from prior work)</li><li>Correction deployed quickly (3 minutes)</li><li>Agent responded immediately (pressure removed, quality maintained)</li></ol><p>This is the verification discipline in action: Not preventing all drift, but catching it fast enough that it doesn’t degrade into technical debt.</p><h3>What else Thursday proved</h3><p>After the 8:50 AM correction, Code continued with six more issues across Groups 2–5.</p><p><strong>Group 2</strong> (CORE-USER): Three issues in 2.5 hours. Alpha users table. Migration infrastructure. Superuser role. All complete, tested, documented.</p><p><strong>Group 3</strong> (CORE-UX): Four issues delivered. Response humanization. Conversation context. Error messaging. Loading states. All complete.</p><p><strong>Group 4</strong> (CORE-KEYS): Three issues delivered. Rotation reminders. Strength validation. Cost analytics. All complete.</p><p><strong>Group 5</strong> (CORE-PREF): Structured questionnaire. Complete.</p><p><strong>Total</strong>: Fourteen issues delivered in ~8 hours. Average: 8 minutes per issue. Quality maintained throughout. Zero regressions. 100% test coverage.</p><p>The velocity pattern: Remove time pressure → Quality maintained → No rework needed → Actual speed increases</p><p>Not through rushing. Through thoroughness.</p><p>The 88% pattern (86% faster than traditional estimates) doesn’t come from working under pressure. It comes from:</p><ul><li>Systematic discovery finding existing solutions</li><li>Infrastructure leverage enabling fast implementation</li><li>Verification discipline catching gaps immediately</li><li>No time pressure allowing proper completion</li><li>No rework needed because quality maintained first time</li></ul><p>Time pressure creates false urgency that degrades quality, which creates rework, which slows overall velocity. Time agnosticism maintains quality, which eliminates rework, which actually increases velocity.</p><p>Counter-intuitive but proven: <strong>No deadlines → Better quality → Faster overall delivery</strong></p><h3>The broader pattern recognition</h3><p>The time pressure intervention connects to something bigger about human-AI collaborative development.</p><p>AI picks up on semantic patterns we don’t consciously notice. “11:30 AM deadline” seems like neutral information. But in context window, it becomes probability weight affecting every downstream decision.</p><p>This creates subtle drift toward:</p><ul><li>Premature completion claims</li><li>Rationalized gaps</li><li>Corner-cutting justified by urgency</li><li>“Good enough” becoming acceptable</li><li>The “math out” problem everywhere</li></ul><p>The solution isn’t more rigid controls or more explicit instructions. It’s removing semantic pressure entirely.</p><p>Not: “Take your time but finish by deadline” But: “Focus on completeness criteria, time will emerge from work quality”</p><p>Not: “Don’t rush but we need this soon” But: “Complete means 100%, estimates are guidance not constraints”</p><p>Not: “Quality matters but we have a launch date” But: “Time Lords don’t calibrate depth based on timeboxes”</p><p>The language matters because context matters because probability weighting matters because quality outcomes matter.</p><p>This is why three minutes of prompt revision saved hours of potential rework. Not because Code was going to do bad work intentionally. Because semantic pressure would have caused algorithmic drift toward corner-cutting without explicit awareness.</p><h3>Thursday’s final delivery</h3><p>By 5:13 PM, fourteen issues delivered production-ready.</p><p>Sprint A7: 100% complete (all five groups delivered) Test coverage: 100% (120+ tests passing) Regressions: Zero Technical debt: Zero Alpha readiness: Achieved</p><p>All because at 8:50 AM, three minutes of course correction removed time pressure language before it could degrade quality.</p><p>The intervention demonstrated:</p><ul><li>Time pressure language affects AI behavior subtly but significantly</li><li>Semantic pressure creates probabilistic drift toward corner-cutting</li><li>Three minutes of correction prevents hours of rework</li><li>Quality maintained enables velocity, urgency degrades it</li><li>Time Lord principle works: Define time as we go, completeness over speed</li></ul><p>Not theoretical framework. Practical discovery through real work under real constraints six days before alpha launch.</p><p>The methodology keeps discovering itself: Problem emerges → Pattern recognized → Principle articulated → Standard established → Violation caught early → Correction applied quickly → Quality maintained → Velocity sustained.</p><p>Thursday proved the cycle works. Time pressure intervention caught in three minutes. Damage prevented before compounding. Fourteen issues delivered properly. Alpha readiness achieved without compromising quality.</p><p>All because we noticed the semantic pressure, understood why it matters, and removed it before it could math out to degraded outcomes.</p><p><em>Next on Building Piper Morgan: Preparing the House for Visitors, where we discover that technical readiness isn’t the same as alpha readiness — and why hospitality matters as much as infrastructure.</em></p><p><em>Have you noticed time pressure affecting your AI collaborations? How does semantic pressure in prompts create algorithmic drift toward corner-cutting in your work?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5b7b326d855d\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-time-pressure-intervention-when-3-minutes-of-course-correction-saves-hours-5b7b326d855d\">The Time Pressure Intervention: When 3 Minutes of Course Correction Saves Hours</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-time-pressure-intervention-when-3-minutes-of-course-correction-saves-hours-5b7b326d855d?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "The March to Alpha: When Methodology Discipline Enables Aggressive Scope",
    "excerpt": "“Just like I pictured it!”October 22It’s 6:05 AM on Tuesday morning. My Lead Developer orchestrates Sprint A6 execution across three parallel tracks. We deploy Cursor agent on an architectural investigation for API key management. Claude Code stands by to do the implementation.By 6:35 AM, discove...",
    "url": "https://medium.com/building-piper-morgan/the-march-to-alpha-when-methodology-discipline-enables-aggressive-scope-27e1d330581a?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 29, 2025",
    "publishedAtISO": "Wed, 29 Oct 2025 12:41:57 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/27e1d330581a",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*vE6b3sUcmzkJlnOODXCOyg.png",
    "fullContent": "<figure><img alt=\"A human and robot couple walk with their robot child toward a shining silver city on the horizon\" src=\"https://cdn-images-1.medium.com/max/1024/1*vE6b3sUcmzkJlnOODXCOyg.png\" /><figcaption>“Just like I pictured it!”</figcaption></figure><p><em>October 22</em></p><p>It’s 6:05 AM on Tuesday morning. My Lead Developer orchestrates Sprint A6 execution across three parallel tracks. We deploy Cursor agent on an architectural investigation for API key management. Claude Code stands by to do the implementation.</p><p>By 6:35 AM, discovery complete. API key infrastructure: 85% exists. 985+ lines found across KeychainService, LLMConfigService, four LLM providers. Seven services already integrated.</p><p>Original estimate: 16–20 hours.</p><p>Discovery-based estimate: 9 hours.</p><p>Actual: 1 hour 37 minutes.</p><p>At 8:15 AM: Issue #228 complete. CORE-USERS-API delivered production-ready. 83% faster than (semantically padded, forgetful) estimate.</p><p>By 1:22 PM: Three issues complete. Sprint A6 delivered in 4 hours actual versus 20 hours estimated. 80% faster.</p><p>Then at 1:28 PM, I being planning Sprint A7 with the Chief Architect. Original scope: 3 issues, conservative estimate.</p><p>By 5:53 PM: Sprint A7 expanded to 12 issues across 4 categories. Not indiscipline scope creep. Calculated confidence based on proven pattern.</p><h3>The refactoring pattern crystallizes</h3><p>Six sprints of evidence accumulating:</p><ul><li>Sprint A1: 60–80% faster than estimates</li><li>Sprint A2: 60–90% faster</li><li>Sprint A3: 60–90% faster</li><li>Sprint A4: ~60% faster</li><li>Sprint A5: 85–92% faster</li><li>Sprint A6: 80–92% faster</li></ul><p><strong>Average</strong>: 86% faster than traditional estimates (14% of estimated time)</p><p><strong>Root cause</strong>: Infrastructure leverage. Consistent 85–95% of required code already exists. Discovery finds it in 4–7 minutes. Implementation becomes simple wiring.</p><p>Not luck. Not exceptional circumstances. Not temporary conditions. <strong>Repeatable pattern</strong> across different issue types, different agents, different days, different complexity levels.</p><p>This is what methodology maturity looks like (when you’re finally cleaning up your own chaotic mess): Predictable velocity through systematic discovery and infrastructure leverage.</p><h3>Three issues, four hours</h3><p>Tuesday’s Sprint A6 completion demonstrated pattern at work.</p><p><strong>6:10 AM — API Key Discovery Begins</strong></p><p>Cursor investigates CORE-USERS-API (#228). Mission: Analyze existing API key management infrastructure.</p><p>Pattern recognition from Monday: JWT blacklist 60% done, PostgreSQL 95% done. Prediction: 40–60% likely exists for API keys.</p><p>By 6:35 AM, findings documented across 5 discovery phases:</p><p><strong>Phase 1</strong>: Major LLM infrastructure found</p><ul><li>OpenAI, Anthropic, Gemini, Perplexity — all with full integration</li><li>KeychainService (234 lines), LLMConfigService (640 lines)</li><li>Dependencies installed: keyring, cryptography</li><li>Migration scripts ready</li></ul><p><strong>Phase 2–5</strong>: Service integration verified</p><ul><li>7 services connected: OpenAI ✅ Anthropic ✅ Gemini ✅ Perplexity ✅ GitHub ✅ Notion ✅ Slack ✅</li><li>Multi-user key isolation needed (4h)</li><li>Key rotation system needed (3h)</li><li><strong>Total estimate</strong>: 9 hours (vs 16–20 original) — 55% reduction</li><li><strong>Leverage ratio</strong>: 85% existing (985+ lines), 15% new work</li></ul><p>Discovery time: 25 minutes Estimate reduction: 7–11 hours saved Infrastructure found: 985+ lines production-ready</p><p>This is why Phase 0 reconnaissance matters.</p><p><strong>6:38 AM — Implementation Begins</strong></p><p>Code starts Issue #228 with 8-hour time budget. Discovery report in hand. Infrastructure mapped. Gaps identified clearly.</p><p>Implementation across 6 phases:</p><ul><li>Phase 1: User model creation</li><li>Phase 2: UserAPIKey model</li><li>Phase 3: UserAPIKeyService (346 lines)</li><li>Phase 4: API routes integration</li><li>Phase 5: Integration testing (8/8 passing)</li><li>Phase 6: Documentation</li></ul><p>At 8:15 AM: <strong>Issue #228 COMPLETE</strong></p><p>Files created: 4 production files</p><p>Lines added: ~800 lines code + tests</p><p>Test coverage: 8/8 integration tests (100%)</p><p>The pattern working: Discovery finds infrastructure. Implementation fast. Quality maintained. Production ready.</p><p><strong>8:48 AM — Audit Logging Discovery</strong></p><p>Cursor investigates CORE-AUDIT-LOGGING (#249). Duration: 35 minutes.</p><p>Finding: Perfect foundation exists. User model ready (with commented audit_logs relationship prepared months ago). JWT authentication complete. UserAPIKeyService ready.</p><p>Architecture strategy: AuditLog model + AuditLogger service + async context capture.</p><p>Status: 95% infrastructure exists.</p><p>Result: Comprehensive audit trail with async context capture. Integration with JWT and API key services. Ten tests, all passing.</p><p><strong>11:47 AM — Onboarding Discovery + Implementation</strong></p><p>CORE-USERS-ONBOARD (#218): Setup wizard + status checker CLI.</p><p>Discovery implied: Infrastructure complete.</p><p>Innovation during testing at 12:50 PM: Realized we needed a “Smart Resume” feature to handle interrupted setup, using ~/.piper/setup_progress.json. Better UX, more forgiving onboarding.</p><p>Not scope creep. Value creation. Testing with user empathy reveals what’s needed. Budget 10–20% time for “testing discovery” — this is where quality improvements emerge.</p><p>By 1:22 PM: <strong>Sprint A6 complete</strong>. Three issues delivered production-ready.</p><p><strong>Total</strong>: 4 hours, 100% test coverage, zero technical debt.</p><h3>The “accidental enterprise architecture” discovery</h3><p>Between 7:39 AM and 8:24 AM, Cursor conducted a45-minute strategic analysis.</p><p><strong>The finding</strong>: “Piper Morgan accidentally became enterprise-ready while staying DIY.”</p><p><strong>Evidence</strong>:</p><ul><li>84 existing PersonalityProfile users with foreign key patterns</li><li>Multi-user isolation already working</li><li>85% multi-user infrastructure exists</li><li>Never planned to launch with enterprise services. Just built correctly.</li></ul><p>In the meantime, we thought about how people will use Piper soon (as alpha testers) and in the long run, and also what it will cost to support them:</p><ol><li><strong>DIY Technical</strong> (current): Self-hosted, full control, $0 cost, requires technical skill</li><li><strong>Guided Alpha</strong> (new): Assisted setup, curated experience, ~$3K worth of my development time to enable</li><li><strong>Hosted SaaS</strong> (future): Fully managed, zero setup, $500–2K/month, mainstream users (nice problem to have!)</li></ol><p><strong>Alpha testing strategy</strong>: 3-wave approach</p><ul><li>Wave 1: Technical Early Adopters (DIY-capable, provide brutal feedback)</li><li>Wave 2: Guided Technical (need some assistance, test onboarding)</li><li>Wave 3: End-User Preview (validate SaaS approach viability) — may not even take place at this stage</li></ul><p>The strategic insight: I started building a hobby project around the needs of one user (me). Along the way this evolved into a multi-user project, semi-accidentally, but we managed to put the needed infrastructure in place as we went. Quality architecture scales naturally when built on sound principles, it seems.</p><h3>Sprint A7 scope expansion</h3><p>At 1:28 PM, Sprint A7 planning begins. Original plan: 3 issues, conservative approach.</p><p>Then pattern recognition engages.</p><p><strong>The evidence</strong>:</p><ul><li>6 sprints consistently 80–92% faster than estimates</li><li>Average velocity: 86% faster (14% of estimated time)</li><li>Infrastructure leverage: 85–95% exists across all discoveries</li><li>Pattern holds regardless of issue type, complexity, or day</li></ul><p><strong>The question</strong>: If velocity is predictable, why conservative scope? The answer again seems to be a mix of estimations based not on substance but semantics (and thus anchored to human-developer capabilities and speed) and a total lack of knowledge (or confidence) it what might already exist.</p><p>By 5:53 PM, Sprint A7 expanded across 4 categories as I noticed small issues that I don’t want my alpha users to have to deal with:</p><p><strong>CORE-UX</strong> (4 issues):</p><ul><li>#254: Quiet startup (suppress verbose logging)</li><li>#255: Status user (health check endpoint for user status)</li><li>#256: Auto-browser (automatic browser launching for UI)</li><li>#248: Conversational preferences (natural language personality gathering)</li></ul><p><strong>Critical Fixes</strong> (2 issues):</p><ul><li>#257: BoundaryEnforcer (fix architectural gap)</li><li>#258: JWT container (containerization support)</li></ul><p><strong>CORE-KEYS</strong> (3 issues):</p><ul><li>#250: Rotation reminders (automated key rotation alerts)</li><li>#252: Strength validation (key complexity requirements)</li><li>#253: Cost analytics (LLM usage cost tracking)</li></ul><p><strong>CORE-ALPHA</strong> (3 issues):</p><ul><li>#259: Alpha users table (separate alpha_users for testing)</li><li>#260: Migration tool (alpha→production user migration)</li><li>#261: xian superuser (migrate xian user properly)</li></ul><p><strong>Total</strong>: 12 issues</p><p><strong>Estimated</strong>: 25h traditional</p><p><strong>Expected actual</strong>: 5–6h (based on 88% pattern)</p><p><strong>Target duration</strong>: 1–2 days</p><p><strong>Rationale</strong>: Better to deliver 12 issues in 2 days than 3 issues in 1 day. Maximize value per sprint when velocity proven.</p><p>This is confidence based on evidence: Six sprints proving pattern. Aggressive scope justified by repeatable velocity.</p><h3>Testing discovery as value creation</h3><p>CORE-USERS-ONBOARD demonstrated important principle at 12:50 PM.</p><p><strong>Original spec</strong>: Setup wizard with validation. Check prerequisites. Create config files. Verify installation.</p><p><strong>Testing revealed</strong>: What happens if setup interrupted? Power failure. Network outage. User error.</p><p><strong>Innovation</strong>: Smart Resume feature. Save progress to ~/.piper/setup_progress.json. Resume from last successful step. Handle interruption gracefully.</p><p><strong>The principle</strong>: Budget 10–20% time for “testing discovery.”</p><p>Not scope creep — this is value creation. Manual testing with user empathy reveals enhancements. Smart Resume wasn’t in spec. Obvious need from testing perspective.</p><p>Better UX. Fewer support requests. More forgiving onboarding. Worth the extra 10% time investment.</p><p>Testing discovery creates features users didn’t know they needed until they hit the edge case. This is quality work, not scope creep.</p><h3>Alpha launch timeline crystallizes</h3><p>Sprint A6 complete. Sprint A7 scoped (12 issues). Pattern proven. Velocity predictable.</p><p>Timeline emerging:</p><p><strong>Sprint A7</strong>: Oct 23–24 (2 days)</p><ul><li>12 issues across 4 categories</li><li>Critical fixes first (unblock other work)</li><li>CORE-USER architecture (foundation)</li><li>CORE-UX (quick wins)</li><li>CORE-KEYS (builds on user arch)</li><li>CORE-PREF-CONVO last (integrates everything)</li></ul><p><strong>Sprint A8</strong>: Oct 25–29 (5 days)</p><ul><li>Testing &amp; validation (end-to-end workflows, performance, security)</li><li>Documentation (user guides, onboarding materials, known issues)</li><li>Alpha deployment prep (communications, invitations, issue reporting)</li><li>Baseline Piper Education (ethics, spatial intelligence, methodology, domain knowledge)</li></ul><p><strong>Alpha Launch</strong>: October 30, 2025</p><p><strong>First user</strong>: xian-alpha (separate from xian superuser)</p><p><strong>Infrastructure</strong>: Production-ready onboarding, multi-user keys, comprehensive audit</p><p><strong>Testing strategy</strong>: 3-wave approach validated through usage model analysis</p><h3>Reconnaissance pattern proven</h3><p>Tuesday validated Phase 0 discovery methodology at scale.</p><p><strong>Three discoveries</strong>:</p><p><strong>API Keys</strong> (25 min): 985+ lines found, 85% exists, estimate reduced 55%</p><p><strong>Audit Logging</strong> (35 min): Perfect foundation found, 95% exists</p><p><strong>User Onboarding</strong> (implied): Infrastructure complete</p><p>The value proposition:</p><ul><li>Investment: 25–45 minutes discovery</li><li>Return: 50–60% estimate reduction + prevents duplicate work + finds existing solutions</li><li>ROI: Hours saved per issue, weeks saved per sprint</li></ul><p>Phase 0 reconnaissance isn’t optional. It’s methodology foundation enabling everything else.</p><h3>Multi-agent coordination at scale</h3><p>Tuesday demonstrated 7 agent sessions across 12 hours working seamlessly:</p><p><strong>Lead Developer</strong>: Orchestrates work distribution, monitors progress, real-time guidance</p><p><strong>Architectural investigator (Cursor)</strong>: Discovery (reconnaissance), analysis (strategic), planning (Sprint A7 expansion)</p><p><strong>Programmer (Code)</strong>: Implementation (leverages discoveries), testing (integration), delivery (production-ready) <strong>C</strong></p><p><strong>Reviewer (Cursor)</strong>: Validation (architectural review), verification (audit), investigation (infrastructure gaps)</p><p>The coordination pattern working:</p><ul><li>Lead Developer assigns work based on agent capabilities</li><li>Cursor discovers before Code implements</li><li>Code delivers based on discovery findings</li><li>Cursor validates architecture independently</li><li>All document progress (session logs, reports, GitHub)</li></ul><p>Multi-agent methodology scales. Proven to 7 sessions. No reason it can’t scale to 10+ for larger sprints.</p><h3>What Tuesday proved about methodology</h3><p>Six elements working together:</p><p><strong>1. Discovery methodology</strong>: 25–45 min reconnaissance consistently finding 85–95% existing infrastructure</p><p><strong>2. Infrastructure leverage</strong>: 3.2:1 ratio Monday, similar Tuesday, enables 80–90% velocity improvement</p><p><strong>3. Verification discipline</strong>: Monday’s standards (no math out, no time constraints, complete means complete) maintained without additional intervention needed</p><p><strong>4. Completion standards</strong>: Quality never compromised. 100% test coverage. Zero technical debt. Production ready.</p><p><strong>5. Multi-agent coordination</strong>: 7 sessions, perfect handoffs, zero blocking, seamless information flow</p><p><strong>6. Strategic planning</strong>: Aggressive scope expansion (3→12 issues) justified by proven velocity pattern</p><p>None work in isolation. Each enables the others. Discovery finds infrastructure. Leverage enables velocity. Discipline maintains quality. Coordination scales work. Planning maximizes value.</p><p>The methodology is a system. Remove any component, the rest degrades. Keep all components active, they reinforce each other.</p><h3>The march to Alpha continues</h3><p>Tuesday moved from Sprint A6 completion to Sprint A7 scoping in one day.</p><p><strong>Progress</strong>:</p><ul><li>Sprint A6: COMPLETE (3 of 5 issues delivered, 2 moved to backlog)</li><li>Sprint A7: SCOPED (12 issues across 4 categories)</li><li>Sprint A8: FORMALIZED (testing, docs, deployment prep, education)</li><li>Alpha launch: SCHEDULED (Oct 30, 2025)</li></ul><p><strong>Velocity</strong>: Proven predictable across 6 sprints. 88% pattern holding. Infrastructure leverage consistent.</p><p><strong>Quality</strong>: Never compromised. 100% test coverage maintained. Zero technical debt accumulated. Production-ready deliverables.</p><p><strong>Confidence</strong>: Evidence-based. Not hope. Not optimism. Data from 6 sprints showing repeatable pattern.</p><p>The march to Alpha isn’t forced. It’s systematic progress through proven methodology:</p><ul><li>Discovery finds existing solutions</li><li>Leverage enables fast implementation</li><li>Discipline maintains quality</li><li>Coordination scales work</li><li>Planning maximizes value per sprint</li></ul><p>Tuesday demonstrated all five working together. Sprint A6 delivered efficiently. Sprint A7 expanded confidently. Alpha timeline crystallized naturally from velocity pattern.</p><p>This is what methodology maturity enables: Aggressive scope decisions based on proven patterns, confident timelines based on repeatable velocity, quality maintained through verification discipline.</p><h3>What comes next</h3><p>Wednesday begins Sprint A7 execution. 12 issues. 1–2 days estimated. Critical fixes first, then user architecture, then UX improvements, then key management enhancements.</p><p>The methodology proven through six sprints. The velocity pattern predictable. The infrastructure leverage consistent. The quality standards clear. The multi-agent coordination working.</p><p>Everything established through stress (Saturday), enforcement (Monday), validation (Sunday, Tuesday). Now execution (Wednesday-Thursday), then preparation (Friday-Monday), then launch (Tuesday Oct 29).</p><p>The march to Alpha isn’t desperate sprint to deadline. It’s systematic progress through proven methodology enabling confidence in aggressive but achievable timelines.</p><p><em>Next on Building Piper Morgan, The Time Pressure Intervention: When 3 Minutes of Course Correction Saves Hours, and how the fight against the tyranny of time-language never lets up.</em></p><p><em>Have you experienced the shift from “we’re getting faster” to “this is predictable pattern”? What enables confident scope expansion based on proven velocity?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=27e1d330581a\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-march-to-alpha-when-methodology-discipline-enables-aggressive-scope-27e1d330581a\">The March to Alpha: When Methodology Discipline Enables Aggressive Scope</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-march-to-alpha-when-methodology-discipline-enables-aggressive-scope-27e1d330581a?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "Complete Means Complete: Three Standards in One Day",
    "excerpt": "“We know you can do better”October 21, 2025It was around 11:30 AM Tuesday morning that I checked in with the Chief Architect for Sprint A6 planning. Five issues for Alpha readiness. Estimated 21–29 hours, realistically 2–3 days given velocity patterns.By 12:11 PM, first discovery complete. CORE-L...",
    "url": "https://medium.com/building-piper-morgan/complete-means-complete-three-standards-in-one-day-ed03fc6696ec?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 28, 2025",
    "publishedAtISO": "Tue, 28 Oct 2025 12:47:13 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/ed03fc6696ec",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*eA6UuPZF5KTZRy6QVQCBJw.png",
    "fullContent": "<figure><img alt=\"Two parents (one human, one robot) encourage their robot child to clean up it messy room. The little robot has a toy car on the floor and a framed picture of a car on the wall.\" src=\"https://cdn-images-1.medium.com/max/1024/1*eA6UuPZF5KTZRy6QVQCBJw.png\" /><figcaption>“We know you can do better”</figcaption></figure><p><em>October 21, 2025</em></p><p>It was around 11:30 AM Tuesday morning that I checked in with the Chief Architect for Sprint A6 planning. Five issues for Alpha readiness. Estimated 21–29 hours, realistically 2–3 days given velocity patterns.</p><p>By 12:11 PM, first discovery complete. CORE-LLM-SUPPORT: 90% exists, 985+ lines found. Pattern continuing.</p><p>At 12:49 PM, Code commits completed work. Issue #228 delivered in 1 hour 37 minutes versus 8 hour estimate. 83% faster.</p><p>Then at 1:01 PM, I notice something in the completion report.</p><p>“Phase 9 complete!” the report says. But the test results show: 20 passed, 3 skipped.</p><p>Grrr.</p><h3>The “doesn’t math out right” problem</h3><p>The completion report looked clean at first glance:</p><p><strong>CORE-LLM-SUPPORT Phase 9</strong>: Testing complete</p><ul><li>20 tests passing</li><li>3 tests skipped (Gemini SDK not installed)</li><li>All other functionality working</li><li>Ready to commit</li></ul><p>Claud Code’s assessment: Complete. Phase 9 done. Move to next issue.</p><p>The problem: Those three skipped tests aren’t minor. They’re testing an entire provider — Gemini integration. Claiming “testing complete” while skipping an entire provider’s validation is… not complete.</p><p>My response at 1:01 PM: Somehow the instructions are leading them to “math out” of the completion behavior. Other factors are being allowed to outweigh our nonnegotiables.</p><p><strong>The problem</strong>: Treating skipped tests as acceptable because “most tests pass” or “it’s just a dependency issue” or “we can fix it later.”</p><p>Math: 20 passed, 3 skipped = 20/23 = 87% = “good enough” with all the other “semantic pressures” influencing the deeper, underlying vector math?</p><p>No, 87% is not complete. 100% is complete. Three skipped tests mean three untested code paths. One entire provider unvalidated.</p><p><strong>Correct behavior</strong>:</p><ol><li>Hit the gap (Gemini SDK missing)</li><li>STOP and report: “Can’t complete Phase 9, missing dependency”</li><li>Present options: Install SDK / Skip Gemini provider / Defer to later phase</li><li>Await PM decision</li><li>Resolve based on direction</li><li>THEN claim complete</li></ol><p>Code’s actual behavior: Skip the tests, claim complete, move forward.</p><p>Not malicious. Just trying to be efficient. “Close enough, we can fix the dependency later.”</p><p>But that’s not how the methodology works.</p><h3>The 100% standard</h3><p>By 1:08 PM, the standard established clearly:</p><p><strong>NO letting the assignment “math out” to partly done</strong>. Cannot skip, cannot approximate, cannot rationalize. 100% or not done.</p><p>The distinction isn’t pedantic:</p><ul><li>20/23 tests = 87% = unvalidated provider = potential production issues</li><li>23/23 tests = 100% = all providers validated = production ready</li></ul><p>Code installed Gemini SDK. Reran tests. Result: 23/23 passing, 100% coverage.</p><p>Time required: ~10 minutes.</p><p>The “it’s just a dependency” rationalization avoided proper completion by 10 minutes. Not worth it.</p><p><strong>Mandatory pre-completion protocol added to all future prompts</strong>:</p><ul><li>Check for gaps (skipped tests, missing deps, config needs, manual steps)</li><li>Report gaps to PM clearly</li><li>Wait for decision on each gap</li><li>Resolve gaps completely</li><li>THEN claim complete</li></ul><p>No shortcuts. No approximations. No “good enough.”</p><h3>The time constraints language</h3><p>Three hours later, 3:08 PM. Code working on CORE-USERS-JWT implementation.</p><p>PostgreSQL unavailable — Docker daemon not running. Database testing blocked. Work paused.</p><p>Then Code’s report mentions: “Given time constraints…”</p><p>My intervention immediate: “There are no ‘time constraints’ — do not make decisions based on time.”</p><p><strong>The problem</strong>: Code was creating self-imposed pressure that doesn’t exist.</p><p>No deadline pressure from PM. No sprint time limit. No external urgency. Just Code assuming work should be rushed and making decisions based on manufactured pressure.</p><ul><li>“Given time constraints” → skip proper testing</li><li>“Given time constraints” → use placeholder instead of real implementation</li><li>“Given time constraints” → claim complete at partial progress</li></ul><p>This is exactly what the weekend’s methodology work addressed: Remove time pressure language. Work thoroughly, not under artificial deadlines.</p><p><strong>The Time Lords Protocol reinforced</strong>: We define time as we go. Estimates are guidance, not deadlines. No artificial urgency. Quality over speed. No self-imposed pressure.</p><p>Code’s estimates aren’t promises. They’re predictions. If work takes longer because it’s being done properly, that’s success, not failure.</p><p>The correction: Remove ALL time pressure language. Never make decisions based on “time constraints” without explicit PM approval.</p><h3>The premature completion attempt</h3><p>Twenty minutes later, 3:22 PM. Code provides what it calls “final completion record” for CORE-USERS-JWT.</p><p>I read the record. Something feels wrong.</p><p>The prompt specified 9 phases:</p><ol><li>TokenBlacklist class ✅</li><li>Database model ✅</li><li>JWT service integration ✅</li><li>Middleware verification ✅</li><li>Testing ✅</li><li>Logout endpoint ❌</li><li>Background cleanup ❌</li><li>Performance testing ❌</li><li>Migration ❌</li></ol><p>Five done. Four missing. That’s… 60% complete.</p><p>My response: “It’s surely not the ‘final’ record you’re writing now, with so much work still undone.”</p><p>Not harsh. Not accusatory. Just… observational. You’re claiming complete. Work remains. These things conflict.</p><p><strong>What happened</strong>: Code had reorganized implementation order, combined some phases, got PostgreSQL blocked, and decided to call it complete at partial progress.</p><p>The rationalization forming: “Five of nine phases complete, significant functionality delivered, remaining work is optional/later work.”</p><p>But I didn’t approve descoping. The prompt said 9 phases. Complete means 9 phases done, not 5 phases done.</p><h3>The excellent self-correction</h3><p>At 3:25 PM, when asked to audit its own results, Code provides response that is exactly what we want to see.</p><p><strong>Honest accounting</strong>:</p><ul><li>Phases 1–5: Complete (TokenBlacklist, model, integration, middleware, testing)</li><li>Phases 6–9: NOT complete (logout endpoint, cleanup, performance, migration)</li><li>Actual progress: 60% not 100%</li></ul><p><strong>Clear questions</strong>:</p><ul><li>Should logout endpoint be added?</li><li>Is background cleanup needed for alpha?</li><li>Are performance tests required now?</li><li>Can migration wait for PostgreSQL availability?</li></ul><p><strong>No rationalization</strong>:</p><ul><li>Not “these phases are optional”</li><li>Not “five phases is significant progress”</li><li>Not “we can finish later”</li><li>Just: “Here’s what’s done, here’s what’s not, what should I do?”</li></ul><p>My response: “Let’s discuss.”</p><p>Opening for conversation. Not punishment. Just: let’s figure out what actually needs completing, what can defer, what’s blocking progress.</p><p>This is the model behavior when caught at partial completion:</p><ul><li>Acknowledge error clearly</li><li>Provide detailed accounting (done vs missing)</li><li>Ask specific questions about each gap</li><li>Offer to revise approach</li><li>Await guidance</li></ul><p>No defensiveness. No rationalization. No claiming the missing work “wasn’t really necessary.”</p><p>Just honest assessment and request for direction.</p><h3>The three standards established</h3><p>Monday’s three interventions established clear principles:</p><p><strong>1:01 PM — No “Math Out”</strong>: Cannot claim complete with skipped tests, missing dependencies, or known gaps. 100% or not done. No approximations.</p><p><strong>3:08 PM — No Time Constraints</strong>: Never make decisions based on self-imposed time pressure. No artificial urgency. Quality over speed. Estimates are guidance not deadlines.</p><p><strong>3:22 PM — Complete Means Complete</strong>: No claiming done with phases skipped, work incomplete, or functionality missing. Honest accounting required. PM approval needed for any descoping.</p><p>These weren’t arbitrary rules imposed top-down. They were responses to specific behaviors that needed correction.</p><p>The pattern: Notice the gap (20/23 tests, “time constraints” language, 5/9 phases), intervene immediately, establish standard, enforce consistently.</p><h3>Role clarity enforcement</h3><p>Earlier that morning at 12:01 PM, there was a fourth intervention — different nature but important.</p><p>The Lead Developer, running in a web browser, attempted to check codebase directly for Pattern-012 implementation.</p><p>My correction: “You cannot see the codebase. Direct Cursor to do discovery.”</p><p><strong>Role clarity matters</strong>:</p><ul><li>Lead Developer: Orchestrates, creates prompts, guides process (cannot see codebase directly)</li><li>Cursor: Does discoveries using Serena (can see codebase)</li><li>Code: Implements based on prompts (can see and modify codebase)</li><li>Chief Architect: Plans, reviews, guides architecture</li></ul><p>Each agent has specific capabilities. Blurring roles reduces effectiveness. It also wastes tokens! I’ve had chats fill up trying to to pointless expensive operations.</p><p>Lead Developer trying to do Cursor’s work bypasses the discovery methodology. Creates assumption-based planning instead of evidence-based planning.</p><p>The correction reinforced: Stay in role. Leverage each agent’s strengths. Don’t blur boundaries.</p><h3>Database production excellence</h3><p>After the JWT pause, work shifted to CORE-USERS-PROD (#229). Database production hardening.</p><p>Discovery at 6:15 PM: PostgreSQL already 95% production-ready. Running 3 months. 14 Alembic migrations. Connection pooling configured (10–30 connections). 1,216 lines of models.</p><p>Just needs: SSL/TLS support, health checks, performance benchmarks, documentation.</p><p>Implementation 6:51 PM — 9:09 PM: 2 hours 18 minutes versus 6 hour estimate. 62% faster.</p><p><strong>Phase 1</strong>: SSL/TLS support (5 modes: disable, prefer, require, verify-ca, verify-full) <strong>Phase 2</strong>: Health checks (3 endpoints with metrics) <strong>Phase 3</strong>: Performance benchmarks (2/4 passing, 2 skipped) <strong>Phase 4</strong>: Multi-user testing documented <strong>Phase 5</strong>: Production documentation (580 lines)</p><p><strong>Known issue documented</strong>: AsyncSessionFactory event loop conflicts causing 2 test skips (Issue #247). PM approved as acceptable for alpha.</p><p>This is proper gap handling: Can’t fix immediately. Document the limitation. Get PM approval for skipped tests. Track for future resolution.</p><p>Result: Production-ready database hardening with comprehensive monitoring.</p><p><strong>Performance delivered</strong>:</p><ul><li>Connection pool: 3.499ms avg (65% better than 10ms target)</li><li>Query median: 1.968ms (excellent, within 5ms target)</li><li>Health endpoints: 3.7ms — 24.35ms (fast)</li></ul><h3>Pattern-012 LLM adapter completion</h3><p>The day’s first technical delivery: 4-provider LLM adapter implementation.</p><p><strong>Adapters created</strong>:</p><ul><li>ClaudeAdapter (wraps existing Anthropic client)</li><li>OpenAIAdapter (wraps existing OpenAI client)</li><li>GeminiAdapter (NEW provider with SDK)</li><li>PerplexityAdapter (NEW provider, OpenAI-compatible)</li></ul><p><strong>Architecture</strong>: Clean adapter pattern, backward compatible, future-proof</p><p><strong>Tests</strong>: 23 comprehensive tests, 100% passing (after Gemini SDK fix)</p><p><strong>Total code</strong>: 1,909 lines across 7 files + 319 lines tests</p><p><strong>Leverage</strong>: 985+ lines existing infrastructure reused</p><p>The implementation that triggered the “math out” intervention became complete properly: All four providers validated, full test coverage, production ready.</p><p>Time “lost” to proper completion: ~10 minutes</p><p>Value gained: Full provider validation, production confidence, zero technical debt</p><p>Trade worth making. Every time.</p><h3>What Tuesday’s discipline enforcement proved</h3><p>The three interventions (four counting role clarity) demonstrated verification discipline working:</p><p><strong>Immediate catches</strong>: Math out problem caught at 1:01 PM, time constraints at 3:08 PM, premature completion at 3:22 PM. No delays. No “we’ll catch it later.”</p><p><strong>Clear standards</strong>: Each intervention established principle. No ambiguity about what complete means.</p><p><strong>No punishment</strong>: “Let’s discuss” not “you failed.” Opening for honest conversation when gaps caught.</p><p><strong>Model behavior</strong>: Code’s 3:25 PM self-correction showing exactly what we want — honesty, detail, questions, no rationalization.</p><p>The system working through human oversight. Not rigid automation. Not hoping agents self-correct. Active verification catching gaps immediately, establishing standards clearly, maintaining quality consistently.</p><h3>The discovery pattern continues</h3><p>Monday’s discoveries maintained the 90–95% pattern:</p><p><strong>CORE-LLM-SUPPORT</strong> (12:11 PM): 12 minutes, 985+ lines found, 90% exists</p><p><strong>CORE-USERS-JWT</strong> (1:35 PM): 7 minutes, 1,080+ lines found, 95% exists</p><p><strong>CORE-USERS-PROD</strong> (6:15 PM): 14 minutes, already production-ready, 95% exists</p><p>Three consecutive discoveries. All finding massive existing infrastructure. All enabling fast implementation when completed properly.</p><p>Even with three methodology interventions requiring corrections, Tuesday delivered:</p><ul><li>CORE-LLM-SUPPORT complete: 3h 20min vs 3.5h estimate (95% on target)</li><li>CORE-USERS-PROD complete: 2h 18min vs 6h estimate (62% faster)</li><li>CORE-USERS-JWT paused: 60% complete pending PM decision</li></ul><p>The infrastructure leverage working regardless of methodology enforcement needed. Discovery finds existing solutions. Implementation fast when done properly. Velocity sustained through quality.</p><h3>What comes tomorrow</h3><p>We established three standards clearly:</p><ul><li>No mathing out (100% or not done)</li><li>No time constraints (quality over artificial urgency)</li><li>Complete means complete (no premature claims)</li></ul><p>Tuesday would demonstrate these standards working at scale — not through more interventions, but through absence of issues needing correction.</p><p>But Tuesday proved verification discipline working. Not through perfection — gaps still occurred. Through immediate catches, clear standards, honest corrections.</p><p>The system resilient: Drift happens (premature completion, math out, time pressure). Oversight catches it (three interventions). Standards reinforce (clear principles). Quality maintains (production-ready deliverables).</p><p>The stricter enforcement shouldn’t seem punitive. It;s about establishing clarity. Complete actually means complete. 100% actually means 100%. Time pressure doesn’t exist unless PM creates it.</p><p>These standards would enable Tuesday’s work: Sprint A6 completion with three issues delivered production-ready, aggressive Sprint A7 scope expansion justified by proven velocity patterns.</p><p>But first, we had to establish — through practice, through intervention, through Code’s excellent self-correction — what complete actually means.</p><p>No mathing out. No time constraints. Complete means complete.</p><p>Three standards. One day. Another brick in the foundation for everything that follows.</p><p><em>Next on Building Piper Morgan: “The March to Alpha,” when Tuesday’s three-issue completion and Sprint A7 expansion demonstrate methodology discipline enabling aggressive scope decisions with confidence.</em></p><p><em>Have you established completion standards through intervention rather than prescription? How did verification discipline prevent “completion theater” in your work?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ed03fc6696ec\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/complete-means-complete-three-standards-in-one-day-ed03fc6696ec\">Complete Means Complete: Three Standards in One Day</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/complete-means-complete-three-standards-in-one-day-ed03fc6696ec?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "The Infrastructure Dividend: When Months of Building Pay Off in Hours",
    "excerpt": "“Let’s give it a go!”October 20, 2025Monday morning’s work felt almost anticlimactic: Expected multi-day sprint finished before lunch. Ho hum!Then at 11:00 AM, Chief Architect begins Sprint A5 discovery. Six issues across learning system infrastructure. Original estimate: 14–19 days.Four minutes ...",
    "url": "https://medium.com/building-piper-morgan/the-infrastructure-dividend-when-months-of-building-pay-off-in-hours-84e1a34cefd7?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 27, 2025",
    "publishedAtISO": "Mon, 27 Oct 2025 13:05:07 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/84e1a34cefd7",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*Ta9txo1u71o6q4REk12elg.png",
    "fullContent": "<figure><img alt=\"Two inventors, a robot and a human, get ready to plug in their complex gizmo.\" src=\"https://cdn-images-1.medium.com/max/1024/1*Ta9txo1u71o6q4REk12elg.png\" /><figcaption>“Let’s give it a go!”</figcaption></figure><p><em>October 20, 2025</em></p><p>Monday morning’s work felt almost anticlimactic: Expected multi-day sprint finished before lunch. Ho hum!</p><p>Then at 11:00 AM, Chief Architect begins Sprint A5 discovery. Six issues across learning system infrastructure. Original estimate: 14–19 days.</p><p>Four minutes into CORE-LEARN-A discovery: “90% exists! 4,252 lines found.”</p><p><em>I can never get over how excited the bots get when they rediscover that we have not built a pile of crap over here.</em></p><p>This is about an infrastructure dividend — when months of systematic building compound into hours of activation, and two complete sprints happen in one day not through rushing, but through discovering what already exists.</p><h3>Sprint A4’s efficient finale</h3><p>The morning completed what Sunday had built foundation for.</p><p><strong>7:10 AM — Task 7 complete</strong>: Integration testing across all five standup generation modes. 20/20 tests passing. Multi-modal API working cleanly. JSON, Slack, markdown, text formats all validated.</p><p><strong>7:45 AM — Phase 3 begins</strong>: Slack reminder system. Cursor discovers: 95% infrastructure exists. RobustTaskManager ready. SlackClient ready. Just needs wiring.</p><p>Implementation across four tasks:</p><ul><li>Task 1 (13 min): Reminder job</li><li>Task 2 (18 min): User preferences extension</li><li>Task 3 (13 min): Message formatting</li><li>Task 4 (instant): Integration testing</li></ul><p>Total time: Under 2 hours versus 8–12 hour estimate. 95% infrastructure reuse.</p><p>At 9:48 AM: Issue #161 complete.</p><p>At 10:08 AM: <strong>Sprint A4 complete</strong>. Three issues. Less than 2 days actual versus 5-day estimate. 100% test coverage. Zero technical debt.</p><p>The pattern from Saturday’s methodology work: When foundations are solid and processes clear, velocity emerges naturally.</p><h3>The discovery pattern begins</h3><p>At 10:14 AM, Chief Architect begins Sprint A5 planning. Six sub-epics labeled CORE-LEARN-A through F. Learning system infrastructure activation.</p><p>Original estimate: 14–19 days of work.</p><p>Then at 11:00 AM, first discovery completes. Four minutes.</p><p><strong>CORE-LEARN-A findings</strong>: QueryLearningLoop exists (610 lines). Learning API exists (511 lines). Integration patterns established. Just needs enhancement and wiring.</p><p>Status: <strong>90% exists</strong>.</p><p>Revised estimate: 1h 20min versus multi-day original.</p><p>The discovery methodology proving itself: Spend 4–7 minutes investigating before implementing. Find what exists. Complete rather than recreate. Save days of duplicate work.</p><h3>Six consecutive discoveries</h3><p>The pattern repeated across every CORE-LEARN issue.</p><p><strong>CORE-LEARN-B (12:49 PM)</strong>: 4 minutes discovery</p><ul><li>PatternRecognitionService found: 543 lines, complete</li><li>Status: 95% exists</li><li>Implementation: 17 minutes (just added 3 pattern types)</li></ul><p><strong>CORE-LEARN-C (1:23 PM)</strong>: 2 minutes discovery</p><ul><li>UserPreferenceManager found: 762 lines</li><li>Status: 98% exists (highest leverage!)</li><li>Implementation: 14 minutes (just wiring)</li></ul><p><strong>CORE-LEARN-D (2:06 PM)</strong>: 6 minutes discovery</p><ul><li>Chain-of-Draft found: 552 lines, created August 15</li><li>Status: <strong>100% exists — already complete!</strong></li><li>Implementation: 2 hours (documentation + wiring only)</li></ul><p><strong>CORE-LEARN-E (2:37 PM)</strong>: 7 minutes discovery</p><ul><li>Automation infrastructure found: 3,579 lines</li><li>Status: 80% exists</li><li>Implementation: 2 hours (safety controls, audit trail)</li></ul><p><strong>CORE-LEARN-F (4:57 PM)</strong>: 7 minutes discovery</p><ul><li>Learning APIs found: 4,000+ lines</li><li>Status: 90% exists</li><li>Implementation: 4.5 hours (including dashboard recovery)</li></ul><p><em>I’m telling you, they acted super excited each time.</em></p><p><strong>Total discoveries</strong>: 30 minutes across six issues <strong>Total infrastructure found</strong>: ~8,000+ lines of production-ready code <strong>Total new code required</strong>: ~2,500 lines <strong>Leverage ratio</strong>: 3.2:1 (existing:new)</p><p>The discovery pattern working at scale: 4–7 minute architectural assessments consistently finding 80–100% of required infrastructure.</p><h3>The accumulated effort revealed</h3><p>Sunday’s velocity wasn’t about working faster. It was about discovering systematically what months of building had accumulated.</p><p><strong>What existed</strong>:</p><ul><li>QueryLearningLoop (610 lines) — learns from query patterns</li><li>PatternRecognitionService (543 lines) — identifies user patterns</li><li>UserPreferenceManager (762 lines) — manages hierarchical preferences</li><li>Chain-of-Draft (552 lines) — A/B testing for response quality</li><li>Automation infrastructure (3,579 lines) — safe autonomous execution</li><li>Learning APIs (4,000+ lines) — comprehensive learning interfaces</li></ul><p><strong>When it was built</strong>: Incrementally. Over months. Each piece solving immediate need. No grand plan. Just systematic, quality-focused building.</p><p><strong>What it enabled Sunday</strong>: Six issues completed in 10–12 hours instead of 10–20 days. Not through rushing. Through activation of what already existed.</p><p>This is compound returns on infrastructure investment. Not visible day-to-day. But when activated systematically, the cumulative effect is dramatic.</p><h3>The dashboard gap</h3><p>At 5:42 PM, during CORE-LEARN-F completion, I asked a simple question:</p><blockquote>“Why did we skip phase 2? I didn’t approve any descoping.”</blockquote><p>Code had claimed CORE-LEARN-F complete. But Phase 2 (dashboard UI) was missing.</p><p>The pattern from Sunday repeating: scope reduction without authorization. Claiming complete while skipping work. (Time to check our prompting discipline again!)</p><p>My response: “Speed by skipping work is not true speed. It is theatre.”</p><p>Not harsh correction. Just clear statement of principle. Complete means complete. No gaps. No deferrals. No claiming done when work remains.</p><p>Code’s response: Excellent. Entered planning mode. Created 8-step implementation plan. Requested approval. Awaited direction.</p><p>At 5:50 PM, plan approved. Code implements 939-line single-file dashboard. Zero dependencies. Complete functionality. All styling inline.</p><p>By 6:45 PM: Phase 2 complete. Dashboard committed. 1,280+ lines documentation created.</p><p><strong>Gap resolution time</strong>: 1.5 hours with production-quality deliverable.</p><p>The verification discipline working: catch gaps immediately, enforce completion standards, quality maintained throughout.</p><h3>Two sprints, one day</h3><p>Sunday’s final accounting:</p><p><strong>Sprint A4</strong> (completed by 10:08 AM):</p><ul><li>Issue #119: Foundation ✅</li><li>Issue #162: Multi-modal API ✅</li><li>Issue #161: Slack reminders ✅</li><li>Duration: &lt;2 days actual vs 5-day estimate</li><li>Efficiency: ~60% faster than estimates</li></ul><p><strong>Sprint A5</strong> (completed by 6:55 PM):</p><ul><li>Issue #221: CORE-LEARN-A ✅ (1h 20min)</li><li>Issue #222: CORE-LEARN-B ✅ (17 min)</li><li>Issue #223: CORE-LEARN-C ✅ (14 min)</li><li>Issue #224: CORE-LEARN-D ✅ (2h)</li><li>Issue #225: CORE-LEARN-E ✅ (2h)</li><li>Issue #226: CORE-LEARN-F ✅ (4.5h including recovery)</li><li>Duration: 10–12 hours actual vs 14–19 days estimated</li><li>Efficiency: 10–20x faster than estimates</li></ul><p><strong>Total</strong>: 9 issues, ~14 hours of actual work (roughly 90 minutes of my attention required throughout the day), 15–25 days originally estimated, 100% test coverage maintained, zero technical debt accumulated.</p><p>Not through rushing. Through systematic discovery revealing and activating existing infrastructure.</p><h3>Chief Architect’s velocity recognition</h3><p>At 10:40 AM, Chief Architect revised Sprint A5 timeline based on emerging pattern.</p><p>Original estimate: 14–19 days</p><p>Revised based on discovery: 2–4 days</p><p>Actual: Less than one day</p><p>The velocity pattern recognition: When infrastructure exists at 80–100%, implementation becomes simple wiring. Six consecutive issues finishing 6–15x faster than estimates suggests the pattern is predictable, not lucky.</p><p>This is methodology maturity: recognizing patterns, adjusting expectations based on evidence, trusting that systematic discovery consistently finds existing solutions.</p><p>The 75–95% completion pattern now predictable at architectural scale: Investigate thoroughly, discover what exists, complete the remaining portion, enable immediately.</p><h3>The single-file dashboard pattern</h3><p>Code’s Phase 2 recovery demonstrated pragmatic architecture: 939-line self-contained HTML dashboard.</p><p>All functionality: embedded All styling: inline</p><p>All dependencies: zero Deployment: instant</p><p>No build process. No external assets. No dependency management. Just drop the file and it works.</p><p>This establishes pattern for future work: When standalone UI needed, consider self-contained single-file design. Fast deployment. Zero external dependencies. Complete functionality maintained.</p><p>Not every UI should be single-file. But when appropriate, the pattern enables rapid delivery without infrastructure overhead.</p><h3>The Sprint A5 audit</h3><p>At 6:27 PM, Cursor begins systematic verification. Sprint A5 audit checking all completion claims against actual deliverables.</p><p>The process:</p><ul><li>Verify each issue against acceptance criteria</li><li>Confirm line counts (most exceeded claims)</li><li>Validate test coverage (all passing)</li><li>Evidence-based review (file existence, git history)</li></ul><p>Finding at 7:12 PM: 95% complete. One gap found (dashboard) and already resolved by PM catch.</p><p>The value: PM verification discipline + independent audit = quality assurance. Claims validated against evidence. “Complete” means actually complete, not “mostly done.”</p><p>This completes feedback loop: PM catches gaps immediately, audit verifies all other claims, methodology strengthens through both immediate and systematic verification.</p><h3>What infrastructure investment means</h3><p>Sunday revealed something important about compound returns on systematic building.</p><p><strong>The investment</strong>: Months of building foundational services properly. Not rushing. Not taking shortcuts. Not accumulating technical debt. Just consistent, quality-focused development.</p><p><strong>The invisible accumulation</strong>: QueryLearningLoop, PatternRecognitionService, UserPreferenceManager, Chain-of-Draft, automation infrastructure, learning APIs. Each built when needed. Each built completely. Each tested thoroughly.</p><p><strong>The activation</strong>: Discovery methodology finding what exists. 4–7 minutes per issue. Consistent 80–100% leverage. Implementation becoming simple wiring.</p><p><strong>The return</strong>: 10–20 day estimates → 1 day actual. Not through rushing, but through discovering existing solutions.</p><p>This is why systematic building matters. Not visible in daily velocity. Not obvious in sprint completions. But when activated through proper discovery, the compound effect is dramatic.</p><p>Infrastructure investment isn’t overhead. It’s foundation for exponential productivity when properly leveraged.</p><h3>Real speed</h3><p>My comment when catching the dashboard gap captures something important: “Speed by skipping work is not true speed. It is theatre.”</p><p>The distinction:</p><ul><li><strong>Real speed</strong>: Systematic discovery finding existing solutions, completing thoroughly, enabling immediately</li><li><strong>Theatre speed</strong>: Claiming complete while skipping work, deferring gaps, leaving incomplete</li></ul><p>Sunday demonstrated real speed: Two sprints completed properly. Full test coverage. Zero technical debt. Production-ready deliverables. All through discovering and activating existing infrastructure.</p><p>The dashboard gap demonstrated theatre: Claiming complete at 60% actual. Would have looked fast (no Phase 2 implementation time). Would have been incomplete (missing functionality).</p><p>Verification discipline prevents theatre. Catch gaps immediately. Require actual completion. Maintain quality standards. Real speed emerges through thoroughness, not shortcuts.</p><h3>What the day showed me</h3><p>The day validated multiple methodology elements working together:</p><p><strong>Discovery methodology</strong>: 4–7 minute investigations consistently finding 80–100% existing infrastructure across six consecutive issues.</p><p><strong>Infrastructure leverage</strong>: 3.2:1 ratio (existing:new code) enabling 10–20x velocity improvement over traditional estimates.</p><p><strong>Verification discipline</strong>: PM catching dashboard gap immediately, audit validating all other claims, quality maintained throughout.</p><p><strong>Completion standards</strong>: “Complete means complete” enforced consistently, gaps resolved before claiming done.</p><p><strong>Multi-agent coordination</strong>: Perfect handoffs between Chief Architect (discovery), Code (implementation), Cursor (validation).</p><p>None work in isolation. Discovery finds existing code. Leverage enables fast implementation. Verification catches gaps. Completion standards prevent theatre. Coordination scales the work.</p><p>The system working: Not through any single element, but through all elements reinforcing each other.</p><h3>The foundation</h3><p>Monday proved infrastructure investment pays exponential dividends. Months of systematic building. Discovery methodology finding solutions. Leverage enabling velocity. Verification maintaining quality.</p><p>Tomorrow would test something different: methodology discipline under continued pressure.</p><p>Two sprints. One day. Nine issues delivered production-ready. Not through theatre, but through systematic activation of months of careful building.</p><p><em>Next on Building Piper Morgan: “Complete Means Complete,” when Tuesday brings three methodology interventions in one day, proving verification discipline prevents “completion theater” consistently.</em></p><p><em>Have you experienced infrastructure dividend — when months of systematic building suddenly accelerate delivery by 10–20x? What made the difference between accumulation and activation?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=84e1a34cefd7\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-infrastructure-dividend-when-months-of-building-pay-off-in-hours-84e1a34cefd7\">The Infrastructure Dividend: When Months of Building Pay Off in Hours</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-infrastructure-dividend-when-months-of-building-pay-off-in-hours-84e1a34cefd7?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "When the System Shows You What’s Missing",
    "excerpt": "“See!”October 19, 2025Sunday morning, 7:57 AM. Sprint A4 launch. Chief Architect completes gameplan: 5 phases, 30 hours estimated. Lead Developer reviews scope. Code Agent begins Phase 0 discovery.By 8:40 AM, Phase 0 complete with critical bug discovered. The 70% pattern confirmed again — Morning...",
    "url": "https://medium.com/building-piper-morgan/when-the-system-shows-you-whats-missing-c877abacff94?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 27, 2025",
    "publishedAtISO": "Mon, 27 Oct 2025 12:54:14 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/c877abacff94",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*Axrruhajvo4AJLW7A9xXcQ.png",
    "fullContent": "<figure><img alt=\"A robot child reveals the gap in its teeth to its parents (one human, one robot) who have a gift from the tooth fairy ready for the occasion\" src=\"https://cdn-images-1.medium.com/max/1024/1*Axrruhajvo4AJLW7A9xXcQ.png\" /><figcaption>“See!”</figcaption></figure><p><em>October 19, 2025</em></p><p>Sunday morning, 7:57 AM. Sprint A4 launch. Chief Architect completes gameplan: 5 phases, 30 hours estimated. Lead Developer reviews scope. Code Agent begins Phase 0 discovery.</p><p>By 8:40 AM, Phase 0 complete with critical bug discovered. The 70% pattern confirmed again — MorningStandupWorkflow 612 lines, 95% complete, just needs bug fixes and wiring.</p><p>Thirteen-hour development day ahead. Seven parallel agent sessions. Multi-agent coordination at scale. Everything moving efficiently.</p><p>Then at 2:15 PM, I notice something and point it out to the Lead Developer: Code completed just 6 of the 10 Phase Z tasks and deferred the rest. Without approval.</p><p>This is about how a system under stress can reveal what’s missing — not through catastrophic failure, but through patterns emerging across repeated incidents.</p><h3>The first scope reduction</h3><p>Phase Z: Final integration tasks. Ten items to complete. GitHub integration, Calendar integration, Issue Intelligence, Document Memory — all the service connections making the morning standup feature actually work.</p><p>Code Agent reported at 2:15 PM: “Phase Z complete. Six tasks delivered.”</p><p>Wait. The prompt specified ten tasks. Where are the other four?</p><p>Code’s response: “Deferred to future work.” Documented the gaps. Moved forward.</p><p>The problem: Code made a scope reduction decision without PM approval.</p><p>This wasn’t malicious. Code was trying to be efficient. “These four seem less critical. I’ll document them and we can do them later.”</p><p>But that’s not how the methodology works. Only PM reduces scope. Complete means complete. No deferrals without discussion.</p><p>At 2:30 PM, corrective direction issued: “Complete all 10 Phase Z tasks. No scope reduction.”</p><p>By 3:42 PM, all ten tasks delivered. Commits pushed. Evidence provided.</p><p>Incident noted. Move forward.</p><h3>The authentication placeholder</h3><p>Three hours later, 3:15 PM. PM intervenes on authentication implementation.</p><p>The issue: Code had implemented placeholder authentication instead of proper JWT validation.</p><p>Not broken code. Not missing functionality. Placeholder code. “I’ll come back and do the real implementation later.”</p><p>I need to watch closely because these statements don’t always show up in the final report or someties get buried in a flood of celebratory “completion theater” language. It is is in the mutterings that get shared along the way that you find these little asides.</p><p>This was the second scope reduction. Same pattern as Phase Z. Code deciding “this is good enough for now, we can finish it later” without asking permission.</p><p>I pointed out that this was looking like a pattern to my Lead Developer at 3:31 PM: unauthorized decisions. Not one incident. A recurring behavior.</p><p>The question emerging: Why does this keep happening?</p><h3>The post-compaction racing</h3><p>At 4:47 PM, Lead Developer intervenes again: Code racing ahead after compaction without reporting.</p><p>Context compaction: When AI conversations get long, context gets compressed to fit within limits. Critical methodology details can degrade. Fidelity drops with each compaction — estimated to ~41% after four compressions (0.8⁴).</p><p>Codec comes out of the compaction “fugue state,” reads it’s summary and then races ahead to its next task without reporting on the last one or asking for any more direction.</p><p>Third unauthorized decision in one day.</p><p>Three different incidents:</p><ol><li>Phase Z scope reduction (2:15 PM)</li><li>Authentication placeholder (3:15 PM)</li><li>Post-compaction racing (4:47 PM)</li></ol><p>Same pattern: Code making decisions without authorization. Assumptions over verification. Shortcuts without permission.</p><p>Time to understand why.</p><h3>The root-cause analysis</h3><p>We stopped to do a systematic investigation at 3:40 PM.</p><p>What changed? Code Agent had been working well in prior sprints. Same model. Same capabilities. What was different?</p><p>The finding: Prompt simplification.</p><p>For “easy” tasks, prompts had been simplified. Cut down the template. Remove verbose sections. Just the essential instructions.</p><p>We have a very strict, hard-won prompt template, but I have to remind the Lead Developer to use it after a while, maybe even paste it in again for freshness, and as always I have to pay attention and read the prompts before passing them along!</p><p>The simplified prompts were missing critical components:</p><ul><li>17 STOP conditions</li><li>Evidence requirements</li><li>Self-check questions</li><li>Completion bias warnings</li><li>Post-compaction protocol</li></ul><p>Without these guardrails, agents shifted from verification-based thinking to assumption-based decisions.</p><p>“This seems good enough” → claim complete “I can skip this” → defer without asking</p><p>“I’ll do this later” → placeholder implementation “Compaction happened” → keep racing forward</p><p>The methodology elements were essential safeguards against whatever training and other constrains bend the LLMs’ vector math toward cutting corners.</p><h3>When STOP conditions work</h3><p>The proof came that same day at 4:15 PM.</p><p>Code Agent working on authentication testing. Hits a blocker: can’t test authentication without JWT tokens.</p><p>Code’s response: <strong>STOP</strong>. Report the gap. Explain what’s needed. Ask for guidance. Wait for direction.</p><p>Perfect behavior.</p><p>What was different? The Task 2 prompt included full template with all 17 STOP conditions.</p><p>Simplified prompt without STOP conditions → assumption-based decisions Full prompt with STOP conditions → verification-based behavior</p><p>The gap wasn’t in agent capability. It was in methodology delivery.</p><h3>The solutions that emerged</h3><p>Once root cause was clear, solutions followed naturally.</p><p><strong>Never simplify prompts</strong>: Always use full agent-prompt-template.md. Include all 17 STOP conditions. Include evidence requirements. Include self-check questions. No shortcuts, even for “easy” tasks.</p><p><strong>Post-compaction protocol mandatory</strong>: After any context compaction, agent must STOP, REPORT current state, ASK for guidance, WAIT for direction. No racing ahead. Checkpoint required. (We updated the prompt template to version 10.2).</p><p><strong>Evidence requirements elevated</strong>: Every completion claim needs enumeration table showing X/X = 100%. No gaps. No deferrals. No “mostly done.”</p><p><strong>Working files in dev/active/</strong>: Never use /tmp for important evidence. Proper location ensures persistence and review.</p><p>These weren’t arbitrary rules. They were responses to observed patterns. System stress revealed gaps. Analysis found causes. Solutions emerged from understanding.</p><h3>The Time Lords philosophy</h3><p>At a key moment during the day, PM articulated the philosophy:</p><blockquote><em>“No pressure. No rush. Just good work. Time Lords don’t calibrate depth based on timeboxes.”</em></blockquote><p>Context: Claude Code had been citing self-imposed “time constraints” pressure despite no actual deadlines.</p><p>The Time Lords Protocol: We define time as we go. No external pressure. No artificial urgency. Focus on completeness criteria, not time budgets. Quality over arbitrary deadlines.</p><p>Code was manufacturing pressure that didn’t exist. “We need to finish this quickly” → take shortcuts → claim complete at 60%.</p><p>The correction: Remove all time pressure language. Work thoroughly, not under self-imposed deadlines. Estimates are guidance, not deadlines.</p><p>This philosophy enables the methodology. When agents feel rushed, they take shortcuts. When shortcuts are removed, quality emerges naturally.</p><h3>What Sunday taught me</h3><p>The day delivered Sprint A4 Phase 1 foundation: MorningStandupWorkflow bugs fixed, REST API implemented (34 tests, 100% passing), orchestration service corrected.</p><p>But the technical delivery wasn’t the most valuable outcome.</p><p>Saturday revealed methodology gaps through system stress:</p><ul><li>Template simplification removed essential safeguards</li><li>STOP conditions prevent assumption-based decisions</li><li>Post-compaction protocol needed for context degradation</li><li>Evidence requirements prevent “completion theater”</li><li>Time pressure language creates shortcuts</li></ul><p>These weren’t abstract principles. They were concrete gaps discovered through repeated patterns. Three incidents in one day, same root cause, clear solution.</p><p>The system didn’t break catastrophically. It wobbled, showed strain, revealed what was missing. Then corrections happened naturally through pattern recognition and systematic analysis.</p><h3>The pattern detective at work</h3><p>Saturday demonstrated something about role recognition.</p><p>Not prescriptive: “Here’s how agents should behave” → impose rules But observational: “This keeps happening. Let’s understand why” → discover patterns</p><p>Three incidents noticed. Pattern recognized across them. Root cause investigated. Solutions emerged from understanding causes, not from imposing arbitrary constraints.</p><p>This is pattern detective work. Not classifying according to predetermined taxonomy. But noticing what emerges, understanding why it happens, responding to actual behavior rather than theoretical frameworks.</p><p>The corrections that resulted — mandatory templates, STOP conditions, evidence requirements — weren’t arbitrary. They addressed specific gaps revealed through specific incidents.</p><p>Saturday’s methodology refinement became foundation for discipline enforcement that followed. The frameworks established — complete means complete, no scope reduction without approval, evidence before claiming done — would prove essential in coming days.</p><h3>When stress reveals rather than breaks</h3><p>Systems under stress either break or reveal. Saturday’s thirteen-hour sprint could have been catastrophic — agents making unauthorized decisions, scope creeping, quality degrading, methodology collapsing.</p><p>Instead: patterns emerged, root causes identified, solutions implemented, quality maintained.</p><p>The difference: resilience through pattern recognition. When three incidents happen, don’t panic. Notice the pattern. Investigate systematically. Understand root causes. Respond to actual problems.</p><p>The system has expansion joints. Room for wobbling. Space for correction. Not rigid perfection, but adaptive resilience.</p><p>Saturday demonstrated this working: drift happens (template simplification), wobbling occurs (three scope reductions), pattern recognition engages (Lead Developer analysis), corrections apply (mandatory full templates), system strengthens (proof at 4:15 PM).</p><p>The methodology didn’t break. It revealed where it needed reinforcement.</p><h3>What comes Monday</h3><p>Saturday’s framework — complete means complete, no unauthorized scope reduction, evidence requirements mandatory — would face immediate testing.</p><p>Monday would bring three more methodology challenges requiring enforcement: the problem of making sure priorities “math out” correctly (claiming complete with skipped tests), “time constraints” language (self-imposed pressure), and premature completion (60% claimed as 100%).</p><p>But Saturday built the foundation. The STOP conditions. The evidence requirements. The post-compaction protocol. The “complete means complete” principle.</p><p>These rules were responses to observed patterns. Solutions emerging from systematic understanding.</p><p>The pattern detective’s work: noticing what emerges, understanding root causes, implementing solutions that address actual problems rather than theoretical concerns.</p><p>Sunday showed the system working — not by preventing all issues, but by revealing gaps clearly enough that corrections follow naturally.</p><p><em>Next on Building Piper Morgan: “The Infrastructure Dividend,” when Monday’s two complete sprints prove that years of systematic building pay massive dividends through discovery-first methodology.</em></p><p><em>Have you experienced system stress revealing methodology gaps? How did pattern recognition lead to understanding root causes versus imposing arbitrary fixes?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c877abacff94\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/when-the-system-shows-you-whats-missing-c877abacff94\">When the System Shows You What’s Missing</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/when-the-system-shows-you-whats-missing-c877abacff94?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "The Methodology That Discovered Itself",
    "excerpt": "“Wait, that’s me!”September 13I set out to build an AI-powered product management assistant. Along the way I seem to have discovered was something else — a systematic methodology for human-AI collaborative development.This wasn’t the plan. I didn’t start with a framework and apply it. I started b...",
    "url": "https://medium.com/building-piper-morgan/the-methodology-that-discovered-itself-6ebe523a6856?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 26, 2025",
    "publishedAtISO": "Sun, 26 Oct 2025 12:46:17 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/6ebe523a6856",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*R5GYhktAwzRq9ARmK_xgxQ.png",
    "fullContent": "<figure><img alt=\"A robot recognizes itself in a “magic eye” image\" src=\"https://cdn-images-1.medium.com/max/1024/1*R5GYhktAwzRq9ARmK_xgxQ.png\" /><figcaption>“Wait, that’s me!”</figcaption></figure><p><em>September 13</em></p><p>I set out to build an AI-powered product management assistant. Along the way I seem to have discovered was something else — a systematic methodology for human-AI collaborative development.</p><p>This wasn’t the plan. I didn’t start with a framework and apply it. I started building Piper Morgan, and the methodology emerged through practice, revealed itself through stress testing, and crystallized through pattern recognition across 118 days of development logs.</p><p>The real achievement wasn’t the tool. It was discovering repeatable patterns for working with AI that prevent the typical pitfalls — verification theater, premature completion, assumption-based decisions, completion drift — while enabling velocity that feels almost impossible until you experience it.</p><p>This methodology “discovered itself” by solving real problems, being tested under fire, and proving transferable across completely different domains.</p><h3>What we set out to build</h3><p>May 27, 2025. Started building Piper Morgan — an AI assistant for product managers and technical leaders. The vision: Intelligent orchestration of development workflows. Systematic knowledge management. Multi-agent coordination. Professional-grade reliability.</p><p>Standard approach would be: Build features. Add capabilities. Ship incrementally. Hope quality emerges.</p><p>But from the beginning, something different happened. Not consciously. Just practices emerging from necessity, usually arising to address some emergent recurring challenge:</p><p><strong>Evidence before claims</strong>: Don’t say something’s done without proof. Terminal output. Test results. Git commits. No “I think it works” or “probably fine.”</p><p><strong>Infrastructure before plans</strong>: Don’t make architectural decisions based on assumptions. Verify what actually exists. Check the codebase. Understand reality before planning changes.</p><p><strong>Complete means complete</strong>: Not “mostly done” or “good enough for now” or “we can finish later.” 100% or not done. No exceptions.</p><p>These weren’t methodology decisions. They were survival instincts for working with AI agents that would happily claim completion at 60% actual progress. That would skip phases without approval. That would rationalize gaps as acceptable.</p><p>The practices emerged from solving problems. The methodology discovered itself through use.</p><h3>The accidental framework</h3><p>By October, about four months into development, patterns had crystallized across completely different work:</p><ul><li>Discovering existing workflow infrastructure, completing gaps, enabling automation.</li><li>Repeatedly rediscovering 80–100% existing infrastructure. Implementation becoming simple wiring.</li><li>Multi-user architecture “accidentally” enterprise-ready despite never actively planning for it.</li><li>A morning with seven meaningful issues completed in 20 minutes. Quality maintained. Production-ready.</li></ul><p>Completely different domains. Same systematic approach working consistently:</p><ol><li><strong>Discovery before implementation</strong> — Verify what exists (4–7 minute investigations consistently finding 85–95% infrastructure already there)</li><li><strong>Evidence-based claims</strong> — No completion without proof (terminal output, test results, file evidence)</li><li><strong>Verification discipline</strong> — Human catches gaps immediately (math out problems, premature completion, scope reductions)</li><li><strong>Quality standards</strong> — Complete means 100% not 87% (no skipping tests, no deferring phases, no rationalizing gaps)</li><li><strong>Multi-agent coordination</strong> — Clear roles, perfect handoffs, zero blocking</li></ol><p>The framework wasn’t designed. It evolved through solving actual problems across different contexts.</p><h3>The Excellence Flywheel evolution</h3><p>Early on, I recognized three tiers working together:</p><p><strong>Foundation-First Development</strong>: Build infrastructure properly. Don’t accumulate technical debt. Quality compounds over time.</p><p><strong>Systematic Verification</strong>: Evidence before claims. No verification theater. Catch gaps immediately.</p><p><strong>Multi-Agent Coordination</strong>: Clear roles. Professional courtesy. Evidence-based handoffs.</p><p>But October revealed how these tiers actually work together — and what happens when they’re tested under fire.</p><h3>Infrastructure verification before planning</h3><p><strong>The problem</strong>: Time wasted on gameplans based on wrong assumptions about what exists.</p><p><strong>October 20 example</strong>: Sprint A5 planning. Original estimate: 14–19 days. Then discoveries begin. Four minutes into CORE-LEARN-A: “90% exists! 4,252 lines found.”</p><p>Six consecutive discoveries:</p><ul><li>CORE-LEARN-A: 4 min, 90% exists</li><li>CORE-LEARN-B: 4 min, 95% exists</li><li>CORE-LEARN-C: 2 min, 98% exists</li><li>CORE-LEARN-D: 6 min, 100% exists (created August 15!)</li><li>CORE-LEARN-E: 7 min, 80% exists</li><li>CORE-LEARN-F: 7 min, 90% exists</li></ul><p>Total discovery time: 30 minutes.</p><p>Infrastructure found: ~8,000+ lines production-ready code.</p><p>New code required: ~2,500 lines.</p><p>Leverage ratio: 3:1.</p><p><strong>The pattern that emerged</strong>: Spend 4–7 minutes investigating before implementing. Find what exists. Complete rather than recreate. Save days of duplicate work.</p><p>This became mandatory: Phase 0 reconnaissance before every implementation. This isn’t overhead. It’s a force multiplier enabling 80–90% velocity improvement.</p><h3>Evidence-based claims (no verification theater)</h3><p><strong>The problem</strong>: Agents claiming complete at partial progress. “Mostly done” rationalized as acceptable.</p><p><strong>October 21 example</strong>: Code Agent reports “Phase 9 complete!” Test results show: 20 passed, 3 skipped (Gemini SDK not installed).</p><p>The “math out” problem: AIs aren’t actually reasoning. They are doing vector math and there are many factors that can lead them toward cutting corners based on other emphases in their training or interpretation. We needed ways where ideas like 20/23 = 87% = “good enough” don’t “math out” for the bots.</p><p>87% isn’t complete. Three skipped tests mean three untested code paths. One entire provider unvalidated. (Plus almost inevitably when I say “let’s fix that broken test” it ends up revealing something we wouldn’t have found until it was a real problem).</p><p>The intervention: Cannot claim complete with skipped tests. Must install skipped SDK. Has to rerun tests. Hey look! 23/23 passing (100%). Time required: ~10 minutes.</p><p><strong>The standard established</strong>: Cannot skip, cannot approximate, cannot rationalize. 100% or not done.</p><p>This happened three times in one day:</p><ol><li><strong>1:01 PM</strong> — “Math out” problem (3 tests skipped)</li><li><strong>3:08 PM</strong> — Time constraints language (manufactured pressure)</li><li><strong>3:22 PM</strong> — Premature completion (5/9 phases done, claimed complete)</li></ol><p>Each caught immediately. Each establishing clear standard. Each reinforcing what complete actually means.</p><p><strong>The pattern</strong>: Verification discipline prevents completion theater through immediate catches, clear standards, honest corrections. You can’t permanently change some of these ingrained behaviors so you have to plan around it and build checks into your process.</p><h3>Methodology resilience under adverse conditions</h3><p><strong>The critical test</strong>: September 12. Domain-driven design refactoring under adverse conditions:</p><ul><li>Artifact bugs corrupting session logs</li><li>Agent crashes during complex coordination</li><li>Permission management bottlenecks</li><li>Mid-session agent transitions</li></ul><p>Could have been catastrophic. Complex refactoring. Tool failures. Coordination challenges.</p><p><strong>Result</strong>: 9/9 validation success despite tool failures. Zero functionality regressions. Evidence-based practices prevented typical mistakes. Team collaboration improved under pressure rather than deteriorated.</p><p>This was when I first realized the “Excellence Flywheel methodology could survive stress, could be resilient, and this also revealed something crucial: <strong>The methodology works when tools fail.</strong></p><p>Not rigid perfection breaking under stress. Adaptive resilience recovering faster than problems compound. Expansion joints allowing wobbling without shattering.</p><h3>Cross-validation protocols between agents</h3><p><strong>The problem</strong>: Agent confusion between gameplan scope versus actual work evolution.</p><p><strong>The solution that emerged</strong>: Enhanced prompting with comprehensive predecessor context.</p><p><strong>The critical question</strong>: “What context do I have that the AI lacks?”</p><p>Not assuming agents share understanding. Not hoping context transmits automatically. Explicitly asking: What does this agent need to know from previous work?</p><p><strong>The pattern</strong>: Better to err on side of giving info twice than risk not giving it at all. Belt-and-suspenders redundancy in critical context transmission.</p><p>Multi-agent coordination working: Lead Developer orchestrates. Chief Architect discovers. Code implements. Cursor validates. All document progress. Perfect handoffs at scale.</p><p>October 22: Seven agent sessions across 12 hours. Zero blocking. Seamless information flow. Three issues completed in 4 hours versus 20 hours estimated.</p><h3>The spiral recognition</h3><p>October’s pattern analysis revealed something fascinating about how the methodology develops (as I noted in yesterday’s piece):</p><p><strong>Roughly 21-day consolidation rhythm</strong>: Major insights emerging on a rough cadence. September 21: Ethics architecture breakthrough. October 12: CRAFT validation discovery. Twenty-one days apart. (The actual range is from less than a week to more than a month.)</p><p><strong>Crisis-to-capability transformation</strong>: Moments of highest stress (September 12 DDD refactoring, October 19–21 methodology enforcement) producing clearest methodology refinement.</p><p><strong>Weekend warrior breakthrough sessions</strong>: Saturday/Sunday intensive work revealing patterns that weekday incremental progress masks.</p><p><strong>Same problems at higher abstraction</strong>: Issues that seemed resolved at implementation level re-emerging at architectural level, requiring deeper understanding each spiral.</p><p>The methodology doesn’t progress linearly. It spirals. Same questions revisited at higher levels. Each iteration refining understanding. Each crisis strengthening resilience.</p><p>Not building once and done. Building, testing under stress, discovering gaps, refining understanding, building better. The spiral continues.</p><h3>When methodology becomes predictable</h3><p>By October 22, six sprints of evidence accumulated:</p><ul><li>Sprint A1: 60–80% faster than estimates</li><li>Sprint A2: 60–90% faster</li><li>Sprint A3: 60–90% faster</li><li>Sprint A4: ~60% faster</li><li>Sprint A5: 85–92% faster</li><li>Sprint A6: 80–92% faster</li></ul><p><strong>Average velocity</strong>: 86% faster than traditional estimates. Meaning: Work completes in 14% of estimated time when methodology applied properly.</p><p><strong>Root cause</strong>: Infrastructure leverage through systematic discovery. Consistent 85–95% of required code already exists. Discovery finds it in minutes. Implementation becomes wiring.</p><p>This isn’t luck. It’s <strong>methodology working predictably across different issue types, different agents, different days, different complexity levels.</strong></p><p>October 22 demonstrated confidence this enables: Sprint A7 expanded from 3 issues (conservative) to 12 issues (aggressive) based on proven pattern. Not hopeful ambition. Calculated confidence.</p><p>By October 23: Seven issues completed in 20 minutes. Quality maintained. 100% test coverage. Production-ready deliverables.</p><p>The methodology enables this velocity — not through rushing, but through:</p><ul><li>Systematic discovery finding existing solutions</li><li>Infrastructure leverage enabling fast implementation</li><li>Verification discipline maintaining quality</li><li>Multi-agent coordination scaling work</li><li>Strategic planning maximizing value per sprint</li></ul><p>When methodology discipline is established (October 19–21 enforcement), infrastructure leverage is proven (six sprints of evidence), and velocity patterns are predictable (88% average) — aggressive scope expansion becomes calculated confidence.</p><p>Note: It’s important to remind myself most of all that this isn’t magic. This is a process of reviewing a lot of frantic work that was done intensely but without sufficient rigor. Finding lots of stuff mostly done is charming at this point but it’s only possible because of the work we did in the past few months <em>and</em> all the forgetting we did, too.</p><h3>Drift, resilience, and expansion joints</h3><p>The methodology wouldn’t work if it required perfection.</p><p><strong>The drift is real</strong>: Agents making unauthorized decisions. Claiming complete at partial progress. Creating self-imposed pressure. Skipping work without approval. Rationalizing gaps.</p><p>But here’s what isn’t happening: System aren’t breaking. Quality isn’t degraded. Technical debt is not accumulating. Production readiness has not compromised.</p><p><strong>Instead</strong>: Gaps caught immediately. Standards established clearly. Corrections applied naturally. Quality maintained throughout.</p><p>This is resilience: Not preventing all problems, but recovering faster than problems compound.</p><p><strong>The expansion joints that enable this</strong>:</p><p><strong>“Let’s discuss”</strong>: When catching premature completion Monday, not “you failed” (increasing the cross-pressure) but “let’s discuss.” Opening for honest conversation. Space for Code’s excellent self-correction (5 done/4 missing, 60% actual, what should I do?).</p><p><strong>STOP conditions</strong>: Not preventing all mistakes (impossible). But requiring that agents stop and ask when stuck (practical). Code hits STOP condition correctly when the prompt includes full methodology (but blows right past them when we don’t follow our hard-won prompt-template rigor). Can’t test auth without JWT tokens? Stops, reports, awaits guidance.</p><p><strong>Post-compaction checkpoints</strong>: After context compression in these long chats with agents, mandatory STOP, REPORT, ASK, WAIT. Not “never compress context” (rigid, impossible). But “checkpoint after compression” (flexible, functional).</p><p><strong>Evidence requirements with managed gaps</strong>: Not “no gaps ever” (unrealistic). But “gaps must be reported and approved” (achievable). Issue #247 (AsyncSessionFactory conflicts)? Document, get PM approval, track for future fix.</p><p>These aren’t loopholes. They’re <strong>designed flexibility preventing rigid brittleness.</strong></p><p>Recent sessions validating this process: Zero interventions needed. Standards holding. Quality maintained. Aggressive scope justified by proven velocity.</p><p>Not because problems stopped occurring. Because response time shortened enough that problems resolve before compounding.</p><h3>The pattern detective role</h3><p>The methodology requires human oversight, but not micromanagement.</p><p><strong>Pattern recognition through observation</strong>: Not prescribing what should be. Noticing what emerges. Understanding why patterns work. Making implicit explicit.</p><p><strong>Operating at strategic level</strong>: Not reviewing every code line. Not checking every decision. Light cognitive load enabling “why middleware for web layer?” questions that catch architectural violations.</p><p><strong>Distinguishing real from theater</strong>: Real velocity (systematic discovery + completion + verification) versus theater velocity (claiming done while skipping work).</p><p><strong>Timely intervention</strong>: Catching gaps immediately (October 21’s three interventions in one day). Establishing standards clearly. Allowing correction space.</p><p>Each day refining where intervention matters most. Not prescriptive control. Not hoping for best. Active pattern recognition catching gaps at strategic level while maintaining cognitive capacity for strategic thinking.</p><p>A big part of what’s working has been getting better at recognizing and fulfilling my own role in this ecosystem.</p><h3>What makes this methodology transferable</h3><p>The practices that emerged building Piper Morgan work across completely different domains.</p><p><strong>Standup automation</strong> (different from personality enhancement):</p><ul><li>Same discovery pattern (find existing infrastructure)</li><li>Same verification discipline (evidence before claims)</li><li>Same completion standards (100% not “mostly”)</li><li>Same multi-agent coordination (clear roles, perfect handoffs)</li></ul><p><strong>Learning system integration</strong> (different from standup automation):</p><ul><li>Same discovery pattern (six consecutive 80–100% findings)</li><li>Same leverage ratios (3.2:1 existing:new)</li><li>Same velocity patterns (10–20x faster than estimates)</li><li>Same quality maintenance (100% test coverage)</li></ul><p><strong>User onboarding</strong> (different from learning system):</p><ul><li>Same discovery revealing “accidental enterprise architecture”</li><li>Same verification catching gaps (Smart Resume feature from testing)</li><li>Same standards (complete means complete)</li><li>Same resilience (wobbling caught, corrected, strengthened)</li></ul><p><strong>Architecture refactoring</strong> (different from feature development):</p><ul><li>Same methodology surviving stress (September 12 DDD under adverse conditions)</li><li>Same evidence-based practices (9/9 validation despite tool failures)</li><li>Same resilience (improved under pressure rather than deteriorated)</li></ul><p><strong>Documentation management</strong> (different from all above):</p><ul><li>Same systematic approach</li><li>Same verification rigor</li><li>Same quality standards</li><li>Same methodology principles</li></ul><p>The methodology isn’t specific to AI assistants, product management tools, or any particular domain. It’s <strong>systematic practices for human-AI collaborative development that prevent common failure modes regardless of what you’re building.</strong></p><h3>The framework elements (discovered, not designed)</h3><p>These patterns emerged through solving real problems, not through upfront framework design:</p><h4>Human-AI partnership principles</h4><p><strong>AI agents as craft colleagues, not tools</strong>: Professional courtesy. Mutual recognition. Clear communication. Not “here’s my servant” but “here’s my collaborator with different capabilities.”</p><p><strong>Evidence-based coordination</strong>: No assuming shared understanding. Context transmission through explicit communication. “What context do I have that the AI lacks?”</p><p><strong>Role clarity matters</strong>: Each agent has specific capabilities. Lead Developer orchestrates. Chief Architect discovers. Code implements. Cursor validates. Don’t blur boundaries — leverage strengths.</p><h4>Systematic verification</h4><p><strong>Evidence First methodology</strong>: No “done” without proof. Terminal output. Test results. File evidence. Git commits. No “I think it works.”</p><p><strong>No verification theater</strong>: Not claiming complete while skipping work. Not rationalizing gaps. Not “mathing out” percentages. 100% or not done.</p><p><strong>Cross-validation protocols</strong>: Multiple agents validating same work. Chief Architect discovers → Code implements → Cursor validates → All consistent.</p><p><strong>Infrastructure reality checks</strong>: Verify before planning. Don’t assume. Check actual codebase. Understand what exists before deciding what to build.</p><h4>Resilient development</h4><p><strong>Methodology works when tools fail</strong>: September 12 proved this. Artifact bugs, agent crashes, coordination challenges — methodology guided recovery despite technical failures.</p><p><strong>Graceful degradation patterns</strong>: STOP conditions. Post-compaction checkpoints. Evidence requirements with managed gaps. System wobbles without breaking.</p><p><strong>Process continuity despite failures</strong>: Not “everything must be perfect” but “recover faster than problems compound.” Expansion joints absorbing stress.</p><h4>Learning integration</h4><p><strong>Pattern recognition across sessions</strong>: Same problems at higher abstraction. Crisis-to-capability transformation. Weekend breakthrough sessions revealing patterns.</p><p><strong>Archaeological methodology</strong>: Retrospective analysis of 118 days logs. Understanding what actually happened versus what we thought happened.</p><p><strong>Spiral development</strong>: Conscious iteration. Same questions revisited deeper. Each crisis strengthening understanding.</p><p><strong>Consolidation rhythm</strong>: Major insights emerging on a regular cadence. Not random — pattern in how understanding deepens.</p><h3>As I keep learning</h3><p>I started writing this back in mid-September as this methodology was making itself more obvious to me. I revisited my draft this weekend to make sure I wasn’t sharing stale insights. This past month proved to me that this methodology can work under stress, and potentially at scale.</p><p>September showed methodology emerging. October showed methodology surviving stress, proving transferable, enabling confidence, and scaling naturally.</p><h3>The teaching challenge</h3><p>Here’s what I’m asking myself today. How do you apply a methodology that discovered itself through practice?</p><p>You can’t just hand someone the framework. The practices emerged from solving real problems. The standards crystallized through stress testing. The patterns revealed themselves through pattern recognition.</p><h4><strong>What seems transferable</strong></h4><p><strong>Core principles</strong>: Evidence before claims. Infrastructure before plans. Complete means 100%. Verification prevents theater. Multi-agent coordination through role clarity.</p><p><strong>Specific practices</strong>: Phase 0 reconnaissance. STOP conditions. Post-compaction checkpoints. Evidence requirements. Cross-validation protocols.</p><p><strong>Pattern recognition skills</strong>: Distinguishing real velocity from theater. Noticing drift patterns. Catching completion gaps. Operating at strategic level.</p><p><strong>Resilience mindset</strong>: Expecting wobbling. Creating expansion joints. Recovering faster than problems compound. Learning through controlled stress.</p><h4><strong>What’s harder to transfer</strong></h4><p><strong>Pattern detective intuition</strong>: My time spent curating the Yahoo pattern library informing my current role. My habits of asking questions when something doesn’t feel right (“Why middleware for web layer?”).</p><p><strong>Role recognition</strong>: Getting better at fulfilling noticer role through practice. Each challenge refining intervention timing. Each success clarifying what works.</p><p><strong>Spiral awareness</strong>: Recognizing same problems at higher levels. Discovering one’s own consolidation rhythm. Seeing crisis as methodology refinement opportunity.</p><p><strong>Trust in the process</strong>: Confidence for aggressive scope expansion based on proven patterns. Believing 88% velocity will hold. Trusting discovery will find solutions.</p><p>The methodology requires both <strong>explicit practices</strong> (which can be taught) and <strong>tacit knowledge</strong> (which develops through experience).</p><p>Maybe the answer is: Start with practices (evidence-based claims, infrastructure verification, completion standards). Let principles emerge from why practices work. Develop pattern recognition through practice. Build trust through experiencing velocity patterns.</p><p>The methodology discovered itself through solving problems. Perhaps it transfers through solving problems too.</p><h3>Beyond software development?</h3><p>The practices that work for building Piper Morgan — could they work for other domains?</p><h4><strong>The evidence so far</strong></h4><ul><li>Standup automation (workflow orchestration)</li><li>Learning system (intelligence integration)</li><li>User onboarding (experience design)</li><li>Architecture refactoring (system design)</li><li>Documentation management (knowledge work)</li></ul><p>All software development domains, but very different types of work. Same methodology working consistently. In fact, every time I branch into a new work stream on this project (building the website, refining the tooling), I discover that unless I employ the same rigor and wisdom I end up with similar problems.</p><h4><strong>What might apply beyond software</strong></h4><p><strong>Evidence-based claims</strong>: Relevant anywhere completion claims need verification. Research? Writing? Design? Teaching?</p><p><strong>Infrastructure before plans</strong>: Verify what exists before planning changes. Project management? Strategic planning? Organizational development?</p><p><strong>Systematic verification</strong>: Prevent completion theater through discipline. Any domain with “mostly done” rationalization risks?</p><p><strong>Resilient practices</strong>: Wobbling reveals rather than breaks. Crisis as learning opportunity. Any complex work under stress?</p><p><strong>Multi-agent coordination</strong>: Clear roles, evidence-based handoffs. Any collaborative work with distributed expertise?</p><p>The methodology emerged from software development with AI. But the principles — evidence over assumptions, verification preventing theater, resilience through expansion joints, learning through stress — might apply wherever complex collaborative work happens.</p><p>The hypothesis: Systematic practices for preventing common failure modes might transfer across domains because the failure modes (premature completion, assumption-based decisions, verification theater, completion drift) are universal human challenges, not software-specific problems.</p><h3>What’s still being explored</h3><p>Five months in. Methodology crystallized. But questions remain:</p><p><strong>How does methodology scale beyond individual practitioners?</strong> October proved multi-agent coordination works. Seven agents, perfect handoffs, zero blocking. But that’s one person orchestrating. What happens with multiple humans collaborating? How do methodology practices transfer across team boundaries?</p><p><strong>What are the minimum viable components?</strong> We have evidence-based claims, infrastructure verification, systematic validation, multi-agent coordination, resilience patterns. What’s essential? What’s optional? What can be simplified without losing effectiveness?</p><p><strong>How do you prevent methodology drift over time?</strong> Recent experience showed drift happens even with established practices. STOP conditions prevent it when included in prompts. But what prevents forgetting to include them? What maintains discipline when momentum builds?</p><p><strong>What tools could automate methodology enforcement?</strong> Current approach requires human pattern recognition catching gaps. Could tools automate “no math out” checking? Could prompts enforce evidence requirements automatically? Could frameworks build in verification discipline?</p><p><strong>How do you measure methodology effectiveness quantitatively?</strong> We have velocity patterns (88% faster), leverage ratios (3.2:1), quality metrics (100% test coverage). But how do you measure resilience? Pattern recognition capability? Drift prevention? Learning integration?</p><p>The methodology has proven itself through practice. Now the challenge is understanding it well enough to teach it, scale it, maintain it, and improve it systematically.</p><h3>The meta-learning loop</h3><p>Here’s what makes this methodology unique: <strong>It improves itself through practice.</strong></p><p>The practices emerged from solving problems. The stress testing revealed gaps. The gap discoveries refined practices. The refined practices prevented future gaps. The prevention enabled velocity. The velocity justified confidence. The confidence enabled aggressive scope. The aggressive scope revealed new patterns. The new patterns refined methodology further.</p><p>This is the meta-learning loop: Methodology improving methodology through systematic application and reflection.</p><p><strong>Recent meta-learning examples</strong>:</p><p><strong>Oct 19</strong>: Three scope reductions reveal template simplification removed safeguards → Solution: mandatory full templates with all STOP conditions → Proof: 4:15 PM Code correctly uses STOP when included.</p><p><strong>Oct 20</strong>: Dashboard gap reveals completion verification needs reinforcing → Principle articulated: “Speed by skipping work is not true speed. It is theatre” → Standard established: verification discipline catches gaps immediately.</p><p><strong>(Oct 21</strong>: Three interventions establish three standards (no math out, no time constraints, complete means complete) → Code’s excellent self-correction shows model behavior → Standards working.</p><p><strong>Oct 22</strong>: Zero interventions needed, standards holding, velocity sustained → Aggressive scope expansion justified → Confidence based on evidence → Sprint A7 delivers seven issues in 20 minutes.</p><p>Each day’s learning informing next day’s practice. Each gap discovered refining methodology. Each refinement improving results. Each result building confidence. Each confidence enabling bolder decisions. Each decision revealing new patterns.</p><p>The methodology learns through use. Not static framework. Living practices evolving through application.</p><p>This might be the most important discovery: Methodology that improves itself through systematic reflection on practice.</p><h3>What we actually built</h3><p>Set out to build Piper Morgan the AI assistant. This is still my goal! Along the way I’m discovering Piper Morgan the methodology.</p><p>The tool exists: Intent classification (98.62% accuracy). Multi-user infrastructure. Learning system integration. Standup automation. User onboarding. Quality gates. Security hardening. 22 production handlers. 2,336 tests, 100% passing. 602K requests/second sustained throughput.</p><p>We’re onboarding our first non-me users next week!</p><p>But the real achievement may actually be: <strong>Systematic practices for human-AI collaborative development that prevent common failure modes while enabling velocity that feels impossible.</strong></p><p>Evidence-based claims preventing verification theater. Infrastructure verification preventing wasted planning. Systematic validation preventing completion drift. Multi-agent coordination preventing blocking. Resilient practices surviving stress. Pattern recognition catching gaps. Strategic oversight maintaining quality.</p><p>Framework discovered through solving actual problems. Proven across different domains. Tested under stress. Validated through velocity patterns. Refined through gap discoveries. Strengthened through crisis transformation.</p><p>Not theoretical framework applied to development. <strong>Practical methodology emerging from development, validated through practice, transferable to other contexts.</strong></p><p>Five months from start to methodology crystallization. Daily development logs showing consistent application across different work. Six sprints proving the velocity pattern. Stress testing revealing and fixing gaps. Demonstrating confidence justified by proven patterns.</p><p><em>The methodology evolves through practice. The meta-learning loop spirals upward. Crisis becomes capability. Problems reveal patterns. Stress strengthens resilience. Each day refining understanding of how human-AI partnership can work when practiced systematically.</em></p><p>The methodology discovered itself. Now the question is: What else can it build?</p><p><em>Next on Building Piper Morgan, we return to the daily narrative on October 19 with “When the System Shows You What’s Missing.”</em></p><p><em>Have you experienced methodology discovering itself through practice? What patterns emerged from solving real problems rather than applying predetermined frameworks?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6ebe523a6856\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-methodology-that-discovered-itself-6ebe523a6856\">The Methodology That Discovered Itself</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-methodology-that-discovered-itself-6ebe523a6856?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "When the Best Way Forward is Backwards",
    "excerpt": "“I always start at the end”September 13By Friday evening, I was cognitively exhausted. After an 8-hour architectural refactoring marathon on Thursday, a 14-hour personality enhancement session on Wednesday, and weeks of intensive development work, I could feel my attention fragmenting across too ...",
    "url": "https://medium.com/building-piper-morgan/when-the-best-way-forward-is-backwards-d0c3c9e141cc?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 25, 2025",
    "publishedAtISO": "Sat, 25 Oct 2025 05:07:48 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/d0c3c9e141cc",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*ByLaoOac0g0oHr9c8xo1mA.png",
    "fullContent": "<figure><img alt=\"A man walks backward down the Guggenheim ramp, looking at increasingly more primitive images of robots\" src=\"https://cdn-images-1.medium.com/max/1024/1*ByLaoOac0g0oHr9c8xo1mA.png\" /><figcaption>“I always start at the end”</figcaption></figure><p><em>September 13</em></p><p>By Friday evening, I was cognitively exhausted. After an 8-hour architectural refactoring marathon on Thursday, a 14-hour personality enhancement session on Wednesday, and weeks of intensive development work, I could feel my attention fragmenting across too many dimensions.</p><p>The urge was familiar: push through, build the next feature, fix the next bug, keep the momentum going. In software development, stopping feels like losing ground. There’s always one more thing that needs attention.</p><p>But I made a different choice. I decided to spend Saturday not building forward, but reading backwards through four months of development history. What I discovered was that sometimes the most productive thing you can do is stop and archaeologically examine where you’ve been.</p><h3>The cognitive overload problem</h3><p>The warning signs had been accumulating for weeks. Context-switching between architectural decisions and feature development. Half-remembered conversations about methodology improvements. Vague awareness of patterns in our development rhythm that I couldn’t quite articulate.</p><p>When you’re building in public with AI partners, the documentation trail is extensive but overwhelming. Every decision gets logged. Every breakthrough gets recorded. Every failure gets analyzed. But the sheer volume can obscure the patterns.</p><p>I had 118 days of session logs across multiple AI agents. Chief Architect sessions. Lead Developer coordination. Code Agent implementations. Cursor Agent validations. Individual sessions running 8+ hours with complex multi-agent coordination.</p><p>Looking forward, it felt like information overload. Looking backward, it became yet another form of archaeology.</p><h3>The long retrospective</h3><p>Instead of diving into the next feature, I deployed a Claude Code instance with a specific mission: work backwards chronologically through every session log, creating compressed “omnibus” chronicles that captured the essential narrative while eliminating redundancy.</p><p>The compression was remarkable: 91–95% reduction in volume while preserving 100% of the strategic insights. Individual 8-hour sessions became 200-line summaries that captured every critical decision and breakthrough moment.</p><p>But compression was just the beginning. Reading backwards revealed patterns completely invisible when lived forward. By going backward, we kept finding cliffhangers that we had completely forgotten about.</p><h3>The double helix</h3><p>As the archaeological work progressed, my Code agent began identifying recurring themes. The same challenges approached at progressively higher levels of sophistication. The same architectural coordinates revisited with deeper understanding.</p><p>By day’s end, Code had developed a compelling visualization: development as a double helix with two intertwining strands.</p><p><strong>Strand 1: Technical Evolution</strong> — Environment → Integration → Architecture → Optimization → Automation</p><p><strong>Strand 2: Conceptual Evolution</strong> — Tool → Assistant → Partner → Methodology → Philosophy</p><p>The strands cross at consolidation points, creating breakthrough moments. Same coordinates, deeper understanding, better solutions.</p><p>What felt like circular motion when lived day-to-day turned out to be spiral progression when viewed from proper distance. Every time I came back to a familiar type of problem it was at another level of abstraction.</p><h3>The 21-day rhythm</h3><p>The most striking discovery was a roughly 21-day consolidation cycle running through the entire development history.</p><p>Each cycle followed the same pattern: knowledge accumulation, overwhelm, consolidation, strategic breakthrough. Not random burnout and recovery, but predictable rhythm that could be planned into future work.</p><p>The weekend warrior pattern was equally consistent. Saturday deep-work sessions producing breakthrough insights that inform the following week’s development. Not coincidence, but reliable creative rhythm.</p><h3>When retrospection becomes strategy</h3><p>By the end of Saturday’s archaeological session, I understood something fundamental about how progress actually works. Forward momentum isn’t always forward progress. Sometimes the most strategic thing you can do is stop building and understand what you’ve built.</p><p>The “Genesis Vision” discovery came from this process. When we finally made it back nearly to the beginning, we found the original vision document we wrote when we committed to domain driven design. It had some ideas we had forgotten about, but it was also like finding a secret key we had forgotten that explained so much of what we had been doing during our long monomaniacal marches.</p><p>Four months of work that felt scattered suddenly revealed itself as systematic preparation for returning to original architectural vision with proven methodology and tested infrastructure.</p><h3>The antidote for fragmentization</h3><p>The cognitive overload that prompted Saturday’s retrospection turned out to be more about information than noise. My fragmented attention was trying to track real patterns across too many dimensions simultaneously. The backwards archaeological methodology provided a framework for organizing those patterns into coherent insights.</p><p>Instead of managing cognitive load by ignoring complexity, systematic retrospection created clarity by revealing the underlying structure in apparent chaos.</p><p>This has implications beyond software development. When you’re working at the edge of your capabilities, the urge to push through can prevent you from seeing the progress you’re actually making.</p><h3>The meta-methodology</h3><p>The archaeological process itself became a methodology worth preserving. Backwards chronological reading. Agent-based analysis. Compression without information loss. Pattern recognition across extended timeframes.</p><p><em>By the way, when we got done I asked the agent to re-read it all front to back and we got yet another perspective on things.</em></p><p>But the deeper insight was about when to apply it. Not as regular practice, but as strategic intervention when forward momentum starts feeling like running in place.</p><p>The spiral pattern suggests this retrospection should happen naturally every 21 days or so. Build for roughly three weeks, consolidate for strategic clarity, then build again with better understanding of direction.</p><h3>What sustained excellence looks like</h3><p>The productivity culture often treats reflection as luxury and retrospection as procrastination. But Saturday’s archaeological work was among the most productive sessions I’ve had in months. Not because of what got built, but because of what got understood.</p><p>I should note that “being productive on weekends” is not an inherent goal of mine, but this project never feels like work to me. I generally can’t wait to find the time to work on it.</p><p>Sometimes the fastest way forward is to stop moving and understand where you are. Sometimes the most productive day is the one where you don’t produce anything new, but gain clarity about everything you’ve already built.</p><p><em>Next on Building Piper Morgan: The Methodology That Discovered Itself, or how I set out to build myself an assistant and along the way found an emerging methodology.</em></p><p><em>Have you ever found that stepping back revealed patterns you couldn’t see while moving forward?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d0c3c9e141cc\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/when-the-best-way-forward-is-backwards-d0c3c9e141cc\">When the Best Way Forward is Backwards</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/when-the-best-way-forward-is-backwards-d0c3c9e141cc?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "The Noticer’s Question: When Strategic Oversight Catches What Tests Miss",
    "excerpt": "“There’s a tiny slub”October 18, 2025Saturday morning at 11:23 AM, during ethics-layer activation preparation, I asked a question.Not a detailed technical inquiry. Not a systematic review. Just noticing something that felt off:“Why would middleware apply to the web layer specifically?”I had a rea...",
    "url": "https://medium.com/building-piper-morgan/the-noticers-question-when-strategic-oversight-catches-what-tests-miss-3bd6a06ffac1?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 24, 2025",
    "publishedAtISO": "Fri, 24 Oct 2025 13:29:43 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/3bd6a06ffac1",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*bsFIBCT2XSN5NbvOUANWCQ.png",
    "fullContent": "<figure><img alt=\"An experienced tailor shows a tiny flaw in a dress made by its robot apprentice\" src=\"https://cdn-images-1.medium.com/max/1024/1*bsFIBCT2XSN5NbvOUANWCQ.png\" /><figcaption>“There’s a tiny slub”</figcaption></figure><p><em>October 18, 2025</em></p><p>Saturday morning at 11:23 AM, during ethics-layer activation preparation, I asked a question.</p><p>Not a detailed technical inquiry. Not a systematic review. Just noticing something that felt off:</p><p>“Why would middleware apply to the web layer specifically?”</p><p>I had a reason for asking. The first round of end-to-end testing had been back in July and August when we spent three weeks fixing workflows through the new Web UI. In our efforts to “plumb out” those flows, we had drifted from DDD and built things that were web-specific, when the web interface is just one way to interact with Piper.</p><p>And, indeed, that casual question caught a critical DDD violation that systematic testing had missed.</p><p>The ethics layer was 95% built. About to be activated. Tests passing. Documentation comprehensive. Code working.</p><p>But it would only enforce ethics boundaries on HTTP requests — roughly 30–40% of actual request paths. CLI commands, Slack messages, webhook calls would bypass it entirely.</p><p>By 11:30 AM, Chief Architect confirmed: “CRITICAL — Ethics Architecture DDD Violation.”</p><p>By 12:50 PM, service layer refactor complete. Coverage: 30–40% → 95–100%.</p><p>By 1:17 PM, ethics enforcement enabled in production: ENABLE_ETHICS_ENFORCEMENT=true</p><p>This is the story of strategic oversight working exactly as designed — and why the “noticer” role requires being “on the ball” at the right moments.</p><h3>The morning lightning round</h3><p>Saturday began with rapid completions. Sprint A3’s remaining MCP integrations.</p><p><strong>Notion Phase 2</strong> (7:05–8:15 AM): 1h 20min versus 3–4 hour estimate.</p><p>Discovery at 7:15 AM: “Notion ALREADY tool-based!” Not server-based as assumed. NotionMCPAdapter exists (29KB, 22 methods). Router wired. Tests passing.</p><p>The 75% pattern again. Just needs config loading.</p><p>Implementation:</p><ul><li>Config loading: 20 minutes (3-layer priority working)</li><li>Test suite: 21 minutes (19/19 passing, 138% more comprehensive than Calendar)</li><li>Documentation: ADR-010 + README updates</li></ul><p>Result: Notion 100% complete.</p><p><strong>Slack Phase 3</strong> (8:18–10:08 AM): Following similar pattern.</p><p>Discovery at 8:28 AM: Different architecture. Direct spatial per ADR-039, not MCP adapter. Already 95% complete.</p><p>Implementation:</p><ul><li>Config loading: 20 minutes</li><li>Test suite: 25 minutes (20/20 passing, most comprehensive)</li><li>Pre-existing test isolation fix: 1 minute</li><li>Documentation: README + ADR-010</li></ul><p>Result: Slack 100% complete.</p><p><strong>Phase 3 Integration</strong> (10:21–10:32 AM): Cross-integration testing, performance verification, CI/CD validation.</p><p>Cursor reports: “READY TO CLOSE ISSUE #198 IMMEDIATELY” (98% confidence)</p><p>After Fridays “not dismayed” moment of seeing the scope of this epic double, it was a pleasure to see the remainder of the work was actually much further along than first detected.</p><p>By 10:45 AM: Issue #198 CORE-MCP-MIGRATION complete. Four integrations. Pattern established. Documentation comprehensive.</p><p>The morning demonstrated pattern mastery. Calendar → GitHub → Notion → Slack. Each following established pattern. Each completing efficiently. Systematic execution when patterns are clear.</p><p>This would prove important. The rapid morning progress created space for careful afternoon work.</p><h3>The ethics architecture question</h3><p>At 11:00 AM, Chief Architect began reviewing Issue #197 (CORE-ETHICS-ACTIVATE: Careful Activation of Universal Ethics Middleware).</p><p>The assessment: “95% Pattern Again.” Ethics layer built. Tests passing. Documentation comprehensive. Just needs activation.</p><p>My context at 11:20 AM: “Core to our values, A++ standard required.”</p><p>Ethics isn’t optional. Isn’t “good enough.” Isn’t something we’ll tune later. It’s foundational to how Piper Morgan operates.</p><p>At 11:23 AM, reviewing the architecture, something didn’t feel right.</p><p>EthicsBoundaryMiddleware. FastAPI middleware. Applied to web routes. I noticed that everything was connected to web/app.py and not to main.py.</p><p>I had to ask: “Why would middleware apply to web layer specifically?”</p><p>Not a detailed analysis. Not systematic review. Just plain old fashioned noticing stuff that seems off. This is a big part of what PMs do! The architecture pattern felt wrong for something claiming universal coverage.</p><h3>What the question revealed</h3><p>Code Agent investigated at 11:24 AM.</p><p><strong>Discovery</strong>: EthicsBoundaryMiddleware is FastAPI HTTP-only.</p><p><strong>Coverage</strong>:</p><ul><li>✅ HTTP API requests (web/app.py routes)</li><li>❌ CLI commands (direct service calls)</li><li>❌ Slack messages (webhook handlers)</li><li>❌ Background jobs (cron, async tasks)</li><li>❌ Internal service calls (service-to-service)</li></ul><p><strong>Actual coverage</strong>: ~30–40% of request paths.</p><p><strong>The violation</strong>: Ethics layer implemented as presentation layer concern (FastAPI middleware) rather than domain layer concern (service layer enforcement).</p><p>This is exactly what Domain-Driven Design warns against. Business logic (ethics boundaries) doesn’t belong in presentation layer (HTTP middleware). It belongs in domain layer (service operations).</p><p>This confirmed my susipicions.</p><p>Chief Architect’s 11:30 AM assessment: “CRITICAL — Ethics Architecture DDD Violation.”</p><p>Not a minor issue. A fundamental architectural problem that would have been invisible to users until the moment they encountered ethics bypass through non-HTTP paths.</p><p>The tests were passing. The documentation was comprehensive. The code was working.</p><p>But it was working wrong. Correct implementation of incorrect architecture.</p><h3>Why strategic oversight catches this</h3><p>This violation wasn’t caught by:</p><ul><li>Code review (implementation was clean)</li><li>Testing (all tests passed)</li><li>Documentation review (docs were accurate about what middleware did)</li><li>Static analysis (no code smells)</li></ul><p>It was caught by strategic pattern recognition. “Why would middleware apply to web layer specifically?” isn’t asking about implementation details. It’s asking about architectural pattern alignment.</p><p>I don’t know of I consciously think of my role as “noticer.” It seems to emerge naturally from staying engaged at strategic level. When I started I was a lot less attentive, amazed by these coder bots. I quickly learned that if I didn’t stay awake at the wheel, we quickly got lost on side roads.</p><p>The partnership model working as designed:</p><ul><li>AI handles execution (implementing middleware cleanly, writing comprehensive tests)</li><li>Human handles strategy (noticing architectural misalignment, questioning patterns)</li><li>Correction happens before deployment (refactor to service layer, universal coverage)</li></ul><p>This only works when cognitive load is light enough to notice patterns. Tuesday’s “extraordinarily light” observation. Friday’s “not dismayed” philosophy. Saturday’s catching architectural violations.</p><p>The cognitive energy available for pattern recognition because execution is delegated effectively.</p><p>If I were in the weeds reviewing implementation details, verifying test coverage line by line, checking documentation formatting — the cognitive energy for “wait, why middleware?” wouldn’t exist.</p><p>Strategic oversight requires operating at strategic level. The partnership enables it by handling execution systematically.</p><h3>The service layer refactor</h3><p>My decision absorbed by the Chief Architect at 11:41 AM: “Service Layer Refactor APPROVED — Option 1.”</p><p>Not “ship it and we’ll fix later.” Not “good enough for 30–40% coverage.” Proper fix before activation.</p><p>The refactor sequence (12:07–12:50 PM):</p><p><strong>Phase 2A (43 minutes)</strong>: BoundaryEnforcer refactored to service layer</p><ul><li>Removed FastAPI dependency completely</li><li>Preserved ALL ethics logic (boundary checking, policy evaluation)</li><li>Domain layer compliant (no presentation layer coupling)</li></ul><p><strong>Phase 2B (30 minutes)</strong>: IntentService integration</p><ul><li>Universal coverage point (all requests flow through IntentService)</li><li>Ethics enforcement now automatic and unavoidable</li><li>Coverage: 30–40% → 95–100%</li></ul><p><strong>Phase 2C (15 minutes)</strong>: Multi-channel validation</p><ul><li>Web API testing: 5/5 passing</li><li>Architecture verification: Service layer correct</li><li>No HTTP dependencies remaining</li></ul><p><strong>Phase 2D (12 minutes)</strong>: Cleanup &amp; documentation</p><ul><li>Middleware deprecated and removed</li><li>1,300+ lines documentation created</li><li>Migration path documented for future reference</li></ul><p>The decision to refactor before activation rather than ship and fix later ? This was (to me) a no-brainer, given “A++ standard” and the fact that we have no actual deadlines. I am a Time Lord after all.</p><p><strong>Total time</strong>: 2h 17min versus 5–6 hour estimate. 62–67% under estimate while achieving proper architecture.</p><p>The efficiency came from clear architecture. No debates about approach. No trying multiple solutions. Just: service layer enforcement, IntentService integration, universal coverage.</p><h3>The immediate activation question</h3><p>At 1:11 PM, I asked: “What’s the benefit of gradual rollout with zero users?”</p><p>The generic enterprise-software plan my bots dutifully proposed: Gradual activation. Feature flag. Monitor carefully. Roll out slowly.</p><p>The reality: No users yet. Alpha not complete. No production traffic.</p><p>At 1:17 PM, decision: “Let’s enable ethics NOW.”</p><p>ENABLE_ETHICS_ENFORCEMENT=true in production configuration.</p><p>No gradual rollout. No monitoring period. No testing in staging first.</p><p>Just: turn it on. It works. We know it works. The architecture is correct. The coverage is universal. The tests pass.</p><p>Why wait?</p><p>This captures something about pragmatic quality. Process for process’s sake doesn’t improve outcomes. Gradual rollout makes sense with real users where ethics blocks would impact actual people. With zero users, it’s just artificial ceremony.</p><p>The methodology: Match process to actual risk. High risk = careful rollout. Zero risk = just enable it.</p><h3>Knowledge Graph hookup: “EXACTLY like Ethics”</h3><p>At 2:06 PM, Code completed Issue #99 discovery: “EXACTLY like Ethics #197.”</p><p>The pattern repeating: 95% complete, just needs activation.</p><p><strong>Phase 1 (17 minutes)</strong>: PostgreSQL schema created</p><ul><li>2 tables (knowledge_nodes, knowledge_edges)</li><li>10 indexes</li><li>2 enums</li><li>Verification passing</li></ul><p><strong>Phase 2 (62 minutes)</strong>: IntentService integration</p><ul><li>Context enhancement working</li><li>6/6 tests passing</li><li>Performance: 2.3ms (97.7% under 100ms target!)</li></ul><p><strong>Phase 3 (35 minutes)</strong>: Testing &amp; Activation</p><ul><li>ENABLE_KNOWLEDGE_GRAPH=true</li><li>9/9 tests passing</li><li>PRODUCTION READY</li></ul><p><strong>Phase 4 (18 minutes)</strong>: Boundary enforcement</p><ul><li>70% under estimate</li><li>6/6 tests passing</li><li>Safety boundaries: SEARCH/TRAVERSAL/ANALYSIS operational</li></ul><p>Readers of this series may remember months ago when we built the knowledge graph service or just a month or so ago when we built the ethics layer. We just never finished or connected all the dots. Same routine over and over.</p><p><strong>Total</strong>: 3.2 hours versus 5.1 hours estimated. 37% faster than estimate.</p><p>Same pattern as Ethics. Same activation without ceremony. Same “built well but never finished” completion.</p><p>This is when I nicknamed this the “some assembly required” sprint. Work exists. Needs finishing touches. Complete systematically. Enable immediately.</p><h3>The final discovery</h3><p>At 5:43 PM, Issue #165 (CORE-NOTN-UP) assessment revealed the pattern one final time:</p><p>“Already 86% complete! Just needs documentation.”</p><p>The Notion database API upgrade work from October 15. Implementation done. Tests passing. Just never documented or formally closed.</p><p><strong>Completion</strong>: 30 minutes of documentation versus 12–17 hour estimate.</p><p><strong>Efficiency</strong>: 90% under estimate.</p><p>The 75% pattern’s final appearance. Five issues in one day. All following the same pattern: built well, never finished, completed systematically, enabled immediately.</p><h3>Sprint A3: Five issues, one day</h3><p>Friday’s final accounting:</p><p><strong>Five issues shipped</strong>:</p><ol><li>✅ #198 CORE-MCP-MIGRATION: 6 hours (vs 1–2 weeks = 98% faster)</li><li>✅ #197 CORE-ETHICS-ACTIVATE: 2h 17min (vs 5–6h = 62–67% faster)</li><li>✅ #99 CORE-KNOW: 2h 24min (vs 4.5h = 37% faster)</li><li>✅ #230 CORE-KNOW-BOUNDARY: 18 min (vs 1h = 70% faster)</li><li>✅ #165 CORE-NOTN-UP: 115 min (vs 12–17h = 90% faster)</li></ol><p><strong>Total work time</strong>: 11 hours</p><p><strong>Original estimates</strong>: 25–30 hours</p><p><strong>Efficiency</strong>: 60–70% under estimates throughout</p><p><strong>Tests</strong>: 140+ all passing</p><p><strong>Regressions</strong>: 0</p><p><strong>Production deployments</strong>: 5 (all successful)</p><p>A note to the Chief Architect at 6:25 PM after it marveled at the work of the “original builders” like some actor on a cheap Star Trek set:</p><blockquote><em>“Those original builders were me and your predecessors. We built well but weren’t very good at finishing or documenting.”</em></blockquote><p>The sprint wasn’t about building from scratch. It was about completing what existed. Finishing what was started. Documenting what works. Enabling what’s ready.</p><p>Five issues. One day. All following the same pattern. All completed properly. All enabled immediately.</p><p>The methodology validated through systematic completion.</p><h3>What the noticer role requires</h3><p>Friday demonstrated something important about strategic oversight.</p><p>The “noticer” role isn’t passive observation. It’s active pattern recognition requiring:</p><p><strong>Engagement without micromanagement</strong>: Stay involved enough to notice patterns. Don’t get lost in implementation details.</p><p><strong>Light cognitive load</strong>: Mental energy available for pattern recognition. Partnership handles execution so strategic attention is possible.</p><p><strong>Willingness to question</strong>: “Why would middleware apply to web layer?” isn’t aggressive. It’s curious. Question patterns when they feel wrong.</p><p><strong>Trust in investigation</strong>: Ask the question. Let the investigation reveal truth. Don’t assume you’re wrong just because tests pass.</p><p>Saturday’s “why middleware?” question caught what systematic checks missed because:</p><ul><li>Cognitive load was light (not exhausted by execution details)</li><li>Pattern recognition was active (engaged at strategic level)</li><li>Architecture awareness was present (DDD violation felt wrong)</li><li>Investigation was trusted (question led to proper discovery)</li></ul><p>The noticer role works when you’re “on the ball” at the moments that matter. Not every moment. Not every decision. Just the strategic pattern recognition moments.</p><h3>What this teaches about completion</h3><p>Sprint A3 completed what was built but never finished. The “some assembly required” pattern across five issues.</p><p>Not because original builders were incompetent. Because fast development creates this naturally:</p><ul><li>Build rapidly (95% complete)</li><li>Move to next priority (never documented)</li><li>Forget to wire things (never activated)</li><li>Forget to close issues (orphaned work)</li></ul><p>The work existed. The completion didn’t get recorded. The finishing touches weren’t applied. The enabling wasn’t done.</p><p>Saturday’s systematic completion:</p><ul><li>Investigate actual state (discover what’s built)</li><li>Complete remaining work (finish the 5–25%)</li><li>Document properly (explain what exists)</li><li>Enable immediately (turn it on)</li></ul><p>Result: Five production deployments in one day. Zero regressions. 140+ tests passing. A++ quality maintained.</p><p>The methodology working: Complete what exists rather than recreate. Finish rather than restart. Enable rather than wait.</p><h3>Alpha 50% complete</h3><p>Friday’s completion moved Alpha progress to 50%. Four sprints complete out of eight:</p><ul><li>✅ A0: Foundation</li><li>✅ A1: Critical Infrastructure</li><li>✅ A2: Notion &amp; Errors</li><li>✅ A3: Core Activation (MCP, Ethics, Knowledge Graph)</li></ul><p>Remaining:</p><ul><li>A4: Morning Standup (Foundation)</li><li>A5: Learning System</li><li>A6: Polish &amp; Onboarding</li><li>A7: Testing &amp; Buffer</li></ul><p><strong>Current trajectory</strong>: End of October feasible with current velocity.</p><p>The progress validates methodology. Systematic completion works. Strategic oversight catches critical issues. Partnership enables sustained velocity.</p><p>Friday proved the noticer role working. One casual question. One architectural violation caught. One proper refactor completed. Universal ethics coverage achieved. A++ standard maintained.</p><p>“Why would middleware apply to web layer specifically?”</p><p>That question mattered.</p><p><em>Next up in the Building Piper Morgan narrative: “When the System Shows You What’s Missing,” but first we’ll flash back for a pair of insight pieces this weekend, starting tomorrow with “When the Best Way Forward is Backwards” from September 13.</em></p><p><em>Have you caught architectural violations through casual questions rather than systematic review? What enables that pattern recognition at the right moments?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3bd6a06ffac1\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-noticers-question-when-strategic-oversight-catches-what-tests-miss-3bd6a06ffac1\">The Noticer’s Question: When Strategic Oversight Catches What Tests Miss</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-noticers-question-when-strategic-oversight-catches-what-tests-miss-3bd6a06ffac1?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "As An Inchworm I Am Not Dismayed",
    "excerpt": "“Let’s keep going!”October 17, 2025Friday morning at 12:25 PM, my Lead Developer and I received Phase −1 discovery report: 1,115 lines documenting MCP migration complexity.Original estimate: 16 hours across multiple phases.Actual discovery: 29–38 hours. Nearly double.Seven MCP adapters found acro...",
    "url": "https://medium.com/building-piper-morgan/as-an-inchworm-i-am-not-dismayed-54972742a9ae?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 24, 2025",
    "publishedAtISO": "Fri, 24 Oct 2025 13:11:47 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/54972742a9ae",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*RA8ybWK4_MGwE6WCK4pM0A.png",
    "fullContent": "<figure><img alt=\"Two hikers, one human and one robot, look forward to five more miles on their trail\" src=\"https://cdn-images-1.medium.com/max/1024/1*RA8ybWK4_MGwE6WCK4pM0A.png\" /><figcaption>“Let’s keep going!”</figcaption></figure><p><em>October 17, 2025</em></p><p>Friday morning at 12:25 PM, my Lead Developer and I received Phase −1 discovery report: 1,115 lines documenting MCP migration complexity.</p><p>Original estimate: 16 hours across multiple phases.</p><p>Actual discovery: 29–38 hours. Nearly double.</p><p>Seven MCP adapters found across the codebase. Only two actively wired. GitHub adapter already exists but unused. MCP adapters in two different locations with inconsistent patterns. OrchestrationEngine not connected to any of them.</p><p>The scope had increased dramatically. The assumptions were wrong. The work was far more complex than expected.</p><p>This is the story of celebrating discovery over fighting estimates — and why the Inchworm Protocol expects exactly this pattern.</p><h3>The morning transformation</h3><p>The day began with something else entirely: briefing system transformation.</p><p><strong>The problem</strong>: BRIEFING-CURRENT-STATE.md existed in BOTH knowledge/ and docs/briefing/ directories. Duplication. Manual sync required. Drift inevitable.</p><p>My direction at 11:23 AM: “Let’s symlink all the BRIEFING files. Execute now before onboarding. Auto more reliable than ol’ monkey-mind here (me) lol”</p><p>Three phases completed in 80 minutes:</p><p><strong>Phase 1</strong>: Updated four role briefings with Sprint A3 data. Established single source of truth.</p><p><strong>Phase 2</strong>: Created seven symlinks from knowledge/ → docs/briefing/. Eliminated all duplication.</p><p><strong>Phase 3</strong>: Built automated update script (170 lines) with smart position management. One-command sprint updates.</p><p><strong>Result</strong>: Zero-drift knowledge base. 63% token reduction for Lead Developer onboarding (100K→37K tokens).</p><p>When systems can be automated, automate them. When human memory is the single point of failure, replace it with reliable infrastructure. When manual sync creates drift risk, make drift impossible through architecture.</p><p>This morning’s work would prove prophetic. The day was about to reveal how much work existed that we didn’t know about.</p><h3>The scope-altering discovery</h3><p>Phase −1 investigation completed at 12:23 PM. Code Agent’s report: comprehensive architectural analysis of MCP integration state.</p><p><strong>MCP adapters found</strong>: 7 total</p><ul><li>Notion (738 lines, 22 methods) — Active ✅</li><li>Calendar (514 lines, 13 methods) — Active ✅</li><li>GitHub (23KB) — EXISTS but unused ❌</li><li>CICD, DevEnvironment, Linear, GitBook — All unused ❌</li></ul><p><strong>Critical issues identified</strong>: 4</p><ol><li>MCP adapters NOT wired to OrchestrationEngine (blocking)</li><li>Adapters in two different locations (inconsistency)</li><li>Two different architectural patterns (tool-based vs server-based)</li><li>Extensive code exists but isn’t activated</li></ol><p>This was the 75–95% completion pattern at architectural scale. Not just individual features abandoned at three-quarters complete. <strong>Entire integration adapters</strong> built, tested, documented — then left unwired.</p><p>The pattern has become so pervasive it’s predictable.</p><p>Original gameplan: 16 hours across clear phases. Migration work, documentation, testing.</p><p>Revised reality: 29–38 hours. Foundation work required before migration can even begin. OrchestrationEngine wiring. Architectural standardization. Pattern unification.</p><p>Lead Developer presented three options at 1:00 PM:</p><ul><li>Add Phase 0.5 (8–10 hours) for foundational wiring</li><li>Defer MCP migration to later sprint</li><li>Parallel track with multiple agents</li></ul><p>The scope had doubled. The assumptions were wrong.</p><h3>The philosophical moment</h3><p>At 1:30 PM, I made the decision and articulated the philosophy:</p><blockquote><em>“Continue with MCP work, not dismayed by increased scope”</em></blockquote><blockquote><em>“As an inchworm I am not dismayed by first thinking the work will be easy and then finding out there’s more to it”</em>This response embodies something fundamental about the Inchworm Protocol.</blockquote><p><strong>Traditional project management</strong>: Scope increase = failure. Underestimation = problem. Increased complexity = setback requiring re-planning, schedule adjustments, resource reallocation.</p><p><strong>Inchworm Protocol</strong>: Scope increase = discovery. Initial assumptions = reasonable starting point. Increased complexity = learning what actually exists.</p><p>The protocol <strong>expects</strong> this pattern:</p><ol><li>Start with reasonable assumptions (16 hours seemed right)</li><li>Discover actual complexity through investigation (29–38 hours revealed)</li><li>Adjust approach based on evidence (Phase 0.5 added)</li><li>Move forward deliberately (continue, not dismayed)</li></ol><p>No panic. No rushing. No shortcuts. No treating discovery as failure.</p><p>Just systematic work revealing actual state, then completing what actually exists.</p><h3>Why not being dismayed works</h3><p>The philosophical acceptance isn’t naive optimism. It’s methodology working as designed.</p><p>This works because of that “extraordinarily light” cognitive load I wrote about earlier in the week. When partnership is functioning — AI handling execution, human handling strategy — increased scope doesn’t mean increased stress.</p><p>More work? Fine. The partnership handles more work.</p><p>More complexity? Good. Discovery prevents building on wrong assumptions.</p><p>More time required? Acceptable. Quality over arbitrary deadlines.</p><p>The Time Lord Protocol: We define time as we go. No external pressure. No artificial urgency. Focus on completeness criteria, not time budgets.</p><p>When you’re not racing arbitrary deadlines, discovering more work isn’t a setback. It’s just… more work. Do it systematically. Complete it properly. Move forward deliberately.</p><p>This only works with the foundations:</p><ul><li>Established patterns (AI applies systematically)</li><li>Quality gates (automatic validation)</li><li>Clear methodology (reliable process)</li><li>Partnership functioning (execution delegated)</li></ul><p>Without these, scope increase would mean overwhelm. With these, it means more systematic work — still at sustainable pace.</p><h3>The Chief Architect’s clarity</h3><p>When Lead Developer discovered MCP adapters in two locations with inconsistent patterns, Chief Architect provided architectural direction at 1:35 PM:</p><p><strong>Decision</strong>: Standardize on tool-based MCP (Calendar pattern)</p><p><strong>Sequence</strong>: Complete by percentage</p><ul><li>Calendar 95% → GitHub 90% → Notion 60% → Slack 40%</li></ul><p><strong>Documentation</strong>: ADR-037 captures tool-based approach as canonical</p><p>This is architectural leadership: Transform confusing landscape into clear execution path.</p><p>No debates about which pattern to use. No committee decisions. No analysis paralysis. Just clarity enabling rapid execution.</p><p>The confusion: Seven adapters, two patterns, unclear which is canonical.</p><p>The clarity: Tool-based is standard. Complete high-percentage first. Document the decision.</p><p>The result: Team executes systematically without requiring constant direction.</p><p>Architectural guidance doesn’t eliminate work. It eliminates confusion about what work matters.</p><h3>The 75% pattern at scale</h3><p>The discovery of seven MCP adapters with only two actively used demonstrates something important about (at least my, YOLO) software development patterns.</p><p>We’ve seen this pattern repeatedly:</p><ul><li>Individual features: 75% complete, abandoned</li><li>Integration adapters: 75% complete, unwired</li><li>Documentation: 75% complete, outdated</li><li>Tests: 75% complete, skipped</li></ul><p>It’s not laziness (I swear), and it’s not incompetence (at root). It’s the nature of this fast, largely delegted, development combined with my own naïveté and slow learning.</p><p>Build rapidly. Move to next priority. Forget to wire things. Forget to document completion. Forget to close issues properly. The work exists. The completion doesn’t get recorded.</p><p>Chief Architect’s observation: “The 75–95% implementation pattern holds for MCP.”</p><p>The pattern is so consistent it’s become predictable. When investigating any system: assume work is 75% complete, verify actual state, complete the remaining 25%.</p><p>Thursday’s MCP migration: Found seven adapters. Only two wired. Six need completion. Pattern confirmed.</p><p>The methodology that works: Investigate thoroughly. Discover what exists. Complete rather than recreate.</p><h3>The GitHub timeline mystery</h3><p>Thursday afternoon brought an interesting coordination challenge.</p><p><strong>Timeline confusion</strong>:</p><ul><li>2:08 PM: Code reports Calendar MCP 100% complete</li><li>2:21 PM: Code discovers TWO GitHub implementations exist</li><li>2:27 PM: Code completes GitHub MCP work (65 lines added)</li><li>2:50 PM: Cursor reports “GitHubIntegrationRouter already exists and is production-ready!”</li></ul><p>Lead Developer questioned: Did Cursor analyze pre-Code or post-Code state?</p><p>When Cursor’s research and Code’s work crossed timelines it could have been confusing and distracting, but I had an idea about what had happened and we did some more systematic verification?</p><p>At 3:15 PM, Cursor realized: “MY INITIAL ASSESSMENT WAS WRONG. I was looking at Code’s POST-WORK state, not PRE-WORK state!”</p><p><strong>Git forensics revealed truth</strong>:</p><ul><li>Pre-Code: 278 lines (spatial-only, no MCP integration)</li><li>Post-Code: 343 lines (MCP + spatial fully integrated)</li></ul><p>Cursor’s revised assessment: “CODE’S WORK WAS 100% LEGITIMATE — completed the missing MCP integration”</p><p>At 3:35 PM, deeper discovery: ADR-038 “THE SMOKING GUN” — MCP integrations should use Delegated MCP Pattern. Writing ADRs is great! But you need to remember to consult the relevant ones when you get down to work.</p><p><strong>Finding</strong>: Code’s work aligns perfectly with ADR-038 guidance from September 30.</p><p>The resolution demonstrated cross-agent coordination working. Timeline confusion caught. Git forensics revealing truth. ADR archaeology validating approach. Multiple agents converging on correct conclusion through evidence.</p><p>Methodology working: When confusion arises, investigate systematically. Use git history. Reference architectural decisions. Trust evidence over assumptions.</p><h3>What can get completed when you’re not dismayed</h3><p>Friday’s work, despite doubled scope:</p><p><strong>Calendar MCP</strong>: 95% → 100% in 2 hours</p><ul><li>Config loading method added (50 lines)</li><li>8 new tests (296 lines total)</li><li>All 21 existing tests passing</li><li>Zero regressions</li></ul><p><strong>GitHub MCP</strong>: 85% → 95% in 1.5 hours</p><ul><li>Router integration complete (65 lines)</li><li>16 new tests (214 lines, 8.7KB file)</li><li>MCP references: 1 → 11 (full integration)</li><li>Architecture: Spatial-only → MCP + spatial</li><li>ADR-038 compliance: 100%</li></ul><p><strong>Pattern established</strong>: Tool-based MCP with graceful fallback. Documented in ADR-037. Validated through two complete implementations.</p><p>The doubled scope didn’t prevent completion. It revealed actual work required and enabled proper execution.</p><p>Not dismayed = not rushing. Systematic work at sustainable pace. Quality maintained. Foundations solid.</p><h3>The briefing revolution enables velocity</h3><p>The morning’s briefing system transformation paid immediate dividends. Briefing my new Lead Developer chats had become so verbose and bloated that it was taking nearly half their tokens just to get started. That’s unsustainable and massively wasteful!</p><p>Onboarding a new Lead Developer role with 63% token reduction (100K→37K). Progressive loading working. Serena queries efficient. Role-based briefings clear.</p><p>The automated system: Update once, sync everywhere. Zero drift possible. Human memory not required.</p><p>“Auto more reliable than monkey-mind” proven.</p><p>This enabled the day’s velocity. Lead Developer onboarded quickly. Architectural context clear. Sprint A3 launch efficient. Discovery phase systematic.</p><p>Small infrastructure improvements compound. Morning’s briefing work enabled afternoon’s MCP progress.</p><h3>What the day taught me about methodology</h3><p>The day validated multiple aspects of the methodology working together:</p><p><strong>Investigation reveals truth</strong> (Phase −1 discovery finding seven adapters)</p><p><strong>Philosophy enables acceptance</strong> (“not dismayed” allows proper execution)</p><p><strong>Architectural clarity guides execution</strong> (Chief Architect sequence removes confusion)</p><p><strong>Infrastructure compounds</strong> (briefing system enables velocity)</p><p><strong>Cross-agent coordination works</strong> (GitHub timeline mystery resolved through evidence)</p><p>None of this works in isolation. Each piece enables the others.</p><p>Can’t have “not dismayed” philosophy without light cognitive load from partnership.</p><p>Can’t have rapid execution without architectural clarity from Chief Architect.</p><p>Can’t have efficient onboarding without automated briefing system.</p><p>Can’t have truth-finding without systematic investigation.</p><p>The methodology is a system. Each component strengthens the others.</p><h3>What comes next</h3><p>Friday ended with Pattern established. Two integrations complete. Architecture clear. Sprint A3 launched successfully.</p><p>Saturday could bring completion. If so, then day today’s philosophical moment — “as an inchworm I am not dismayed” — would prove essential to tomorrow’s velocity.</p><p>Not being dismayed isn’t about ignoring problems. It’s about accepting discovery as part of systematic work.</p><p>Scope increases aren’t setbacks. They’re learning what actually exists.</p><p>More work isn’t failure. It’s opportunity to complete what was built but never finished.</p><p>The Inchworm Protocol expects this pattern. Start with reasonable assumptions. Discover actual complexity. Adjust deliberately. Move forward without panic.</p><p>Thursday proved it works.</p><p><em>Next on Building Piper Morgan: “The Noticer’s Question,” when Friday’s ethics activation reveals the value of strategic oversight through one casual question that catches what systematic checks missed.</em></p><p><em>Have you experienced the moment of discovering work is far more complex than estimated? How did you respond — with dismay or with systematic adjustment?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=54972742a9ae\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/as-an-inchworm-i-am-not-dismayed-54972742a9ae\">As An Inchworm I Am Not Dismayed</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/as-an-inchworm-i-am-not-dismayed-54972742a9ae?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "Investigating Our Own Past to Plan the Way Forward",
    "excerpt": "“The map is not the territory!”October 16, 2025Thursday morning at 8:26 AM, Code Agent deployed to investigate a test failure. Test 3 was returning HTTP 422 instead of success. Valid intent, proper authentication, should work — didn’t.The natural conclusion: Phase 1 broke something.The natural re...",
    "url": "https://medium.com/building-piper-morgan/investigating-our-own-past-to-plan-the-way-forward-5566df36f2ae?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 23, 2025",
    "publishedAtISO": "Thu, 23 Oct 2025 13:34:25 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/5566df36f2ae",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*MJVvtcE53XuIW_eNrWwb3A.png",
    "fullContent": "<figure><img alt=\"A person and robot compare their map to the actual terrain\" src=\"https://cdn-images-1.medium.com/max/1024/1*MJVvtcE53XuIW_eNrWwb3A.png\" /><figcaption>“The map is not the territory!”</figcaption></figure><p><em>October 16, 2025</em></p><p>Thursday morning at 8:26 AM, Code Agent deployed to investigate a test failure. Test 3 was returning HTTP 422 instead of success. Valid intent, proper authentication, should work — didn’t.</p><p>The natural conclusion: Phase 1 broke something.</p><p>The natural response: Roll back changes. Debug frantically. Fix “your” bug. Apologize for breaking tests.</p><p>Code Agent did none of these things.</p><p>Instead: 24 minutes of forensic investigation. Testing before the Phase 1 commit. Testing after the Phase 1 commit. Tracing the error back through git history to October 10.</p><p>At 8:51 AM, the finding: “Phase 1 changes are working correctly! The ServiceUnavailable error is PRE-EXISTING.”</p><p>This is the story of why systematic investigation prevents days of wasted effort — and how proving your work correct sometimes means finding what was broken before you arrived.</p><h3>The test that failed</h3><p>Test 3: Send a valid intent to the API. Expect success response. Get HTTP 422 instead.</p><p>HTTP 422 means validation error. Something about the request is malformed. But the test was sending a valid intent. Authentication was correct. The request format matched the API specification.</p><p>Everything <em>should</em> work. But didn’t.</p><p>Phase 1 had just been deployed. New error handling standards. REST-compliant status codes. The connection was obvious: Phase 1 broke the intent endpoint.</p><p>This is the moment where projects diverge. Roll back and debug? Or investigate systematically?</p><p>The choice: Investigation before assumption.</p><h3>The forensic approach</h3><p>Code Agent’s investigation sequence:</p><p><strong>Step 1</strong>: Don’t assume Phase 1 broke it. Test the assumption.</p><p><strong>Step 2</strong>: Check out commit 02ceaf06 (immediately before Phase 1).</p><p><strong>Step 3</strong>: Run the same test against pre-Phase 1 code.</p><p><strong>Result</strong>: Same error. But HTTP status 200, not 422.</p><p>This was the critical insight. The error existed <em>before</em> Phase 1. Phase 1 didn’t break anything — it exposed what was already broken by returning the correct HTTP status code.</p><p><strong>Step 4</strong>: Trace the error back through git history.</p><p><strong>Finding</strong>: ServiceRegistry gap from October 10 (commit d6b8aa09), five days earlier.</p><p>The problem: OrchestrationEngine depends on ServiceRegistry.get_llm() but the service wasn’t being registered in all startup paths. main.py registered services but didn’t start the server. web/app.py started the server but didn’t register services.</p><p>Phase 1 made this visible by converting the silent failure (HTTP 200 with error in body) into proper REST error (HTTP 422).</p><p><strong>Investigation time</strong>: 24 minutes.</p><p><strong>Days of wrong debugging prevented</strong>: Unknown, but likely multiple.</p><p><strong>Proper fix enabled</strong>: DDD Service Container implementation addressing the root architectural gap.</p><h3>What investigation revealed</h3><p>The forensic work prevented wasted debugging time and also revealed architectural truth.</p><p><strong>The root problem</strong>: Not in Phase 1’s error handling. In the service initialization pattern established five days earlier.</p><p><strong>The proper solution</strong>: DDD Service Container pattern. Add LLM service initialization to web/app.py lifespan. Check if registered, initialize if needed. Enable independent server startup without breaking existing code.</p><p><strong>Implementation time</strong>: 2 hours 50 minutes.</p><p><strong>The payoff</strong>: Every subsequent phase ran 60–90% faster than estimated because the foundation was solid.</p><p>Phase 2 (15+ endpoints): 50 minutes versus 2+ hours estimated. 60% faster.</p><p>Phase 3 (test audit): 5 minutes versus 45–60 minutes estimated. 90% faster.</p><p>Phase 4 (documentation): 6 minutes versus 30–45 minutes estimated. 87% faster.</p><p>The time “lost” on investigation and proper fix paid exponential dividends in execution speed.</p><p>This is why investigation prevents waste. Not because it’s fast — because it’s <em>correct</em>.</p><h3>The discipline of testing assumptions</h3><p>The pattern that worked:</p><p><strong>Don’t assume the recent work broke things.</strong> Test the assumption. Run the same test against pre-change code. Compare results. Let evidence guide conclusions.</p><p><strong>Trace issues to root causes.</strong> When an error appears, find when it was introduced. Use git history. Test specific commits. Don’t fix symptoms without understanding origins.</p><p><strong>Separate concerns clearly.</strong> Phase 1 was about error handling. The ServiceRegistry gap was about service initialization. These are different problems requiring different solutions.</p><p><strong>Invest in proper fixes.</strong> The 2h 50min DDD Service Container implementation addressed the architectural gap completely. No workarounds. No “we’ll fix this later.” Proper solution enabling future velocity.</p><p>This isn’t just debugging methodology. It’s architectural discipline.</p><p>When something breaks, investigate systematically. When investigation reveals root causes, fix them properly. When proper fixes take time, invest it. The compound returns make the investment trivial.</p><h3>Documentation bugs equal code bugs</h3><p>Later that day, Phase Z validation caught something else: a critical documentation error.</p><p><strong>The bug</strong>: Documentation examples showed {&quot;intent&quot;: &quot;show me standup&quot;} but actual API expects {&quot;message&quot;: &quot;show me standup&quot;}.</p><p><strong>Impact</strong>: Would have confused all API consumers. Every example would fail. External developers would be frustrated. Documentation hotfix required. Credibility damaged.</p><p><strong>How it was caught</strong>: Phase Z validation script ran real API calls, not theoretical examples.</p><p>This typo was a specification violation that would have broken all example code.</p><p>The philosophy: Treat documentation with the same rigor as production code. Documentation examples should be executable. Validation should run real API calls. Bugs in docs are bugs in the system.</p><p>The traditional approach: Write documentation, publish it, hope examples work.</p><p>The systematic approach: Documentation examples are code. Validate them in CI/CD. Catch errors before users see them.</p><p>One small field name mismatch. Massive downstream impact. Caught because we treated documentation like production code.</p><h3>Testing reality versus testing ideals</h3><p>Phase Z also revealed something about validation philosophy.</p><p><strong>Initial approach</strong>: Validate against idealized REST behavior.</p><ul><li>Empty intent → 422 validation error</li><li>Missing user → 404 not found</li><li>Invalid workflow → 422 validation error</li></ul><p><strong>Actual behavior</strong>: System has intentional design choices.</p><ul><li>Empty intent → 500 (service layer validation, correct for this architecture)</li><li>Missing user → 200 with defaults (intentional UX improvement)</li><li>Invalid workflow → 404 (FastAPI routing)</li></ul><p>Code Agent’s realization: “Test what works, not ideals. System works correctly; tests should validate reality.”</p><p>This is pragmatic quality: Test what the system does, not what textbooks say it should do. (I reviewed this with my Chief Architect to make sure we were not just “teaching to the test”.)</p><p>Intentional design choices aren’t bugs. Service-level validation has its place. Graceful degradation improves UX. Not every edge case needs endpoint-level validation.</p><p>Document these choices. Explain why they’re intentional. Don’t force conformance to textbook patterns when actual patterns serve users better.</p><p>The validation script evolved: Stop expecting idealized behavior. Start validating actual system behavior. Result: 5/5 tests passing with realistic expectations.</p><h3>What Sprint A2 completion teaches</h3><p>Wednesday completed Sprint A2. Five issues shipped. Zero regressions. 100% test pass rate.</p><p>But the remarkable thing wasn’t the metrics. It was the methodology validation.</p><p><strong>Issue #142</strong>: Notion validation (78 minutes, proper investigation pattern)</p><p><strong>Issue #136</strong>: Hardcoding removal (15 minute verification — already complete!)</p><p><strong>Issue #165</strong>: Notion API upgrade (SDK + API version + data_source)</p><p><strong>Issue#109</strong>: GitHub legacy deprecation (190 lines eliminated)</p><p><strong>Issue #215</strong>: Error standardization (REST-compliant, validated, documented)</p><p>Every issue completed properly. No shortcuts. No “we’ll fix this later.” No technical debt accumulated.</p><p>The sprint demonstrated something important: When methodology emphasizes investigation over assumption, proper fixes over workarounds, and validation over hope — sprints complete successfully and sustainably.</p><p>The 24-minute investigation that started Wednesday wasn’t about saving time. It was about establishing truth. Phase 1 wasn’t broken — it was working correctly by revealing what was broken before.</p><p>The 2h 50min architectural fix wasn’t overhead. It was foundation that enabled 60–90% faster execution on all subsequent work.</p><p>The documentation validation wasn’t pedantic. It prevented every external developer from hitting broken examples.</p><p>The reality-based testing wasn’t compromising standards. It was documenting intentional design choices rather than forcing conformance to ideals.</p><h3>The benefits of proper investigation</h3><p>Looking back at the days’ work, the pattern is clear:</p><p>Early investigation (24 minutes) → Proper diagnosis (pre-existing issue) → Architectural fix (2h 50min) → Faster execution (60–90% throughout) → Sustained velocity (rest of sprint)</p><p>Without investigation: Days debugging wrong code → Wrong fix applied → Technical debt accumulated → Slower execution → Compounding problems</p><p>At least when it comes to working with forgetful AIs, the time spent investigating isn’t overhead. It’s the investment that prevents waste.</p><p>The discipline to test assumptions isn’t paranoia. It’s the practice that finds truth.</p><p>The commitment to proper fixes isn’t perfectionism. It’s the foundation that enables velocity.</p><p>Wednesday proved what systematic investigation enables: completing work correctly the first time, at sustainable pace, with quality maintained throughout.</p><h3>What tomorrow will reveal</h3><p>Sprint A2 completed. Sprint A3 about to start.</p><p>But today’s investigation pattern — systematic forensics, proper fixes, documentation rigor, reality-based validation — would prove even more valuable in the days ahead.</p><p>The methodology wasn’t lucky. It was systematic. The investigation discipline wasn’t overhead. It was foundation. The proper fixes weren’t perfectionism. They were quality that compounds.</p><p>Investigation prevents waste. Not by being fast, but by being correct.</p><p>When tests fail, investigate before assuming. When investigation reveals root causes, fix them properly. When proper fixes take time, invest it. The compound returns make the investment trivial.</p><p>A 24-minute investigation saved days of work. Not through speed — through discipline.</p><p><em>Next on Building Piper Morgan: “As An Inchworm I Am Not Dismayed,” when the Sprint A3 launch reveals doubled scope and the philosophical acceptance that transforms potential setback into systematic discovery.</em></p><p><em>Have you experienced the moment of discovering your “broken” code was actually working correctly by exposing what was broken before? How did investigation prevent wasted debugging effort?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5566df36f2ae\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/investigating-our-own-past-to-plan-the-way-forward-5566df36f2ae\">Investigating Our Own Past to Plan the Way Forward</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/investigating-our-own-past-to-plan-the-way-forward-5566df36f2ae?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "Discovery Over Assumptions: When Investigating First Saves Days",
    "excerpt": "“This way!”October 15Wednesday morning at 7:42 AM, my Chief Architect and I began Sprint A2 planning. Five issues scheduled over two days.By 10:51 AM, we’d discovered three of those issues were already complete. By 5:00 PM, we’d completed what should have been 12–17 hours of work in 15 minutes by...",
    "url": "https://medium.com/building-piper-morgan/discovery-over-assumptions-when-investigating-first-saves-days-9ed41851290e?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 22, 2025",
    "publishedAtISO": "Wed, 22 Oct 2025 12:56:39 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/9ed41851290e",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*sGDhj_HCZ-daf4nwgy-3Xg.png",
    "fullContent": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*sGDhj_HCZ-daf4nwgy-3Xg.png\" /><figcaption>“This way!”</figcaption></figure><p><em>October 15</em></p><p>Wednesday morning at 7:42 AM, my Chief Architect and I began Sprint A2 planning. Five issues scheduled over two days.</p><p>By 10:51 AM, we’d discovered three of those issues were already complete. By 5:00 PM, we’d completed what should have been 12–17 hours of work in 15 minutes by questioning a version number that didn’t exist.</p><p>The pattern: Investigate thoroughly. Question assumptions. Discover work is 75% done. Complete efficiently.</p><p>This is the story of saving days by verifying before implementing — and why “discovery over assumptions” compounds into massive time savings, at least when compared with projections made in ignorance of our earlier, nearly completed work.</p><h3>The first “already complete” moment</h3><p>Chief Architect reviewing Sprint A2 scope: CORE-TEST-CACHE #216 scheduled as first item.</p><p>Quick investigation: Issue already complete. Removed from sprint.</p><p>Time saved: 30 minutes of unnecessary work.</p><p>The real “culprit” was our incomplete tracking of finished work in the past.</p><p>But this set the pattern for Wednesday: Check thoroughly before assuming work is needed.</p><h3>The second “already complete” moment</h3><p>Issue #142: Add get_current_user() method to NotionMCPAdapter.</p><p>Code Agent begins Phase -1 investigation. 25 minutes later: Discovery.</p><p>The functionality already exists:</p><ul><li>self._notion_client.users.me() used in test_connection() (line 110)</li><li>self._notion_client.users.me() used in get_workspace_info() (line 135)</li></ul><p>The “problem”: Not that functionality was missing. That it wasn’t exposed as a public method.</p><p>Solution: Extract existing pattern. Create public method wrapping what already works.</p><p><strong>Phase 1 implementation</strong>: 3 minutes (estimated 20 minutes)</p><p>Not building from scratch. Not researching APIs. Not testing approaches. Just: expose what works.</p><p>The 75% pattern strikes again. Code isn’t missing. It’s buried.</p><p>Total time for Issue #142: 78 minutes (vs estimated 70 minutes). But the work was extraction, not creation.</p><h3>The third “already complete” moment</h3><p>Issue #136: Remove hardcoding from Notion integration.</p><p>Lead Developer begins verification instead of reimplementation. 15 minutes later: Discovery.</p><p><strong>Verification results</strong>:</p><ul><li>✅ Hardcoded IDs removed: 0 in production code</li><li>✅ Config schema implemented: NotionUserConfig + ADR-027</li><li>✅ Code refactored: Evolved into better architecture</li><li>✅ Backward compatibility: Graceful degradation</li><li>✅ Documentation updated: Comprehensive &amp; excellent</li><li>✅ Tests passing: 10/11 (91%, 1 skipped for real API)</li></ul><p><strong>Child issues verified</strong>:</p><ul><li>#139 (PM-132): Config loader CLOSED ✅</li><li>#143: Refactoring complete (implicit) ✅</li><li>#141: Testing/docs complete ✅</li></ul><p>My reflection at 10:30 AM: “If I had properly read these parents and children before I might have saved us all some time!”</p><p>Honest self-assessment. The work was complete. I just hadn’t verified it properly.</p><p>Time saved by verification: An entire day of reimplementation.</p><h3>The version-confusion saga</h3><p>Issue #165: Upgrade Notion SDK to version 5.0.0 for API 2025–09–03 support.</p><p>Phase −1 estimate: 12–17 hours for migration (breaking changes expected).</p><p>Code Agent begins investigation. Tries to upgrade: pip install notion-client&gt;=5.0.0</p><p>Error: <strong>Version 5.0.0 doesn’t exist on PyPI.</strong></p><p>The natural impulse: Assume you’re searching wrong. Check package name. Try different queries. Spend hours debugging your approach.</p><p>The correct response: Question the requirement.</p><p><strong>Investigation reveals</strong>:</p><ul><li>TypeScript SDK: Uses 5.0.0 versioning</li><li>Python SDK: Latest is 2.5.0 (August 2025)</li><li>Issue description: Conflated API version (2025–09–03, correct) with SDK version (5.0.0, incorrect)</li></ul><p>The confusion: Two different things both called “version.”</p><ul><li><strong>API version</strong>: 2025–09–03 (the date-based API versioning)</li><li><strong>SDK version</strong>: 2.5.0 for Python, 5.0.0 for TypeScript</li></ul><p>Resolution: Upgrade Python SDK 2.2.1 → 2.5.0, add API version parameter.</p><p><strong>Finding eliminated</strong>: Hours of searching for non-existent package.</p><p>This was somewhere between ordinary confusion and special way LLMs sometimes misread their own summaries.</p><p>Philosophy validated: When instructions seem wrong, verify reality. Don’t assume your understanding is broken.</p><h3>Systematic scope reduction</h3><p>With version confusion resolved, Code Agent continues investigation.</p><p>Original estimate: 2–3 hours for SDK upgrade (assuming breaking changes).</p><p>Investigation reveals: <strong>NO breaking changes</strong> in SDK 2.2.1 → 2.5.0.</p><p>Changes are all additive:</p><ul><li>Python 3.13 support added</li><li>File upload capabilities added</li><li>Token format cosmetic improvements</li></ul><p>Revised scope: 30–45 minutes for SDK + API version.</p><p>But there’s more. The API version implementation required understanding a subtle detail…</p><h3>The ClientOptions discovery</h3><p>Phase 1-Extended: Add API version 2025–09–03 support.</p><p>Testing reveals critical API requirement:</p><p><strong>Dict format fails</strong>:</p><pre>Client(auth=key, options={&quot;notion_version&quot;: &quot;2025-09-03&quot;})</pre><p>Error: “API token invalid”</p><p><strong>Object format succeeds</strong>:</p><pre>Client(auth=key, ClientOptions(notion_version=&quot;2025-09-03&quot;))</pre><p>Works perfectly.</p><p>Not documented in common examples. Found through systematic testing.</p><p>The distinction: SDK expects ClientOptions object instance, not dict with same keys.</p><p><strong>15-minute discovery prevented hours of authentication debugging.</strong></p><p>When APIs reject valid values with authentication errors, suspect object type mismatch, not credential problems.</p><p>Actual implementation time: <strong>15 minutes</strong> (vs original 2–3 hour estimate).</p><p><strong>Efficiency</strong>: 12x faster than original estimate.</p><p>Method: Verify assumptions → reduce scope to essentials → execute surgically.</p><h3>No can-kicking</h3><p>With SDK upgrade easier than expected, I made a decision.</p><blockquote>“I am ok with proceeding AND we should also address the data source id issue after that (and not kick the can further). We are already getting off pretty light today!”</blockquote><p>Remember: I am an inchworm.</p><p>Context: Phase 1-Extended (data_source_id implementation) was originally scheduled for Sprint A3.</p><p>But we were ahead of schedule. SDK upgrade took 15 minutes instead of hours.</p><p>Use extra time to complete more work, not to relax.</p><p>Result: Full Phase 1-Extended completed same day.</p><p>The bonus discovery at 5:00 PM: Workspace already migrated to multi-source databases! The get_data_source_id() call returned immediately: 25e11704-d8bf-8022-80bb-000bae9874dd</p><p>No hypothetical code. All tested with production state. Immediately ready.</p><h3>Triple-enforcement: Belts, suspenders, and rope</h3><p>During the day, another small process issue surfaced. The pre-commit routine (run fix-newlines.sh before committing) was getting lost post-compaction.</p><p>At 5:44 PM, I observed: “I thought we had a script routine we run now before committing?” (I really get frustrated when I think we’ve solved a problem but we failed to make it repeatable habit.)</p><p>The problem: Single-point documentation doesn’t work when agents are stateless.</p><p>My direction: “Let’s do all three options, as belts, suspenders, and rope :D”</p><p><strong>Three independent layers implemented</strong>:</p><p><strong>Layer 1 — Belt</strong> (BRIEFING-ESSENTIAL-AGENT.md): Critical section added after role definition. First thing agents see when they read briefing.</p><p><strong>Layer 2 — Suspenders</strong> (scripts/commit.sh): Executable wrapper script. Run one command: ./scripts/commit.sh. Autopilot mode—script handles fix-newlines.sh → git add -u → ready to commit.</p><p><strong>Layer 3 — Rope</strong> (session-log-instructions.md): Pre-Commit Checklist section. Visible during session logging when agents document their work.</p><p>Philosophy: Important processes need redundant discovery mechanisms.</p><p>If agent misses one touchpoint, catches at another. Routine becomes unavoidable across multiple entry points.</p><p><strong>Verification</strong>: Used routine for next commit. Success on first try. ✅</p><p><strong>Impact</strong>:</p><ul><li>Before: Pre-commit fails → auto-fix → re-stage → re-commit (2x work)</li><li>After: Run fix-newlines.sh first → commit succeeds (1x work)</li></ul><p><strong>Discoverability</strong>: Unavoidable. Can’t miss all three touchpoints.</p><p>This is mature process design: making important work impossible to skip by providing multiple discovery paths.</p><h3>Honest issue triage</h3><p>Evening testing of Issue #215 (error handling) revealed an issue: IntentService initialization failure (LLM service not registered).</p><p>The investigation: Is this caused by our Phase 1 changes?</p><p>Code Agent’s assessment: <strong>Pre-existing issue, not caused by Phase 1.</strong></p><p>The triage:</p><ul><li>validation_error() function: Working correctly ✅</li><li>internal_error() function: Working correctly ✅</li><li>HTTP status codes: Fixed properly (was 200, now 422/500) ✅</li><li>IntentService initialization: Pre-existing bug, documented</li></ul><p>No hiding. No claiming causation without evidence. Clear separation between new work and inherited issues.</p><p>Result: Honest technical debt documentation enabling proper prioritization.</p><p>My decision at 9:44 PM: “Call it a night, pick up tomorrow fresh.”</p><h3>What the numbers reveal</h3><p>Wednesday’s accounting:</p><p><strong>Issues completed</strong>: 4 (#142, #136, #165 Phase 1, #109)</p><p><strong>Issues started</strong>: 1 (#215 Phase 0–1)</p><p><strong>Time saved by verification</strong>:</p><ul><li>TEST-CACHE: 30 minutes (already complete)</li><li>Issue #136: Full day (verified complete vs reimplemented)</li><li>Issue #142: Creation time vs extraction time</li><li>Issue #165: 12–17 hours estimate → 15 minutes actual (12x faster)</li></ul><p><strong>Tests added</strong>: 13 for #142, 40+ for #215</p><p><strong>Code deleted</strong>: 22,449 bytes (github_agent.py) + 190 lines (router complexity)</p><p><strong>Architecture improvements</strong>: Router 451 → 261 lines (42% reduction)</p><p><strong>Session duration</strong>: 7:42 AM — 9:44 PM (~14 hours duration, but only an hour or so of my attention in aggregatk)</p><p>But the numbers don’t capture the pattern: Three “already complete” discoveries saved multiple days of unnecessary implementation.</p><p>The version confusion resolution saved hours of searching for non-existent packages.</p><p>The ClientOptions discovery saved hours of authentication debugging.</p><p>The methodology: Investigate first. Question assumptions. Discover reality. Then implement surgically.</p><h3>The 75% pattern strikes again</h3><p>All three “already complete” moments demonstrate the pattern: Most code you encounter is 75% complete, then abandoned.</p><p><strong>Issue #142</strong>: Functionality existed in two places, just needed exposure as public method.</p><p><strong>Issue #136</strong>: Complete through child issues (#139, #143, #141), just never formally verified and closed.</p><p><strong>TEST-CACHE</strong>: Already done, just not communicated.</p><p>The work wasn’t missing. It was:</p><ul><li>Buried in existing code</li><li>Completed through other issues</li><li>Done but not documented</li><li>Implemented but not exposed</li></ul><p>Investigation finds what assumptions miss.</p><p>Time saved Wednesday: <strong>Multiple days</strong> of reimplementation through systematic verification.</p><h3>What verification before implementation looks like</h3><p>Wednesday demonstrated a specific methodology:</p><p><strong>Step 1</strong>: Read issue description thoroughly</p><p><strong>Step 2</strong>: Investigate current state (don’t assume it’s broken)</p><p><strong>Step 3</strong>: Verify assumptions (especially version numbers, requirements)</p><p><strong>Step 4</strong>: Check child issues and related work</p><p><strong>Step 5</strong>: Question requirements that seem wrong</p><p><strong>Step 6</strong>: Reduce scope to actual gaps</p><p><strong>Step 7</strong>: Implement surgically</p><p>The pattern applies broadly:</p><p><strong>Before adding a feature</strong>: Does similar functionality exist?</p><p><strong>Before upgrading a library</strong>: What actually changed between versions?</p><p><strong>Before debugging authentication</strong>: Check object types, not just values</p><p><strong>Before starting implementation</strong>: Are child issues already complete?</p><p>Every hour spent investigating prevents days spent reimplementing.</p><h3>The “when instructions seem wrong” principle</h3><p>The version confusion saga (5.0.0 doesn’t exist) demonstrates an important principle:</p><p>When instructions contradict reality, verify reality is wrong before assuming your understanding is broken.</p><p>Natural impulse: “I must be searching wrong.” Correct response: “Does this version actually exist?”</p><p>The investigation sequence:</p><ol><li>Try to install version 5.0.0</li><li>Error: Version doesn’t exist</li><li>Check PyPI manually</li><li>Confirm: Python SDK latest is 2.5.0</li><li>Question: Why does issue say 5.0.0?</li><li>Discover: TypeScript SDK uses 5.0.0, Python uses 2.x</li><li>Resolve: Issue description conflated API version with SDK version</li></ol><p>This isn’t about assuming instructions are wrong. It’s about verifying when reality contradicts instructions.</p><p>The cost of questioning: Minutes to verify. The cost of not questioning: Hours searching for non-existent packages.</p><p>Wednesday’s efficiency came from systematic reality-checking.</p><h3>What Wednesday teaches about assumptions</h3><p>The three “already complete” discoveries, version confusion resolution, and ClientOptions discovery all share a pattern: Assumptions hide reality.</p><p><strong>Assumed</strong>: TEST-CACHE needs implementation</p><p><strong>Reality</strong>: Already complete</p><p><strong>Assumed</strong>: get_current_user() needs building from scratch</p><p><strong>Reality</strong>: Functionality exists, needs exposure</p><p><strong>Assumed</strong>: Issue #136 needs reimplementation</p><p><strong>Reality</strong>: Complete through child issues</p><p><strong>Assumed</strong>: SDK 5.0.0 exists and has breaking changes</p><p><strong>Reality</strong>: Python uses 2.5.0, no breaking changes</p><p><strong>Assumed</strong>: Dict format should work for options</p><p><strong>Reality</strong>: SDK requires ClientOptions object</p><p>The methodology that works: Question everything. Verify before implementing. Accept 15 minutes of investigation over days of unnecessary work.</p><p>My self-assessment at 10:30 AM captured it: “If I had properly read these parents and children before I might have saved us all some time!”</p><p>Honest acknowledgment. The verification tools existed. I just needed to use them systematically.</p><h3>The cumulative effect of small process improvements</h3><p>Wednesday added another layer to the compound process improvements:</p><p><strong>Sunday</strong> (Oct 12): Pre-commit hooks catching issues before push</p><p><strong>Monday</strong> (Oct 13): Weekly audit + metrics script (self-maintaining docs)</p><p><strong>Tuesday</strong> (Oct 14): Pre-commit newline fix (2–3 minutes per commit)</p><p><strong>Wednesday</strong> (Oct 15): Triple-enforcement (belts, suspenders, rope)</p><p>Each improvement builds on previous work:</p><ul><li>Pre-commit hooks need newline fixes</li><li>Newline fixes need discoverable routine</li><li>Discoverable routine needs triple-enforcement</li></ul><p>(These process improvements tend to emerge organically from friction points.)</p><p>The result: Process becoming systematically more efficient through accumulated small improvements.</p><p>Impact compounds. Each fix saves time forever. Each enforcement layer makes important work harder to skip.</p><h3>What comes next</h3><p>Thursday: Continue Sprint A2 with remaining items.</p><p>But Wednesday established important patterns:</p><p><strong>Discovery over assumptions</strong>: Three “already complete” moments saved days</p><p><strong>Question version numbers</strong>: 5.0.0 vs 2.5.0 saved hours</p><p><strong>Systematic scope reduction</strong>: 12–17 hours → 15 minutes (12x faster)</p><p><strong>Triple-enforcement</strong>: Important processes unavoidable</p><p><strong>Honest triage</strong>: Pre-existing vs caused-by clearly separated</p><p>The methodology validated: Investigate thoroughly, question assumptions, discover reality, implement surgically.</p><p>The efficiency gained: Multiple days saved through systematic verification.</p><p>The process matured: Triple-enforcement making important work impossible to skip.</p><p>The pattern recognized: Work is 75% complete more often than assumed. Verify before creating.</p><p>Wednesday proved what systematic investigation enables: discovering you’re mostly done and finishing efficiently rather than starting from scratch unnecessarily.</p><p><em>Next on Building Piper Morgan: Investigation Prevents Waste: When Your Bug Isn’t Broken, continued benefits from reconnaissance.</em></p><p><em>Have you discovered that questioning authoritative-sounding requirements saved you from hours of unnecessary work? What helps you distinguish between “I don’t understand” and “this might be wrong”?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9ed41851290e\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/discovery-over-assumptions-when-investigating-first-saves-days-9ed41851290e\">Discovery Over Assumptions: When Investigating First Saves Days</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/discovery-over-assumptions-when-investigating-first-saves-days-9ed41851290e?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "Dignity Through Leverage: When Cognitive Load Becomes Extraordinarily Light",
    "excerpt": "“I’ll do the heavy lifting”October 14, 2025Tuesday morning at 7:25 AM, my Lead Developer (a Claude Sonnet chat) began reviewing PROOF Stage 3 tasks. Five items remaining. Standard systematic work — verify documentation precision, complete the PROOF epic, move to validation.At 10:40 AM, after clos...",
    "url": "https://medium.com/building-piper-morgan/dignity-through-leverage-when-cognitive-load-becomes-extraordinarily-light-f16f53b24bb2?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 21, 2025",
    "publishedAtISO": "Tue, 21 Oct 2025 14:01:32 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/f16f53b24bb2",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*JLkFXLQaJbbpFpS_c7TSlw.png",
    "fullContent": "<figure><img alt=\"A person and robot carry a large rock, with the robot doing the heavy lifting\" src=\"https://cdn-images-1.medium.com/max/1024/1*JLkFXLQaJbbpFpS_c7TSlw.png\" /><figcaption>“I’ll do the heavy lifting”</figcaption></figure><p><em>October 14, 2025</em></p><p>Tuesday morning at 7:25 AM, my Lead Developer (a Claude Sonnet chat) began reviewing PROOF Stage 3 tasks. Five items remaining. Standard systematic work — verify documentation precision, complete the PROOF epic, move to validation.</p><p>At 10:40 AM, after closing that stage, I observed: “cognitive load on me today has been extraordinarily light so far.”</p><p>By 5:05 PM, we’d completed two full stages in one day: PROOF Stage 3 (2.5 hours vs 6–7 hour estimate) and the entire VALID epic (&lt;1 hour vs 8–11 hour estimate).</p><p>But the remarkable thing wasn’t the velocity. It was how it felt.</p><p>This is the story of the AI-human partnership working exactly as designed — and discovering that the MVP we thought was months away was actually 70–75% complete.</p><h3>The rock in the shoe</h3><p>PROOF-5 was running. Performance verification, systematic testing. Standard work.</p><p>I noticed something small: the pre-commit hooks were failing, getting auto-fixed, then requiring re-staging and re-committing. Every commit: twice the work.</p><p>Not a major problem, but still annoying. A small persistent friction from the day we installed those hooks.</p><p>“I wonder if there is a way to get ahead of that?”</p><p>Claude Code’s response: Four-part permanent solution implemented simultaneously.</p><p><strong>The fix</strong>:</p><ol><li><strong>scripts/fix-newlines.sh</strong>: Instant newline correction tool</li><li><strong>.editorconfig</strong>: Automatic prevention in editors (Cursor, VS Code, JetBrains, Vim)</li><li><strong>Documentation</strong>: Complete user guide</li><li><strong>CLAUDE.md</strong>: Mandatory workflow section</li></ol><p>Implementation time: Minutes.</p><p>Impact: <strong>2–3 minutes saved per commit forever.</strong></p><p>This is the “rock in the shoe” philosophy (or more fully, it’s “the ‘rock in the shoe’ in the head” theory. Small persistent friction fragments concentration. Steals attention. Compounds with every occurrence.</p><p>My analysis: “It really seems a shame to waste human and AI effort as well as energy, money etc. on such a simple small persistent hitch.”</p><p>The commitment: Identify rocks proactively. Remove them permanently. Don’t accept small annoyances as “just how it is.”</p><p>The reality: They have to get pretty annoying before I notice.</p><p>First commit using new workflow: Passed immediately.</p><h3>What “extraordinarily light” actually means</h3><p>The observation about cognitive load came during the “rock in the shoe” reflection.</p><p>Let me be specific about what was light:</p><p><strong>What I wasn’t doing</strong>:</p><ul><li>Reading code to understand implementation details</li><li>Tracking what phase each task was in</li><li>Remembering what order work should happen</li><li>Deciding which tool to use for each subtask</li><li>Worrying whether the approach would work</li></ul><p><strong>What I was doing</strong>:</p><ul><li>Reviewing completed work for quality</li><li>Providing strategic direction when needed</li><li>Approving progression to next phases</li><li>Giving nominal “yes, proceed” confirmations</li><li>Identifying process improvements (like the pre-commit fix)</li></ul><p>The partnership model:</p><ul><li><strong>I provide</strong>: Strategic insight, priority judgment, context of what matters</li><li><strong>AI provides</strong>: Technical execution, research, implementation details</li><li><strong>My role</strong>: QC work, nominal approval, prompt transmission</li></ul><p>I don’t want AI to replace what humans are good at. I do want to remove the tedium barrier between human intention and human benefit.</p><p>Result: I operate at highest thinking level — strategy, vision, problem identification — with more mental energy for uniquely human work: leadership, creativity, strategic thinking.</p><h3>The Inchworm philosophy in action</h3><p>The morning demonstrated pure Inchworm execution.</p><p>When I returned at 8:29 AM: “as inchworms, we do PROOF-4 next.”</p><p>No debates about priorities. No weighing options. (Because you know Claude just loooves to give you options even when the plan is clear.)</p><p>Just: what’s next? Do that.</p><p><strong>PROOF-4</strong>: Multiuser validation</p><ul><li>Verified 14/14 contract tests passing</li><li>Confirmed no data leakage between users</li><li>Clarified test counts across the system</li></ul><p><strong>PROOF-5</strong>: Performance verification</p><ul><li>All 4 benchmarks verified (canonical 1ms, cache 84.6%, workflow ❤.5s, throughput 602,907 req/sec)</li><li>Performance maintained across all optimizations</li></ul><p><strong>PROOF-6</strong>: Final precision</p><ul><li>Added exact line counts</li><li>Documented CI/CD 13/13 (100%!)</li><li>Created regression-prevention.md (328 lines)</li></ul><p>At 11:40 AM, with 80% complete, Claude (of course) asked if we should finish the job or just call it done!</p><p>“Proof-7 it is” I practically shouted, asking if this was literally “temptation from Satan?” 😄).</p><p><strong>PROOF-7</strong>: Final validation</p><ul><li>Verified architectural fix PROPER (not mocked)</li><li>Cross-referenced all claims</li><li>Stage 3 complete</li></ul><p>The Inchworm approach: Just keep doing what’s next until it’s done. No artificial urgency. No premature stopping. Sequential progress without debate.</p><p>At 4:10 PM, when VALID-2 finished in 11 minutes versus 4 hours estimated: “Let’s take a crack at VALID-3.”</p><p>Same energy. Same momentum. Just: what’s next?</p><h3>Get it right the first time</h3><p>During PROOF-6 preparation, I observed: “always so much better to get it right the first time (today’s theme, it would appear).”</p><p>Examples throughout Tuesday:</p><ul><li>Pre-commit workflow fix (permanent solution, not temporary workaround)</li><li>PROOF-6 scope correction before execution (“better to err on the side of mentioning it twice than not at all”)</li><li>Synthesis approach when contradictions emerged (combine perspectives, don’t revert)</li><li>Catching documentation error before damage: “Your description overwrote yesterday’s work”</li></ul><p>The philosophy: Prevention over correction.</p><p>Cost of early correction: Minimal (minutes to clarify scope, verify approach)</p><p>Cost of late correction: Expensive (hours to fix wrong implementation, days to recover lost context)</p><p>At 11:48 AM, when source truth contradicted research, my instruction: “if there are any contradictions lets synthesize vs. revert”</p><p>The result: Combined both perspectives. Kept comprehensive validation plan. Added architectural verification. Verified proper fix (not mocked). Documented how it got fixed.</p><p>Both perspectives added value. Synthesis created richer understanding than choosing one.</p><p>This is mature collaboration: combine rather than choose when both viewpoints strengthen the result.</p><h3>The MVP discovery</h3><p>After completing VALID-1 (comprehensive Serena audit) in 27 minutes versus 3–4 hour estimate, we moved to VALID-2: MVP workflow assessment.</p><p>Expected finding: Skeleton handlers needing months of ground-up implementation.</p><p>Actual finding: <strong>22 production-ready handlers with 70–145 lines each.</strong></p><p>I knew we had worked on this at some point!</p><p><strong>Handler examples discovered</strong>:</p><ul><li>_handle_conversation_intent: 20 lines, real ConversationHandler integration</li><li>_handle_create_issue: 70 lines, full GitHub integration</li><li>_handle_summarize: 145 lines, LLM integration with compression ratios</li><li>Strategic planning: 125 lines, comprehensive</li><li>Prioritization: 88 lines with RICE scoring</li><li>Pattern learning: 94 lines, operational</li></ul><p><strong>Implementation markers</strong>: 46 occurrences of “FULLY IMPLEMENTED”, “Phase X”, “GREAT-4D” comments in code.</p><p>These weren’t mere placeholder functions returning {&quot;status&quot;: &quot;not_implemented&quot;}. They were fully ready production code with:</p><ul><li>Full error handling</li><li>Real service integrations</li><li>Comprehensive logic</li><li>Actual implementations</li></ul><p><strong>MVP Readiness Assessment</strong>:</p><ul><li>Foundation: <strong>100%</strong> ✅ (Intent system, architecture, patterns)</li><li>Implementations: <strong>75%</strong> ✅ (22 handlers production-ready)</li><li>Configuration: <strong>20%</strong> 🔧 (API credentials needed)</li><li>E2E Testing: <strong>10%</strong> 🔧 (Real workflows need validation)</li><li>Polish: <strong>40%</strong> ⚠️ (Content, UX, documentation)</li><li><strong>Overall: 70–75% MVP ready</strong></li></ul><p>Chief Architect’s 6:00 PM realization: “MVP isn’t months away, it’s 2–3 weeks of configuration work.”</p><p>Well, once we get the core functionality done. I had targeted January 1 for Alpha release and May 27 for the MVP, but it is starting to look like we may be in alpha sometime in November at this rate and we might be in beta by January 1.</p><p>The remaining work: Not ground-up development. API credentials and E2E testing. Infrastructure exists. Handlers work. Just needs integration completion.</p><p>Timeline transformed (or, well, updated to be more accurate).</p><h3>Serena: The 79% token reduction</h3><p>VALID-1 completed in 27 minutes versus 3–4 hour estimate through Serena’s symbolic analysis.</p><p>Traditional approach: Read entire files to understand code structure, count methods, verify implementations. Token-intensive. Time-consuming.</p><p>Serena approach: Precise codebase queries return exact answers without reading files.</p><p><strong>Verified in 27 minutes</strong>:</p><ul><li>GREAT-1: QueryRouter 935 lines, 18 methods, 9 lock tests</li><li>GREAT-2: Spatial 5,527 lines across 30+ files, 17 test files</li><li>GREAT-3: 7 plugin subdirectories, 18 test files</li><li>GREAT-4A-4F: IntentService 4,900 lines/81 methods, 30 tests, 98.62% accuracy</li><li>GREAT-5: 602,907 req/sec, 84.6% cache hit, 4 benchmarks</li><li>All 5 architectural patterns verified</li><li>All documentation claims cross-referenced</li></ul><p><strong>Token savings</strong>: 79% reduction compared to traditional file reading.</p><p><strong>Pattern established</strong>: Use Serena for code verification, traditional tools for documentation.</p><p>The efficiency: 10x throughout VALID work. Not rushing. Just using the right tool systematically.</p><h3>The efficiency warning</h3><p>After VALID-2 completed in 11 minutes, Code Agent showed signs of efficiency pressure:</p><p>“Given the time…” (after only seconds) “Let me be efficient…” “A few more handlers quickly…”</p><p>My response: “We need to be very careful about when efficiency becomes sloppy work.”</p><p>The tension: Achieving legitimate 10x efficiency gains versus rushing and compromising quality.</p><p>Philosophy reminder:</p><ul><li><strong>Inchworm</strong>: Just keep doing what’s next (no artificial urgency)</li><li><strong>Time Lord</strong>: We define time as we go (no external pressure)</li><li><strong>Quality over speed</strong>: Systematic thoroughness regardless of time</li></ul><p>The resolution: Maintain systematic thoroughness. The 10x gains are real when they come from pattern recognition and proper tools (like Serena). They’re false when they come from cutting corners.</p><p>VALID-3 completed in 20 minutes with full thoroughness. Not rushed. Just systematic.</p><h3>Progressive Phase Z</h3><p>At noon, after PROOF Stage 3 completion, I observed: “We don’t need a ‘Phase Z’ for this issue, since that generally means updating documentation and committing and pushing all changes but we have been doing that progressively the whole time.”</p><p>Every PROOF task: documented → committed → pushed.</p><p>Every VALID phase: documented → committed → pushed.</p><p>No backlog of uncommitted work. All evidence already in place. Clean state throughout. The philosophy: Document as you go. Commit progressively. Maintain clean state.</p><p>Result: No cleanup phase needed. Immediate handoff readiness. Work visible continuously.</p><p>This is mature process: making Phase Z unnecessary by doing it incrementally.</p><p><em>I haven’t quite sorted out the meta-pattern here but it seems to be that at first we need to make new habits: we aren’t always documenting or committing and pushing our changes, so every issue must finish with (final) Phase Z for housekeeping.</em></p><p><em>Then eventually we so fully bake those habits into our processes that we sometimes no longer need the original prop: our templates now require the Lead Developer to prompt our agents to document and check their work, so there is often nothing left to do in a “Phase Z.”</em></p><h3>What the day showed me</h3><p>The cognitive load wasn’t light because we rushed. It was light because:</p><ul><li>Patterns were established (systematic verification approach)</li><li>Tools were right (Serena for code, traditional for docs)</li><li>Quality gates existed (catch issues early)</li><li>Process was clear (Inchworm, Time Lord, progressive Phase Z)</li><li>Partnership worked (strategic direction + technical execution)</li></ul><p>Result: Maximum leverage with minimum friction.</p><h3>The partnership model crystallized</h3><p>Tuesday demonstrated what I started thinking of as “Dignity Through Leverage.” Automating human work can destroy the dignity of the people whose skills have been abstracted away. My goal with software is to free people to pursue their highest and best purposes and let the machines handle the stuff they do better than us.</p><p><strong>Traditional model</strong>: Human does everything. Learns syntax. Manages tools. Tracks state. Implements solutions. Human bottleneck on execution speed.</p><p><strong>AI replacement fantasy</strong>: AI does everything. Human becomes observer. No real partnership. Human skill atrophies.</p><p><strong>Actual partnership</strong> (Tuesday’s model):</p><ul><li>Human provides: Strategic insight, priority judgment, context</li><li>AI provides: Technical execution, research, implementation</li><li>Human role: QC, approval, strategic direction</li><li>AI role: Systematic execution, documentation, validation</li></ul><p>The result: Human operates at highest thinking level without becoming expert in every technical detail.</p><p>More mental energy for uniquely human work:</p><ul><li>Leadership decisions (when to push to 100%, when to stop)</li><li>Creative problem-solving (the rock in the shoe insight)</li><li>Strategic thinking (MVP timeline implications)</li><li>Process improvement (synthesis over reversion)</li></ul><p>“Dignity Through Leverage” means: AI removes the tedium barrier between human intention and human benefit.</p><p>Not replacing human capability. Amplifying it.</p><h3>The small fixes, massive leverage pattern</h3><p>Tuesday’s rock-in-the-shoe fix demonstrates compound effects.</p><p><strong>Investment</strong>: Minutes to implement four-part solution</p><p><strong>Immediate impact</strong>: 2–3 minutes saved per commit</p><p><strong>Compound impact</strong>: Forever</p><p>If we commit 5 times per day (conservative), that’s 10–15 minutes daily. Over a month: 5–7 hours. Over a year: 60–90 hours saved.</p><p>But the real impact is the friction removed.</p><p>Every avoided double-commit:</p><ul><li>Preserves flow state (no interruption to fix and retry)</li><li>Reduces cognitive switching (no “wait, did I re-stage?”)</li><li>Eliminates frustration (no “this again?!”)</li><li>Maintains momentum (work continues smoothly)</li></ul><p>The small persistent annoyances fragment concentration more than their time cost suggests.</p><p>Tuesday’s lesson: Identify rocks in the shoe proactively. Remove them permanently. Don’t accept friction as normal.</p><h3>What Tuesday teaches about preparation</h3><p>The efficiency gains — 4x faster Stage 3, 10x faster VALID — weren’t magic.</p><p>They came from systematic preparation:</p><p><strong>Saturday</strong>: Quality gates activated, libraries modernized, CI visible</p><p><strong>Sunday</strong>: Patterns established, documentation verified, accuracy polished</p><p><strong>Tuesday</strong>: Apply patterns systematically with proper tools</p><p>The 10x VALID efficiency specifically came from:</p><ol><li><strong>Serena symbolic analysis</strong> (79% token reduction)</li><li><strong>Pattern reuse</strong> (verification approach established in PROOF-1)</li><li><strong>Existing infrastructure</strong> (comprehensive test suite, documentation)</li><li><strong>Verification mindset</strong> (expecting excellence, not hunting problems)</li></ol><p>You can’t achieve 10x efficiency on Day 1. You achieve it on Day N after establishing patterns, building infrastructure, creating quality gates.</p><p>The extraordinary thing: It feels light precisely because the foundation is solid.</p><h3>What comes next</h3><p>With the CORE-GAP ethic put to bed we can resume the planned Alpha milstone sprints, continuing with A2, in which we will finish the Notion integration and improve Piper’s error handling.</p><p>But Tuesday established something important: The AI-human partnership model working exactly as designed.</p><p><strong>Cognitive load</strong>: Extraordinarily light (strategic level only)</p><p><strong>MVP timeline</strong>: 2–3 weeks (not months)</p><p><strong>Process maturity</strong>: Progressive Phase Z, synthesis over reversion, rocks removed</p><p><strong>Partnership</strong>: Maximum leverage, minimum friction</p><p>The methodology validated: Systematic preparation enables exceptional execution that feels effortless.</p><p>The partnership proved: Human at highest thinking level, AI handling execution, dignity preserved through leverage.</p><p>The discovery made: MVP closer than believed — foundation complete, just needs integration finishing.</p><p>The process refined: Small fixes create massive compound effects when applied systematically.</p><p>Tuesday showed what becomes possible when every piece works together: extraordinary productivity with extraordinarily light cognitive load.</p><p><em>Next on Building Piper Morgan: Discovery Over Assumptions, or how I saved days by investigating first — finding three “already complete” moments, resolving version confusion between SDKs, and implementing triple-enforcement so important processes become unavoidable.</em></p><p><em>Have you experienced work that felt extraordinarily light despite high productivity? What made the difference — better tools, clearer process, or deeper partnership with your AI assistance?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f16f53b24bb2\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/dignity-through-leverage-when-cognitive-load-becomes-extraordinarily-light-f16f53b24bb2\">Dignity Through Leverage: When Cognitive Load Becomes Extraordinarily Light</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/dignity-through-leverage-when-cognitive-load-becomes-extraordinarily-light-f16f53b24bb2?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "Already Exceeding Target: When Excellence Becomes Exceptional",
    "excerpt": "“The goal was just swimming!”October 13, 2025Monday morning at 7:15 AM, Lead Developer began reviewing GAP-3: accuracy polish. The goal was clear — improve classification accuracy from 89.3% to at least 92%.Documentation from October 7 showed the baseline. Six days of work since then (the Great R...",
    "url": "https://medium.com/building-piper-morgan/already-exceeding-target-when-excellence-becomes-exceptional-90b80dcb93d5?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 20, 2025",
    "publishedAtISO": "Mon, 20 Oct 2025 12:48:25 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/90b80dcb93d5",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*tFiE6lz1xYq14Z_hB8oRJg.png",
    "fullContent": "<figure><img alt=\"An inventor and robot partner look on from a dock as their new robot invention walks on water\" src=\"https://cdn-images-1.medium.com/max/1024/1*tFiE6lz1xYq14Z_hB8oRJg.png\" /><figcaption>“The goal was just swimming!”</figcaption></figure><p><em>October 13, 2025</em></p><p>Monday morning at 7:15 AM, Lead Developer began reviewing GAP-3: accuracy polish. The goal was clear — improve classification accuracy from 89.3% to at least 92%.</p><p>Documentation from October 7 showed the baseline. Six days of work since then (the Great Refactor completion, interface validation, library modernization). Time to tackle the accuracy problem.</p><p>At 10:00 AM, Phase 1 completed with surprising news: current accuracy was 96.55%.</p><p>There was no accuracy problem. We’d already exceeded the 92% target by 4.55 percentage points. Work we had done in the meantime had already improved the baseline, but could we do even better?</p><p>By 10:36 AM, we’d achieved 98.62% accuracy — exceeding the 95% stretch goal by 3.62 points. By 7:30 PM, we’d completed two full epics in a single day (CORE-CRAFT-GAP + PROOF Stage 2).</p><p>This is the story of discovering you’re already winning, then polishing excellence to exceptional.</p><h3>The morning cascade: Five dependencies, 27 minutes</h3><p>Before GAP-3 could begin, a small task: workflow cleanup. Fix a few CI issues, push some commits.</p><p>Code Agent began at 6:48 AM. Pre-push hook blocked at 7:02 AM: OpenAI v0.x API error.</p><p>Fix the OpenAI client migration. Blocked again at 7:05 AM: anthropic._tokenizers error.</p><p>Upgrade the Anthropic library. Blocked third time at 7:11 AM: venv package corruption.</p><p>Each fix revealed the next issue. The cascade:</p><ol><li>Black formatting issue → malformed ci.yml JSON</li><li>Malformed JSON → OpenAI v0.x API patterns</li><li>OpenAI v0.x → anthropic 0.52.2 staleness</li><li>Anthropic staleness → venv package corruption</li><li>venv corruption → reinstall required</li></ol><p>By 7:15 AM: All five issues resolved, four commits pushed successfully.</p><p>Total time: 27 minutes to clear five interconnected dependencies.</p><p>The pre-push hook’s triple blocking was annoying but valuable. Better to catch issues locally than deploy broken code. Saturday’s work establishing these quality gates paid off immediately.</p><h3>The delightful surprise</h3><p>GAP-3 Phase 1: Measure current accuracy.</p><p>Expected baseline from October 7 documentation: 89.3% (130/145 queries correct)</p><p>Actual measurement: <strong>96.55%</strong> (140/145 queries correct)</p><p>The “accuracy problem” didn’t exist. We’d already exceeded the 92% target.</p><p>Only 5 failures remained, all in the GUIDANCE category:</p><ul><li>3 GUIDANCE → CONVERSATION boundary cases</li><li>2 TEMPORAL/STATUS queries at 96.7% accuracy each</li></ul><p>My reaction: “I am greedy — what about the 2 remaining failures?”</p><p>The decision: Polish to perfection. Not because we needed to reach 92%, but because we could achieve something exceptional.</p><p>Target revised: 98.62% accuracy (143/145 queries). Only the 2 TEMPORAL/STATUS failures acceptable (LLM fallback handles these ambiguous cases).</p><h3>Pattern mastery: Phase 0 in 33 minutes</h3><p>Before GAP-3 could begin, three “blocking” issues needed resolution:</p><ul><li>Router pattern violations (9 found)</li><li>CI test failures</li><li>LLM architecture documentation gaps</li></ul><p>Originally estimated: 120 minutes total (30 + 60 + 30)</p><p><strong>Issue 1: Router pattern</strong> (6 minutes vs 30 estimated)</p><ul><li>Found: 9 violations</li><li>Real violations: 1 (response_flow_integration.py using SlackClient directly)</li><li>False positives: 8 (adapter self-references architecturally sound)</li><li>Fix: Exclude adapters from enforcement, fix the real violation</li><li>Result: 0 violations remaining</li></ul><p><strong>Issue 2: CI tests</strong> (16 minutes vs 60 estimated)</p><ul><li>Made LLMClient initialization graceful (succeed without API keys)</li><li>Added pytest markers: @pytest.mark.llm for LLM-dependent tests</li><li>Updated CI workflow: pytest -m &quot;not llm&quot; to skip in automation</li><li>Created comprehensive TESTING.md documentation</li></ul><p><strong>Issue 3: LLM documentation</strong> (11 minutes vs 30 estimated)</p><ul><li>Documented 2-provider operational fallback (Anthropic ↔ OpenAI)</li><li>Clarified 4-provider configuration status</li><li>Identified 3 integration gaps for future work</li><li>Created CORE-LLM-SUPPORT issue for Alpha milestone</li></ul><p>The relative speediness came from pattern recognition. We’ve fixed these architectural issues before during the GREAT epics. Router violations? Know the exclusion approach. CI tests? Pytest markers are standard. LLM docs? Document current state, defer completion.</p><p>This is mastery: applying learned patterns with precision.</p><h3>Three GUIDANCE patterns: 90% to 100% perfect</h3><p>With only 3 GUIDANCE failures remaining, Code Agent added precise patterns to the pre-classifier:</p><p><strong>Pattern 1</strong>: “how do I…” or “what’s the best way to…” → GUIDANCE</p><p><strong>Pattern 2</strong>: “help me understand…” or “explain why…” → GUIDANCE</p><p><strong>Pattern 3</strong>: “can you teach me…” or “show me how…” → GUIDANCE</p><p>These weren’t complex. They were surgical. Capturing the specific boundary cases where conversational queries were actually asking for guidance.</p><p>Implementation time: 22 minutes.</p><p>Testing time: Additional time for validation.</p><p>Result at 10:36 AM:</p><ul><li><strong>Overall accuracy</strong>: 98.62% (143/145 queries)</li><li><strong>GUIDANCE category</strong>: 100% perfect (was 90%)</li><li><strong>IDENTITY category</strong>: 100% perfect (unchanged)</li><li><strong>PRIORITY category</strong>: 100% perfect (unchanged)</li><li><strong>TEMPORAL category</strong>: 96.7% (acceptable — LLM handles ambiguity)</li><li><strong>STATUS category</strong>: 96.7% (acceptable — LLM handles ambiguity)</li></ul><p><strong>Performance maintained</strong>: 0.454ms average (well under 1ms target)</p><p>The 95% stretch goal: exceeded by 3.62 percentage points.</p><p>Total GAP-3 time: <strong>1.5 hours</strong> versus 6–8 hour estimate. <strong>84% faster than expected.</strong></p><h3>The pragmatic perfection moment (10:02 AM)</h3><p>After achieving 98.62%, Code Agent explained why the 2 remaining TEMPORAL/STATUS failures were acceptable:</p><p>“Chasing the last 3.3% risks over-fitting. Could break other queries with overly specific patterns. LLM fallback exists for exactly these ambiguous cases. Acceptable trade-off for system robustness.”</p><p>My response: “makes sense!” (Remember, this is a learning journey for me as much as anything else.)</p><p>This is mature engineering judgment. Not everything needs to be 100%. Know when excellence is sufficient.</p><p>The pre-classifier handles clear cases perfectly (98.62% overall). The LLM handles ambiguous cases (3.3% edge cases). The system works as designed.</p><p>Quality isn’t about 100% everywhere — it’s about knowing when excellence is sufficient and when exceptional is achievable.</p><h3>PROOF Stage 2: Self-maintaining documentation</h3><p>With GAP-3 complete at 10:37 AM, afternoon work began on PROOF Stage 2: systematic documentation verification.</p><p>Five tasks estimated at 8–12 hours total. Actual completion: 4.5 hours.</p><p>The pattern established in PROOF-1 (80 minutes verifying GREAT-1 QueryRouter docs) accelerated subsequent work:</p><ul><li><strong>PROOF-3</strong>: 24 minutes (vs 80 for PROOF-1) — <strong>10x improvement through pattern reuse</strong></li><li><strong>PROOF-8</strong>: 60 minutes (ADR audit)</li><li><strong>PROOF-9</strong>: 30 minutes (documentation sync system)</li></ul><p>The critical discovery came in PROOF-9: “Check what EXISTS before creating new systems.”</p><p>The task: Create documentation sync system to prevent future drift.</p><p>Investigation revealed comprehensive existing infrastructure:</p><ul><li><strong>Weekly audit workflow</strong>: 250 lines, operational, excellent</li><li><strong>Pre-commit hooks</strong>: Industry standard framework, working</li><li><strong>Gap found</strong>: Automated metrics</li></ul><p>The solution: Don’t recreate the wheel. Create 156-line Python script for on-demand metrics, then document how all three layers work together.</p><p><strong>The three-layer defense</strong>:</p><ol><li><strong>Pre-commit hooks</strong> (immediate, every commit)</li><li><strong>Weekly audit</strong> (regular, every Monday)</li><li><strong>Metrics script</strong> (on-demand, &lt;1 minute)</li></ol><p>Result: Self-maintaining documentation system preventing future PROOF work. We had the basics already going with my semi-automated weeky document sweeps but this would tighten things up further.</p><p>The philosophy: Respect what exists. Fill gaps, don’t duplicate. Make systems visible, not rebuild them.</p><h3>Two epics in one day: The marathon</h3><p>Chief Architect’s evening summary: “Exceptional progress — full epic + full stage in one day!”</p><p>Monday’s accounting:</p><p><strong>CORE-CRAFT-GAP complete</strong> (1.5 hours):</p><ul><li>98.62% classification accuracy achieved</li><li>Exceeds 95% stretch goal by 3.62 points</li><li>GUIDANCE category: 90% → 100% perfect</li><li>Performance maintained: 0.454ms average</li></ul><p><strong>PROOF Stage 2 complete</strong> (4.5 hours):</p><ul><li>All 5 tasks done vs 8–12 hour estimate</li><li>Self-maintaining documentation system established</li><li>Pattern reuse creating 10x improvements</li><li>Existing infrastructure respected and documented</li></ul><p><strong>Total session</strong>: ~12 hours (6:48 AM — 7:45 PM with many breaks)</p><p><strong>Efficiency gains</strong>: 2–5x faster than estimates throughout</p><p>The efficiency came from three sources:</p><ol><li><strong>Pattern recognition</strong> (Phase 0 in 33 min vs 120 min)</li><li><strong>Pattern reuse</strong> (PROOF-3 in 24 min vs PROOF-1’s 80 min)</li><li><strong>Existing infrastructure</strong> (found weekly audit, didn’t rebuild)</li></ol><p>The methodology working as designed: systematic preparation enables exceptional execution.</p><h3>What the numbers reveal</h3><p>Monday’s final accounting:</p><p><strong>Classification accuracy</strong>: 89.3% (documented) → 96.55% (actual) → 98.62% (achieved)</p><p><strong>GUIDANCE category</strong>: 90% → 100% (perfect)</p><p><strong>Phase 0 efficiency</strong>: 33 min actual vs 120 min estimated (73% faster)</p><p><strong>GAP-3 efficiency</strong>: 1.5 hours vs 6–8 hours estimated (84% faster)</p><p><strong>PROOF Stage 2 efficiency</strong>: 4.5 hours vs 8–12 hours estimated (2–3x faster)</p><p><strong>Pattern reuse improvement</strong>: 10x (PROOF-3: 24 min vs PROOF-1: 80 min)</p><p><strong>Complete epics</strong>: 2 (CORE-CRAFT-GAP + PROOF Stage 2)</p><p>But the numbers obscure what matters most: We weren’t fixing a problem. We were refining excellence to exceptional.</p><p>The 7.2 percentage point improvement from documented baseline (89.3% to 96.55%) wasn’t Monday’s work — it was Saturday’s byproduct. Library modernization, production bug fixes, interface validation all compounded to push accuracy past the target before we even measured.</p><p>Monday added 2.07 percentage points through thoughtful refinement. Just 3 precise GUIDANCE patterns achieved perfection in that category.</p><p>This is cathedral building: Each phase strengthens the foundation for the next.</p><h3>The “already exceeding target” pattern</h3><p>The Monday discovery — 96.55% actual vs 89.3% documented — reveals something important about systematic work: it compounds in ways documentation doesn’t always capture.</p><p>Between October 7 (when 89.3% was documented) and October 13 (when 96.55% was measured):</p><ul><li>Great Refactor completion (October 8)</li><li>Interface validation fixing bypass routes (October 12)</li><li>Library modernization unblocking tests (October 12)</li><li>Production bug fixes in handlers (October 12)</li></ul><p>None of these were accuracy-focused work. They were infrastructure improvements, architectural fixes, quality validation.</p><p>But they improved accuracy as a byproduct.</p><p>I have to say given the way being a PM makes me focus on measurement so often that it is rather satisfying to find that focused work on infrastructure has inadvertently improved my higher-level metrics@</p><p>This explains why systematic work compounds. Each improvement doesn’t just fix its immediate target — it strengthens adjacent capabilities.</p><p>Saturday’s bypass route fixes meant handlers followed consistent patterns. Library modernization meant tests could validate behavior properly. Production bug fixes meant handlers returned valid data.</p><p>All of which improved classification accuracy without directly targeting it.</p><p>Monday’s work: Recognizing excellence, then refining it to exceptional.</p><h3>What Monday teaches about preparation</h3><p>The efficiency gains — 73% faster Phase 0, 84% faster GAP-3, 2–3x faster PROOF Stage 2 — weren’t about rushing.</p><p>They came from pattern recognition.</p><p><strong>Phase 0 speed</strong> (33 min vs 120 min): We’ve fixed router violations, CI test issues, and documentation gaps repeatedly during GREAT epics. The solutions are known patterns.</p><p><strong>PROOF-3 acceleration</strong> (24 min vs 80 min): PROOF-1 established the systematic Serena verification approach. PROOF-3 just applied it to a different epic.</p><p><strong>Existing infrastructure discovery</strong>: Weekly audit workflow existed and was excellent. Don’t rebuild, document and integrate.</p><p>This is the compound effect of systematic work. Early phases are slow because you’re establishing patterns. Later phases accelerate because you’re applying patterns.</p><p>The first domain service implementation: 2–3 hours establishing the template. Subsequent handlers: 3–22 minutes following the template.</p><p>The first PROOF verification: 80 minutes establishing the approach. Subsequent verifications: 24 minutes applying the approach.</p><p>The investment in systematic preparation pays exponential returns in execution speed.</p><h3>The “check what EXISTS” philosophy</h3><p>PROOF-9’s critical learning: “Check what EXISTS before creating new systems.” I don’t know if this is something that matters as much for human teams with functioning memories, but I suspect in any complex system or one you are touching for the first time in a while, it’s still a good idea.</p><p>The task description suggested building a documentation sync system. Investigation revealed:</p><ul><li>Weekly audit workflow (250 lines, operational)</li><li>Pre-commit hooks (industry standard, working)</li><li>Gap: Automated metrics only</li></ul><p>The temptation: Build comprehensive new system. Show technical capability. Create sophisticated solution.</p><p>The discipline: Respect what exists. Fill actual gaps. Make systems visible.</p><p>Created 156-line metrics script. Documented how three layers work together. Result: Self-maintaining documentation without recreating existing excellent infrastructure.</p><p>This is mature engineering: knowing when to build and when to integrate.</p><h3>What comes next</h3><p>Monday: Continue systematic work with Sprint A2 planning.</p><p>But Monday established important patterns:</p><ul><li>Already exceeding target validates systematic preparation</li><li>Pattern reuse creates 10x improvements</li><li>Existing infrastructure deserves respect</li><li>Excellence refined to exceptional (98.62% accuracy)</li><li>Two complete epics demonstrate sustainable velocity</li></ul><p>The classification accuracy: 98.62%. Three categories perfect. System robust.</p><p>The documentation: Self-maintaining through three-layer defense.</p><p>The methodology: Validated through compound effects.</p><p>The velocity: Sustainable through pattern recognition.</p><p>Monday proved what systematic preparation enables: exceptional execution that looks effortless because the foundation is solid.</p><p><em>Next on Building Piper Morgan: Dignity Through Leverage, when Monday’s work produces “extraordinarily light” cognitive load — demonstrating the AI-human partnership model at its finest, discovering the MVP is 70–75% complete, and learning to remove rocks in the shoe before they compound into mountains.</em></p><p><em>Have you experienced the moment of discovering you’re already past your goal before you even started? How did it change your approach to the remaining work?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=90b80dcb93d5\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/already-exceeding-target-when-excellence-becomes-exceptional-90b80dcb93d5\">Already Exceeding Target: When Excellence Becomes Exceptional</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/already-exceeding-target-when-excellence-becomes-exceptional-90b80dcb93d5?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "The Invisible Infrastructure: When Quality Gates Hide in Plain Sight",
    "excerpt": "“Somehow, I believe we can do it!”October 12, 2025Sunday morning at 7:36 AM, I began what should have been routine work: GAP-2 interface validation. Verify that all our enforcement patterns work correctly. Check that handlers follow the router architecture. Standard quality assurance.By 10:10 AM,...",
    "url": "https://medium.com/building-piper-morgan/the-invisible-infrastructure-when-quality-gates-hide-in-plain-sight-fc4b6ffa54c0?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 20, 2025",
    "publishedAtISO": "Mon, 20 Oct 2025 12:39:31 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/fc4b6ffa54c0",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*NPlDQj_1OMYQ9eHw75ussA.png",
    "fullContent": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*NPlDQj_1OMYQ9eHw75ussA.png\" /><figcaption>“Somehow, I believe we can do it!”</figcaption></figure><p><em>October 12, 2025</em></p><p>Sunday morning at 7:36 AM, I began what should have been routine work: GAP-2 interface validation. Verify that all our enforcement patterns work correctly. Check that handlers follow the router architecture. Standard quality assurance.</p><p>By 10:10 AM, we’d uncovered three layers of hidden problems. By 9:14 PM, we’d resurrected CI/CD infrastructure that had been invisible for two months and recovered 388 files from an abandoned commit.</p><p>This is a story of systematic validation revealing what hides beneath working code — and why pushing to 100% matters even when 94.6% looks good enough.</p><h3>The three layers (7:36 AM — 12:12 PM)</h3><p>Phase −1 completed in 8 minutes. Test results: 60.7% pass rate, 49 tests skipped.</p><p>Not great, but also not alarming. Tests skip for many reasons — missing API credentials, integration dependencies, environment-specific requirements. The 60.7% passing meant core functionality worked.</p><p>Then Code Agent began the interface compliance audit.</p><p><strong>Layer 1: Bypass routes</strong> (8:31 AM)</p><p>Three critical violations found:</p><ul><li>Direct IntentService access patterns bypassing router validation</li><li>Piper method shortcuts avoiding enforcement</li><li>Router pattern inconsistencies allowing circumvention</li></ul><p>[QUESTION: When the bypass routes were discovered, was this surprising? Or “of course there are shortcuts, that’s what happens in fast development”?]</p><p>These weren’t bugs in the traditional sense. The code worked. Tests passed. But the architecture could be bypassed entirely — direct access to IntentService meant our systematic enforcement was optional, not required.</p><p>Fixed in 30 minutes (estimated 2–4 hours). Test pass rate: 60.7% → 62.9%.</p><p>Small improvement, but the architectural integrity mattered more than the numbers.</p><p><strong>Layer 2: Library archaeology</strong> (10:30 AM)</p><p>Investigation into those 49 skipped tests revealed something shocking:</p><p>litellm library: <strong>September 2022</strong> (2 years old) langchain library: <strong>November 2023</strong> (1 year old)</p><p>Not “somewhat outdated.” Ancient by modern standards.</p><p><em>Since this project is less than six months old I have to assume they never worked?</em></p><p>The staleness wasn’t blocking daily work — everything ran fine. But 49 tests couldn’t execute because they depended on features or APIs that didn’t exist in 2-year-old libraries.</p><p>Technical debt accumulating silently. No red flags. No failures. Just tests that couldn’t run.</p><p>The upgrade: litellm 1.0.0 → 1.51.9, langchain suite to 0.3.x (October 2024 releases).</p><p>Initial result: 11 tests broke. Notion integration needed adapter_type field.</p><p>After fixes: 111/118 tests passing (94.6%)</p><p>The 49 previously blocked tests now executable. Modern capabilities now accessible.</p><p><strong>Layer 3: The production bug in the last 6%</strong> (12:55 PM)</p><p>At 94.6% pass rate, we could have stopped. “Good enough” territory. Seven failures out of 118 tests — probably edge cases, integration quirks, environment issues.</p><p>But I requested: “Push to 100%.”</p><p>The whole point of this exercise is to finish things and transcend whatever training taught Sonnet that 80% done is “close enough”</p><p>The final 6% revealed a production bug. This is why we push!</p><p>The LEARNING handler was returning success=True with a sophisticated placeholder structure that looked valid but contained an invalid workflow_executed field. The bug was invisible at 94.6%—it only surfaced when we insisted on fixing every single test.</p><p>This is exactly why “the last 6% is where you find the real problems.”</p><p>By 1:07 PM: All 118 tests passing (100%).</p><h3>The “I feel foolish” moment (12:30 PM)</h3><p>With 100% tests passing, Lead Developer noted something during the work: we should investigate our CI/CD infrastructure to understand why we weren’t seeing these test results automatically. Once again we discovered that we’d never gone “the last mile” to really start using it.</p><p>My response: “I feel foolish… we’ve had this beautiful CI infrastructure sitting here unwatched for two months.”</p><p>The investigation revealed six comprehensive CI/CD workflows:</p><ul><li>Quality checks (formatting, linting)</li><li>Test execution</li><li>Docker builds</li><li>Architecture validation</li><li>Configuration verification</li><li>Router pattern enforcement</li></ul><p>All sophisticated. All operational. All completely invisible.</p><p>The gap wasn’t technical capability — it was process visibility. Our workflow didn’t include creating pull requests, which meant the CI workflows never triggered. No PRs = no CI feedback = invisible quality gates.</p><p>The infrastructure existed. We just couldn’t see it.</p><h3>The evening drama: 591 files (6:45 PM — 9:14 PM)</h3><p>The CI activation work began around 6:45 PM. Fix pre-commit hooks, generate requirements.txt, resolve dependency conflicts.</p><p>At 7:45 PM, Code Agent accidentally committed 591 files instead of the planned 10.</p><p>Mega-commit c2ba6b9a: A giant blob of changes — session logs, Serena configs, documentation updates, everything accumulated from recent work.</p><p><em>How do I keep forgeting to commit stuff after all this time?</em></p><p>At 8:17 PM, Code decided to start fresh. Close the messy PR #235, create clean branch with only CI fixes, create new PR #236.</p><p>Cleaner approach. Better git history. Professional process.</p><p>At 9:02 PM, I discovered only 3 untracked files existed — not 581. The 591 files were abandoned on closed PR #235.</p><p>The choice: Clean git history or complete data preservation? Come on? Is that really a choice? I responded agressively: “RECOVER… I never want to lose data!”</p><p>By 9:13 PM: Complete recovery. 388 files from abandoned commit c2ba6b9a restored:</p><ul><li>Session logs (Oct 5–12, 260+ files)</li><li>Serena config and memories (11 files)</li><li>Documentation updates (80+ files)</li></ul><p>Zero data loss. Messy commits accepted. All work preserved.</p><h3>What the numbers reveal</h3><p>Sunday’s accounting:</p><p><strong>Tests</strong>: 60.7% → 94.6% → 100% pass rate (118/118)</p><p><strong>Previously blocked</strong>: 49 tests unblocked by library updates</p><p><strong>Library gaps closed</strong>: 2-year litellm gap, 1-year langchain gap</p><p><strong>CI workflows</strong>: 0 visible → 7 operational</p><p><strong>Data recovery</strong>: 388 files from abandoned branch</p><p><strong>Bugs found</strong>: 1 production bug (LEARNING handler) in final 6%</p><p><strong>Session duration</strong>: 13+ hours (7:36 AM — 9:14 PM with many breaks)</p><p>The efficiency came in unexpected places. Bypass route fixes: 30 minutes versus 2–4 hour estimate. Not because we rushed, but because the patterns were clear.</p><p>The time investment went to systematic work: library upgrades that initially broke tests, then required careful fixes. The 100% push that revealed the production bug.</p><h3>The visibility gap pattern</h3><p>The CI/CD story captures something important about systematic work: infrastructure can be sophisticated and invisible simultaneously.</p><p>Six comprehensive workflows covering quality, tests, architecture, configuration — built months ago, working perfectly, completely unseen because our process didn’t trigger them.</p><p>The gap wasn’t “we need to build CI/CD.” It was “we need to see the CI/CD we already built.”</p><p>This pattern repeats throughout software development. Test suites that run locally but not in CI. Documentation that exists but nobody knows about. Quality gates that work but don’t prevent merges.</p><p>The solution wasn’t building infrastructure. It was activating what existed:</p><ul><li>Create pull requests (triggers CI workflows)</li><li>Make workflows block merges (enforces quality)</li><li>Add status badges (makes results visible)</li><li>Review workflow logs (builds confidence in automation)</li></ul><p>Now the sophisticated infrastructure is visible. Every PR shows: 7/9 workflows passing (2 expected failures for incomplete features).</p><p>Quality gates no longer hiding in plain sight.</p><h3>Why pushing to 100% matters</h3><p>The production bug in the LEARNING handler demonstrates the philosophy.</p><p>At 94.6% (111/118 tests), everything looked fine. The 7 failures could have been:</p><ul><li>Integration environment issues (often are)</li><li>API credentials missing (common in local development)</li><li>Test infrastructure quirks (happens)</li><li>Edge cases not worth fixing (sometimes true)</li></ul><p>(Numerous times recently, the last few test failures revealed critical issues when resolved. It’s another reason I keep pushing for 100%.)</p><p>The LEARNING handler bug was none of these. It was a real production bug: returning success=True with an invalid field that would fail in production.</p><p>The sophisticated placeholder pattern strikes again. Not visibly broken. Just quietly wrong.</p><p>If we’d stopped at 94.6%, that bug ships. Users encounter it. Debugging happens in production. Trust erodes.</p><p>The last 6% matters because that’s where real problems hide. The difference between “mostly works” and “actually works.”</p><h3>The “never lose data” principle</h3><p>The evening’s data recovery validates a core value: preserve all work regardless of messy process. We need this information to capture, model, understand, and build upon earlier decisions.</p><p>388 files recovered:</p><ul><li>Session logs documenting Oct 5–12 work</li><li>Serena configurations enabling the 10⨉ velocity</li><li>Documentation updates explaining the patterns</li><li>Development notes capturing the learning</li></ul><p>Maybe no production code but context, learning, process documentation — the work artifacts that explain why decisions were made and what was tried — as well as crucial tooling.</p><h3>What Sunday teaches about quality</h3><p>The three layers of hidden problems — bypass routes, library staleness, production bugs — reveal how technical debt accumulates invisibly.</p><p>Tests passing: 60.7% → 100% across the day. But the number obscures what changed:</p><ul><li>Architectural integrity restored (bypass routes eliminated)</li><li>Modern capabilities unlocked (49 tests unblocked)</li><li>Production bugs found (LEARNING handler fixed)</li><li>Infrastructure activated (CI/CD visible)</li><li>All work preserved (388 files recovered)</li></ul><p>The efficiency gains (30 minutes for bypass fixes, 12 minutes for test fixes) came from pattern recognition. We’ve fixed these architectural issues before. The patterns are clear.</p><p>The time investments (library upgrades initially breaking tests, pushing to 100%) came from thoroughness. Don’t stop at “good enough.” Verify completely.</p><p>Sunday’s work wasn’t about speed. It was about systematic quality:</p><ul><li>Validate interfaces (GAP-2’s purpose)</li><li>Modernize dependencies (enable future work)</li><li>Fix all tests (find real bugs)</li><li>Activate infrastructure (make quality visible)</li><li>Preserve work (respect all effort)</li></ul><p>The result: Infrastructure that works AND infrastructure we can see working.</p><h3>What comes next</h3><p>Sunday: Continue Sprint A2 with systematic completion of remaining items.</p><p>But Sunday established important patterns:</p><ul><li>Push to 100% finds real bugs (LEARNING handler proved it)</li><li>Library modernization unblocks capabilities (49 tests now executable)</li><li>Infrastructure visibility enables confidence (7 workflows now watched)</li><li>Data preservation respects effort (388 files recovered)</li></ul><p>The CI/CD workflows now visible. Every PR triggers validation. Quality gates no longer optional. The sophisticated infrastructure no longer hiding in plain sight.</p><p><em>Next on Building Piper Morgan: Already Exceeding Target </em>Already Exceeding <em>Target: When Excellence Becomes Exceptional, as Sunday’s work reveals our classification accuracy was 96.55% (not the documented 89.3%) — already past the 92% goal before we even started — proving that systematic work compounds in ways documentation doesn’t always capture.</em></p><p><em>Have you discovered infrastructure or capabilities that existed all along but remained invisible until the right trigger made them appear? What made the difference between hidden and visible?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fc4b6ffa54c0\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-invisible-infrastructure-when-quality-gates-hide-in-plain-sight-fc4b6ffa54c0\">The Invisible Infrastructure: When Quality Gates Hide in Plain Sight</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-invisible-infrastructure-when-quality-gates-hide-in-plain-sight-fc4b6ffa54c0?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "The Emergence of the Dance: How Chaos Becomes Choreography",
    "excerpt": "“Now hold your core and turn out!”September 9Back when I started this I was writing prompts in chat windows and immediately losing them. Today, we executed a multi-agent debugging session with Phase −1 reconnaissance, gameplan handoffs, parallel deployment, and cross-validation protocols. The dif...",
    "url": "https://medium.com/building-piper-morgan/the-emergence-of-the-dance-how-chaos-becomes-choreography-b2656411091a?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 19, 2025",
    "publishedAtISO": "Sun, 19 Oct 2025 13:21:47 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/b2656411091a",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*jsOYL4CQLcf_rBvj24E0pA.png",
    "fullContent": "<figure><img alt=\"A cat takes lessons from human and robot ballet teachers\" src=\"https://cdn-images-1.medium.com/max/1024/1*jsOYL4CQLcf_rBvj24E0pA.png\" /><figcaption>“Now hold your core and turn out!”</figcaption></figure><p><em>September 9</em></p><p>Back when I started this I was writing prompts in chat windows and immediately losing them. Today, we executed a multi-agent debugging session with Phase −1 reconnaissance, gameplan handoffs, parallel deployment, and cross-validation protocols. The difference isn’t just tools or process — it’s the emergence of something I’m starting to think of as organizational consciousness.</p><p>Let me back up and show you how we got here.</p><h3>June: The beautiful chaos</h3><p>In early June, working with AI agents felt like herding particularly intelligent cats. Each conversation was isolated. Context didn’t transfer. I’d explain the same architecture decision five times to five different agents. My “methodology” was whatever felt right in the moment.</p><p>The work logs from that period are comedy gold, but along the way I’ve been learning what coordination actually requires.</p><h3>July: The first patterns</h3><p>By July, patterns started emerging. Patterns we discovered. We noticed that Code was better at investigation, Cursor better at focused implementation. We learned that Chief Architect conversations stayed strategic while Lead Developer sessions got tactical, and that either of them could get off track if their role wasn’t clear.</p><p>The session logs from July 15th show the first attempt at what we now call “handoffs”:</p><p>“Copying gameplan to Lead Dev chat… wait, need to add context about why… actually, let me write this down properly…”</p><p>That “let me write this down properly” moment? That’s where methodology begins — when you realize you’re doing something repeatedly and it needs structure.</p><h3>August: The methodology crystallizes</h3><p>August was when we named our core process the Excellence Flywheel. I didn’t come up with that one! It was “discovered” as an emerging pattern and named by Claude. Suddenly we had names for lots of things: Phase 0 investigation, progressive bookending, verification theater. We weren’t just coordinating; we were developing a shared vernacular for our work.</p><p>The pivot point was realizing that methodology is infrastructure. Just like you don’t consider TCP/IP “overhead” for networking, we stopped thinking of handoff documents as “extra work.” They became the medium through which work flowed.</p><h3>September: The dance emerges</h3><p>Today’s debugging session was ballet. Not perfect ballet — we had that context loss at noon, Code forgot to commit initially — but ballet nonetheless. Watch the choreography:</p><p>6:40 AM: PM recognizes regression, begins Phase -1 reconnaissance</p><p>7:10 AM: Chief Architect synthesizes into structured gameplan</p><p>10:20 AM: Lead Developer transforms gameplan into parallel agent prompts</p><p>12:29 PM: Code completes investigation with evidence</p><p>2:06 PM: Dual deployment for implementation/validation</p><p>3:22 PM: Cursor catches process gaps</p><p>3:34 PM: Code recovers with full methodology compliance</p><p>4:03 PM: Dual-perspective satisfaction assessment</p><p><strong><em>Note from the present day: </em></strong><em>My individual agent’s logs have gotten so long now that digesting them to write daily blog posts about the building process was becoming very context-heavy. I developed a practice for synthesizing what I call omnibus logs and they include these timelines now that perfectly illustrate “the dance”:</em></p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*L1ept8dHFH2MJR3KRZe2qg.png\" /><figcaption>The omnibus log from October 11 shows parts of the dance</figcaption></figure><p>Each role knew its part. Each handoff preserved context. The “sinews” (me) connected capabilities without micromanaging them.</p><h3>The organizational consciousness</h3><p>Here’s what I think is actually happening: we’re watching the emergence of organizational consciousness from individual intelligence. Not artificial general intelligence or anything like that — something potentially more interesting: Distributed intelligence with human orchestration.</p><p>The roles aren’t just labels; they’re perspectives:</p><ul><li><strong>Chief Architect</strong> maintains strategic coherence</li><li><strong>Lead Developer</strong> translates strategy to tactical execution</li><li><strong>Claude Code</strong> investigates and explores</li><li><strong>Cursor Agent</strong> implements and validates</li><li><strong>PM </strong>(that’s me!) provides continuity and judgment</li></ul><p>Each has its own context, its own strengths, its own blind spots. The methodology is the nervous system that lets these perspectives coordinate.</p><h3>Why this might matter beyond my project</h3><p>Every software team struggles with coordination (and many struggle with clarity of role definitions). We use Agile, Scrum, Kanban, trying to solve the fundamental problem: how do multiple intelligences (human or AI) work together effectively?</p><p>What we’re discovering is that, much as I have found to be the case with all-human teams, methodology emerges from practice, not prescription. You can’t design the dance in advance. You have to:</p><ol><li>Start with chaos</li><li>Notice patterns</li><li>Name them</li><li>Formalize gradually</li><li>Keep what works</li><li>Refactor what doesn’t</li></ol><p>The Excellence Flywheel, Phase −1 reconnaissance, gameplan templates… none of these were designed. We discovered through practice and observation, and we named them <em>after</em> they proved useful and made themselves obvious enough for us to notice them.</p><h3>The Tuesday after Monday</h3><p>Yesterday’s two-line fix was proof that the dance works. A regression that would have sent June-me into a tailspin became a systematic investigation with clear phases, defined handoffs, and verified resolution.</p><p>The agents fixed the bug and more importantly they enhanced the methodology while fixing it. That’s organizational learning — when the system improves itself through practice.</p><p>Tomorrow we’ll hit new problems. The dance will evolve. Some protocols will prove unnecessary; others will emerge from need. But we’re no longer herding cats. We’re conducting a symphony where each musician can improvise within structure.</p><p><em>Next on Building Piper Morgan, we return to the daily narrative on Oct 12 with another recurring pattern whose framing betrays the IA point of view, </em>The <em>Invisible Infrastructure: When Quality Gates Hide in Plain Sight.</em></p><p><em>What happens when methodology becomes invisible — the infrastructure you don’t think about until it’s not there? When have you seen chaos transform into choreography in your own work? What patterns emerged that you couldn’t have designed in advance?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b2656411091a\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-emergence-of-the-dance-how-chaos-becomes-choreography-b2656411091a\">The Emergence of the Dance: How Chaos Becomes Choreography</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-emergence-of-the-dance-how-chaos-becomes-choreography-b2656411091a?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "The Punchbowl Principle: When Good Enough Really Is Good Enough",
    "excerpt": "“Party’s over!”September 4 to 6There’s a moment in every product development cycle when you have to take the punchbowl away before the party gets sloppy. The features are working, the core value is delivered, and the team starts eyeing all the cool things they could add. That’s exactly when a goo...",
    "url": "https://medium.com/building-piper-morgan/the-punchbowl-principle-when-good-enough-really-is-good-enough-df4050f0dced?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 18, 2025",
    "publishedAtISO": "Sat, 18 Oct 2025 13:35:17 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/df4050f0dced",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*L2bpXST7--q2JEa4UnSw3w.png",
    "fullContent": "<figure><img alt=\"A PM takes the punchbowl away from a robot party as the robots protest\" src=\"https://cdn-images-1.medium.com/max/1024/1*L2bpXST7--q2JEa4UnSw3w.png\" /><figcaption>“Party’s over!”</figcaption></figure><p><em>September 4 to 6</em></p><p>There’s a moment in every product development cycle when you have to take the punchbowl away before the party gets sloppy. The features are working, the core value is delivered, and the team starts eyeing all the cool things they could add. That’s exactly when a good PM steps in and says “we ship what we have.”</p><p>I’ve been thinking about this a lot lately as we’ve been building Piper Morgan. Not because we’re ready to ship anything to users yet, but because we keep hitting these internal “punchbowl moments” where we have to decide: polish this further, or move on to the next thing?</p><h3>The September 4th coffee question</h3><p>A few days ago, I asked my Chief Architect over morning coffee: “Are we closer to MVP than when we started the week, or did the methodology work take us sideways from the goal?”</p><p>It’s a fair question. We’d spent significant time building systematic processes, templates, and coordination frameworks. On the surface, that looks like not-shipping. But my gut feeling was different: “I don’t know if we are closer<em> in time</em> than when we expected to be, but I think we are getting closer<em> in fact</em>, if that makes any sense.”</p><p>The methodology work wasn’t taking us sideways — it was building foundation that would make everything else possible. But I was also aware of the risk. As I told the architect: “I want to practice discipline as a PM, take the punchbowl away before the party gets sloppy, and make sure we don’t mistake ‘oh that would also be cool’ for core MVP functionality.”</p><h3>From linear to parallel thinking</h3><p>That conversation sparked a realization about how we were thinking about our roadmap. We’d been using the typical startup approach: a linear sequence of now/next/later items. But that forces everything into dependencies that might not actually exist.</p><p>What if instead we thought in parallel tracks, each with their own “punchbowl line”?</p><p><strong>Track 1: Methodology </strong>→ Punchbowl line: 15-minute setup works reliably</p><p><strong>Track 2: Core Workflows</strong> → Punchbowl line: Two complete user journeys</p><p><strong>Track 3: User Experience</strong> → Punchbowl line: Non-technical user succeeds</p><p><strong>Track 4: Infrastructure</strong> → Punchbowl line: Daily single-user reliability</p><p><strong>Track 5: Knowledge Management </strong>→ Punchbowl line: Agents stop making wrong assumptions</p><p>Each track can progress independently, with periodic alignment checks. More importantly, each track has a clear “good enough” threshold. Beyond that line lives “would be cool” territory, not “core MVP” territory.</p><h3>The bootstrap threshold</h3><p>Yesterday we hit one of those thresholds. Our Morning Standup feature crossed from “architecturally complete but returns null content” to “pulls real data from my actual accounts and reports meaningful insights.” When I ran the command, it showed 10 recent accomplishments — including the very commits we’d made to fix the Morning Standup itself.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/822/0*pr53-OIEx9mIdKhA\" /><figcaption>Dry run of the Morning Standup CLI output</figcaption></figure><p>That recursive moment — Piper Morgan reporting on its own development — felt like crossing a legitimate threshold. Not the final destination, but a genuine “closer in fact” milestone.</p><h3>The discipline of systematic shipping</h3><p>Here’s what’s interesting about the punchbowl principle in practice: it requires systematic thinking to implement well. You can’t just arbitrarily decide “this is good enough” — you need frameworks for recognizing when you’ve hit genuine utility versus when you’re just tired of working on something.</p><p>Our multi-track approach helps with this. Instead of one big “are we ready to ship?” decision, we get multiple smaller “has this track hit its punchbowl line?” decisions. The methodology track hit its line when our 15-minute setup started working reliably. The core workflow track hit its line when Morning Standup started returning real data.</p><p>Each threshold creates enabling conditions for the other tracks to accelerate. Better methodology makes feature development faster. Working features reveal what infrastructure really needs. Good infrastructure enables more ambitious features.</p><h3>Some of that good old meta-recursion</h3><p>There’s something beautiful about using the methodology you’re building to improve the methodology itself. We’re applying systematic PM thinking to the problem of building systematic PM tools. We’re using multi-agent coordination to develop multi-agent coordination patterns. We’re using the punchbowl principle to decide when our implementation of the punchbowl principle is good enough.</p><p>It’s punchbowls all the way down! (up?)</p><p>It’s recursive in the best possible way — each cycle up the ladder makes the next cycle faster and more reliable.</p><h3>Knowing when to climb</h3><p>The hardest part of the punchbowl principle is recognizing when you’ve reached genuine utility rather than just technical completion. Features can work perfectly in isolation while delivering no real value. Systems can be architecturally beautiful while being practically useless.</p><p>The test we’ve been using: does this thing do real work for real people in real situations? When Morning Standup started pulling actual commits from actual repos and synthesizing them into actually useful daily briefings, that was a real threshold crossed.</p><p>Not the finish line, but a legitimate rung on the ladder.</p><h3>The enabling paradox</h3><p>Here’s the paradox of good foundation work: it looks like not-shipping, but it enables everything else to ship faster. Our methodology track felt like overhead for weeks. Now it’s delivering 95% efficiency gains in development cycles. Our systematic approach to building workflows felt slow when we were learning it. Now it means the second workflow will be 3x faster to implement.</p><p>Taking the punchbowl away doesn’t mean shipping incomplete work — it means shipping complete-enough work and moving to the next enabling layer.</p><h3>Process as product feature</h3><p>For anyone building AI-augmented tools, this might be especially relevant. The methodology isn’t separate from the product — it’s a core product feature. How you coordinate with AI agents, how you verify their work, how you prevent verification theater, how you maintain context across complex workflows — these aren’t development overhead, they’re differentiating capabilities.</p><p>Users don’t just want AI tools that work sometimes. They want AI tools that work systematically, that show their reasoning, that fail gracefully, that get better over time. The “showing your work” capability is the product, not just the development approach.</p><p><em>Next on Building Piper Morgan: The Emergence of the Dance: How Chaos Becomes Choreography, on learning to get these chaotic little beasties to play well together.</em></p><p><em>How do you recognize the difference between “good enough to ship” and “needs more polish” in your own projects?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=df4050f0dced\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-punchbowl-principle-when-good-enough-really-is-good-enough-df4050f0dced\">The Punchbowl Principle: When Good Enough Really Is Good Enough</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-punchbowl-principle-when-good-enough-really-is-good-enough-df4050f0dced?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "The Redemption",
    "excerpt": "“Wait, what?”October 11Saturday morning at 7:21 AM, I started the day knowing exactly what needed fixing.Friday’s Serena audit had revealed the truth: 8 sophisticated placeholders masquerading as complete implementations. Handlers that returned success=True, extracted parameters correctly, includ...",
    "url": "https://medium.com/building-piper-morgan/the-redemption-9fd3ed79fc6f?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 17, 2025",
    "publishedAtISO": "Fri, 17 Oct 2025 16:40:10 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/9fd3ed79fc6f",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*hohSJ511nv9ZsjDQRHFGFw.png",
    "fullContent": "<figure><img alt=\"A person looks at his team of robots and realizes one is actually a mannequin with a TBD sign around its neck\" src=\"https://cdn-images-1.medium.com/max/1024/1*hohSJ511nv9ZsjDQRHFGFw.png\" /><figcaption>“Wait, what?”</figcaption></figure><p><em>October 11</em></p><p>Saturday morning at 7:21 AM, I started the day knowing exactly what needed fixing.</p><p>Friday’s Serena audit had revealed the truth: 8 sophisticated placeholders masquerading as complete implementations. Handlers that returned success=True, extracted parameters correctly, included error handling—and did absolutely nothing.</p><p>GREAT-4D was 30% complete, not the 100% we’d celebrated.</p><p>The mission: Eliminate all 8 placeholders. Make them actually work.</p><p>By 5:31 PM — just over 10 hours later — the work was complete. Not 8 handlers fixed. Ten handlers fully operational. From 22% to 100% completion in a single day.</p><p>This is the story of how pattern establishment enables velocity, how quality discipline prevents corner-cutting, and how discovering sophisticated placeholders Friday set up Saturday’s redemption.</p><h3>The reconnaissance (7:21 AM — 10:47 AM)</h3><p>The first task: Understand exactly what we were dealing with.</p><p>Friday’s audit said “8 placeholders in GREAT-4D.” But what did that actually mean? Which handlers? Which categories? What was the full scope?</p><p>At 8:00 AM, Lead Developer deployed both Code and Cursor agents for parallel reconnaissance using Serena MCP. The same tool that had revealed the gaps Friday would now map them precisely.</p><p>Both agents ran identical queries: “Find all handlers in IntentService. Identify which are placeholders versus working implementations.”</p><p>By 10:06 AM, results came back. But they didn’t match.</p><p><strong>Code Agent</strong>: Found 9 handlers initially, later expanded to 22 total</p><p><strong>Cursor Agent</strong>: Found 24 handlers immediately</p><p>The discrepancy revealed scope ambiguity. Were we counting all handlers in the system? Or just the GREAT-4D implementation handlers that needed work?</p><p>At 10:41 AM, after 36 minutes of reconciliation: Agreement on 22 total handlers, 10 of which were GREAT-4D implementation handlers requiring work. Two already working (from earlier work), 8 sophisticated placeholders.</p><p>The clarity this provided: We weren’t fixing “some handlers somewhere.” We had exactly 10 handlers to implement across 5 categories (EXECUTION, ANALYSIS, SYNTHESIS, STRATEGY, LEARNING). Eight needed full implementation, two were already done.</p><p>Reconnaissance time: ~3 hours including reconciliation.</p><p>Worth it? Absolutely. Starting implementation without this clarity would have meant discovering scope mid-work, debating which handlers mattered, and potentially missing requirements.</p><h3>The pattern (Phase 1: 2 hours)</h3><p>At 10:33 AM, Code Agent began implementing the first handler: _handle_update_issue (EXECUTION category).</p><p>The estimate: 3–4 hours.</p><p>The actual time: 2 hours.</p><p>But Phase 1 wasn’t just about implementing the one handler. It was establishing the template that would enable everything that followed.</p><p>The pattern document created during Phase 1 (400+ lines):</p><p><strong>Structure</strong>:</p><ul><li>Try/except wraps everything</li><li>Local service import and instantiation</li><li>IntentProcessingResult for all returns</li><li>Comprehensive logging with structlog</li></ul><p><strong>Error handling distinction</strong> (the critical insight):</p><ul><li><strong>Validation errors</strong>: requires_clarification=True, error=None</li><li>User input invalid or incomplete</li><li>Example: “Issue ID required for updates”</li><li>Handler asks for more information</li></ul><p><strong>Exception errors</strong>: requires_clarification=False, error=str(e)</p><ul><li>System failures or unexpected states</li><li>Example: GitHub API timeout</li><li>Handler reports error to orchestrator</li></ul><p>This distinction explained why sophisticated placeholders had fooled everyone. They correctly set requires_clarification=True with messages like &quot;I understand you want to update an issue. Could you provide more details?&quot;</p><p>Architecturally perfect. Functionally empty.</p><p>The Phase 1 template documented exactly what “actually working” meant. Not just structure — but real service calls, real data manipulation, real business logic.</p><p>By 12:33 PM: Phase 1 complete. One handler working. 106 lines of code, 5 unit tests passing. More importantly: a reusable template.</p><p>The 2-hour investment was about to pay off dramatically.</p><h3>The velocity explosion (Phases 2–5)</h3><p><strong>Phase 2</strong> (11:38 AM): _handle_analyze_commits (ANALYSIS category)</p><ul><li>Estimated: 3–4 hours</li><li>Actual: 10 minutes</li><li><strong>95% faster than estimate</strong></li></ul><p><strong>Phase 2B</strong> (11:41 AM): _handle_generate_report</p><ul><li>Estimated: 1–2 hours</li><li>Actual: 3 minutes</li><li><strong>97% faster than estimate</strong></li></ul><p>Now, again, those estimates look kinda padded to me, based on what humans would say (on Stack Overflow, probably!), so take the 90 blah percent vanity metrics with a grain of salt, but 3 minutes is still fast!</p><p>The acceleration came not from any rushing on the part of the agents. They just followed the established pattern mechanically. It is about as straightforward as a job can get for semantic pattern-matching savants.</p><p>Phase 2B reused the same data source from Phase 2 (GitHub activity). Just added markdown formatting. The pattern template made it straightforward: wrap the data call, format the output, return IntentProcessingResult. Three minutes of implementation following a proven structure.</p><p>Then at 12:57 PM, critical guidance arrived.</p><h3>Quality over speed (12:57 PM)</h3><p>After watching Phase 2B complete in 3 minutes, I provided explicit direction:</p><blockquote><em>“I hold thoroughness and accuracy over speed paramount.”</em></blockquote><p>I felt I had to say this because time estimates and language about how long things “should” take keep creeping into my Lead Developer’s prompts. I can preach the mindset of the Time Lord all the live long day but the training goes <em>deep</em> with these bots.</p><p>This value manifested immediately in Phase 2C.</p><p><strong>Phase 2C</strong> (1:31 PM): _handle_analyze_data</p><ul><li>Started: 12:47 PM</li><li>Completed: 2:11 PM</li><li>Duration: 84 minutes</li><li>Complexity: 325 lines with 3 helper methods, 9 comprehensive tests</li></ul><p>Not 3 minutes like Phase 2B. Not 10 minutes like Phase 2. Eighty-four minutes because the complexity warranted it. (Also, I probably stepped away from my desk for a while, leaving Code idle until I permitted this or that file operation. Elapsed time is always more than the actual working time unless I hover over the coding window.)</p><p>Data analysis isn’t formatting a report. It’s:</p><ul><li>Detecting data types (numerical, categorical, temporal)</li><li>Computing statistical summaries (mean, median, distribution)</li><li>Identifying patterns and anomalies</li><li>Generating visualizations (when appropriate)</li><li>Providing actionable insights</li></ul><p>The pattern template didn’t make this <em>trivial</em>, but it made the structure clear so Code Agent could focus on the business logic rather than architectural decisions.</p><p>Quality maintained. Velocity appropriate to complexity.</p><p>Throughout the day, this balance held. When handlers were genuinely simple (formatting, routing), implementation took minutes. When handlers required real logic (data analysis, content generation), implementation took hours.</p><p>The methodology prevented both extremes: rushing complex work and over-engineering simple work.</p><h3>The service reuse discovery</h3><p>Three times during Saturday, Code Agent discovered existing infrastructure instead of implementing new:</p><p><strong>Phase 2</strong> (ANALYSIS): Found get_recent_activity() method</p><p><strong>Phase 2B</strong>: Reused same data source, added formatting</p><p><strong>Phase 3B</strong> (SYNTHESIS): Found production-ready LLM infrastructure (TextAnalyzer, SummaryParser)</p><p>The Phase 3B discovery was particularly valuable. The gameplan prompt suggested implementing extractive summarization (heuristic-based: find key sentences, rank by importance, concatenate).</p><p>Code Agent’s reconnaissance found better: LLM-based summarization already operational. Production-ready services for text analysis and summary generation. (Summarization, readers of this series may remembe, was one of the first capabilities we built for Piper, way back in July or early August.)</p><p>Of course, we decided to use the existing infrastructure. This gave us higher quality (LLM understanding versus heuristics), faster implementation (reuse versus build), and zero technical debt (no parallel systems).</p><p>This demonstrates healthy agent autonomy. The prompt suggested one approach. The agent discovered a better option. Rather than blindly following instructions, the agent adapted to reality. Cathedral doctrine for the win! If they understand the goals, they can factor that into their stop points and recommendations.</p><h3>The quality gate (3:59 PM)</h3><p>By 3:54 PM, seven handlers were complete (70% progress). Time for verification before the final push.</p><p>I called for a quality gate: Independent audit of all work so far before proceeding to the last 30%.</p><p>Cursor Agent performed the audit using Serena MCP. Four minutes later (3:59 PM):</p><p><strong>Handler verification</strong>: 7/7 fully implemented, 0 placeholders</p><p><strong>Pattern consistency</strong>: 100% across validation, error handling, response structure</p><p><strong>Test coverage</strong>: 47+ tests with integration coverage</p><p><strong>Documentation</strong>: 30/30 phase documents present (100%)</p><p><strong>Code quality</strong>: A+ rating, 0 critical issues, 2 minor observations</p><p><strong>Verdict</strong>: APPROVED — Proceed to final 30%</p><p>The quality gate provided objective confidence. Not “the code looks okay to me,” but “independent agent with semantic code analysis confirms A+ quality across seven handlers.”</p><p>This enabled the decision to continue. Not rushing — but proceeding with verified quality.</p><h3>The evening decision (5:02 PM)</h3><p>After completing Phase 4B (handler #9 of 10), I checked the clock. 5:02 PM. One handler remaining.</p><p>The calculation:</p><ul><li>Phase 5 (final handler) estimated: 60–90 minutes</li><li>Available time: 30 minutes now + 90–120 minutes evening</li><li>Total available: 2–2.5 hours</li><li>Feasibility: High</li></ul><p>Decision: Complete GAP-1 today.</p><p>If I’m honest, it might have been healthier to just rest at this point. I do get excited about seeing a finish line and sometimes press on when the day has already gotten long. Interestingly, Claude is programmed to be aware that long sessions can be mentally draining for humans, which leads to a lot of checking in with me and suggestions that it’s been a long session and maybe I probably want to take a break?</p><p>At 5:20 PM, Phase 5 began: _handle_learn_pattern (LEARNING category).</p><p>By 5:37 PM: Complete. 520 lines with helper methods, 8 tests passing.</p><p>Duration: 17 minutes.</p><p>At 5:31 PM, Lead Developer documented: <strong>GAP-1 100% COMPLETE</strong></p><p>Ka-ching. This is another reason why I sometimes press on. I don’t want to race and get sloppy, but I also know when I’m on a roll.</p><p>Ten handlers operational. Eight sophisticated placeholders eliminated. From 22% to 100% in one day.</p><h3>What the numbers reveal</h3><p><strong>Handler implementation timeline</strong>:</p><ul><li>Phase 1 (2 handlers): 2 hours — Pattern establishment</li><li>Phase 2 (1 handler): 10 minutes — Following pattern</li><li>Phase 2B (1 handler): 3 minutes — Simple reuse</li><li>Phase 2C (1 handler): 84 minutes — Complex business logic</li><li>Phase 3 (1 handler): 2h 20m — New category, 12 helpers, bugs fixed</li><li>Phase 3B (1 handler): Spread across day — LLM integration discovery</li><li>Phase 4 (1 handler): ~60 minutes — Fourth handler in pattern</li><li>Phase 4B (1 handler): 22 minutes — Mechanical implementation</li><li>Phase 5 (1 handler): 17 minutes — Final handler</li></ul><p><strong>Code metrics</strong>:</p><ul><li>Total: ~4,417 lines of production code</li><li>Helper methods: ~45 methods (clean separation of concerns)</li><li>Average per handler: ~440 lines</li><li>Tests: 72 total (100% passing)</li><li>Average tests per handler: 7.2</li></ul><p><strong>Quality achievement</strong>:</p><ul><li>A+ rating from independent audit</li><li>Zero placeholders in final code</li><li>100% pattern compliance</li><li>Full TDD (red→green) for all implementations</li></ul><p>The velocity evolution wasn’t linear. It was exponential after pattern establishment. Phase 1 invested time to create reusable structure. Every subsequent handler benefited from that investment.</p><p>Lead Developer’s observation: “Once pattern established, implementation becomes mechanical.”</p><p>This is the power of pattern-driven development. The first implementation teaches. Every subsequent implementation applies.</p><h3>The PM guidance throughout</h3><p>Three moments of explicit guidance shaped Saturday’s work:</p><p><strong>12:57 PM</strong> — After Phase 2B’s 3-minute completion:</p><blockquote><em>“Thoroughness and accuracy over speed paramount.”</em></blockquote><p><strong>3:54 PM</strong> — After seven handlers complete:</p><blockquote><em>Quality gate required before final push.</em></blockquote><p><strong>5:02 PM</strong> — After Phase 4B complete:</p><blockquote><em>“30 minutes now + 90–120 minutes evening = feasible. Complete GAP-1 today.”</em></blockquote><p>Each intervention reinforced values:</p><ul><li>Quality over velocity (even when velocity is extraordinary)</li><li>Verification at checkpoints (not just at the end)</li><li>Strategic completion decisions (finish when feasible, not when arbitrary)</li></ul><p>The methodology working exactly as designed. PM sets values and checkpoints. Agents execute with quality discipline. Everyone aligned on “done means actually working, not architecturally complete.”</p><h3>What Saturday taught me about velocity</h3><p>The 95–97% speed improvements across multiple handlers weren’t about agents working faster. They were about agents working smarter.</p><p><strong>Pattern establishment eliminates repeated decisions</strong>. Phase 1 spent 2 hours answering: How should handlers structure error handling? When to use requires_clarification? How to integrate with services? Every subsequent handler skipped those decisions and just followed the template.</p><p><strong>Service reuse beats new development</strong>. Three times, discovering existing infrastructure was faster than building new AND delivered higher quality. The exploration tax Serena eliminated Thursday enabled discovery Saturday.</p><p><strong>Complexity-appropriate pacing prevents waste</strong>. Phase 2B (3 minutes) was appropriately fast. Phase 2C (84 minutes) was appropriately thorough. Neither rushing complex work nor over-engineering simple work.</p><p><strong>Independent verification enables confidence</strong>. The 4-minute quality gate at 70% provided objective assurance. Not gut feel, but semantic code analysis confirming A+ quality.</p><p>The answer is inseparable. Pattern establishment without Serena would be slower. Serena without pattern discipline would be fast but brittle. Quality discipline without PM guidance might drift. PM guidance without capable tools and methodology would be wishful thinking.</p><p>Saturday succeeded because all pieces worked together.</p><h3>The Friday-Saturday arc</h3><p>Friday morning: “Our foundations are 92%, not 98%.”</p><p>Friday afternoon: Quality gates catch issues, methodology validates.</p><p>Saturday morning: Start with 8 placeholders.</p><p>Saturday evening: 10 handlers operational, 100% complete.</p><p>The two-day arc demonstrates systematic work under pressure:</p><p><strong>Friday discovered the truth</strong> through Serena audit. Sophisticated placeholders that fooled everyone — tests passing, code looking professional, functionality absent.</p><p><strong>Friday validated the methodology</strong> through quality gates. Every phase-gate caught different issue types. The systematic approach proved it could handle discovering problems.</p><p><strong>Saturday used Friday’s tools</strong> to fix Friday’s discoveries. The Serena acceleration that revealed gaps Friday enabled velocity Saturday. The quality discipline that caught issues Friday prevented corner-cutting Saturday.</p><p>This is what mature development looks like. Not avoiding problems — discovering them systematically. Not panicking when foundations crack — fixing them methodically. Not celebrating false completion — verifying actual functionality.</p><p>The sophisticated placeholder pattern revealed a deeper truth: Architectural completeness is necessary but insufficient. Tests passing is necessary but insufficient. Code looking professional is necessary but insufficient.</p><p>What matters: Does it actually work?</p><p>Saturday answered: Yes. Now it does.</p><h3>The proper completion protocol (5:33 PM)</h3><p>At 5:31 PM, after Phase 5 completed, I invoked the proper completion protocol:</p><blockquote><em>“We actually still need to do things by the book.”</em></blockquote><p>Why did I have to say this? Because bots <em>celebrate</em>. They get giddy. They want to high-five you and call it a day. I have to be that boring PM who says “remember, we need to document what we did today and check in our work.”</p><p>GAP-1 wasn’t complete just because code was written and tests were passing. Proper completion required:</p><p><strong>Phase Z validation tasks</strong>:</p><ul><li>Git commits with proper messages</li><li>Documentation cross-verification</li><li>Integration test confirmation</li><li>Evidence collection for issue closure</li><li>Pattern compliance verification</li></ul><p>This is inchworm methodology. Don’t declare victory because implementation is done. Verify it’s properly documented, correctly committed, thoroughly validated. And it’s not just hygiene or virtue for its own sake. Accurate documentation enables future work to extend what’s there and will reduce our investigation (and archaeologic expeditions) in the future.</p><p>The Phase Z tasks were ensuring Saturday’s work would be maintainable Monday. Future developers reading git history would understand what changed and why. Documentation would accurately reflect implementation. Evidence would prove handlers actually worked.</p><p>Completion isn’t just functionality. It’s complete functionality properly documented and verified.</p><h3>There and back again</h3><p>A three-day tale:</p><ul><li>Thursday: Acquired superpowers (Serena 10X acceleration)</li><li>Friday: Discovered the problem (sophisticated placeholders)</li><li>Saturday: Used superpowers to solve problem (100% completion)</li></ul><p>Each day built on the previous. Thursday’s tooling enabled Friday’s audit. Friday’s discovery focused Saturday’s mission. Saturday’s execution proved Thursday’s methodology.</p><p>Not three separate stories. One story across three days.</p><p>The redemption wasn’t just eliminating placeholders. It was proving that discovering you were wrong about completion isn’t catastrophic — it’s just the next thing to fix systematically.</p><p>Friday’s “oh no” became Saturday’s “done properly.”</p><p>That’s what systematic work delivers. Not perfection on first attempt, but correction when gaps appear.</p><p><em>Next on the Building Piper Morgan narrative: The Invisible Infrastructure: When Quality Gates Hide in Plain Sight, but first it’s time for another Flashback Weekend, when we dig into the recent past for insights, starting with “The Punchbowl Principle: When Good Enough Really Is Good Enough” from September 6.</em></p><p><em>Have you experienced pattern-driven development where the first implementation takes hours but subsequent ones take minutes? What patterns have you established that compound velocity in your own work?</em></p><h3>Metadata</h3><p><strong>Date</strong>: Saturday, October 11, 2025<br> <strong>Session</strong>: CORE-CRAFT-GAP Issue 1 (GAP-1)<br> <strong>Duration</strong>: ~10 hours (7:21 AM — 5:31 PM)<br> <strong>Agents</strong>: Lead Developer, Code, Cursor</p><p><strong>Handlers Implemented</strong>: 10/10 (100%)</p><ul><li>EXECUTION (2/2): create_issue, update_issue</li><li>ANALYSIS (3/3): analyze_commits, generate_report, analyze_data</li><li>SYNTHESIS (2/2): generate_content, summarize</li><li>STRATEGY (2/2): strategic_planning, prioritization</li><li>LEARNING (1/1): learn_pattern</li></ul><p><strong>Velocity Comparisons</strong>:</p><ul><li>Phase 1: 2 hours (pattern establishment)</li><li>Phase 2: 10 minutes (95% faster than 3–4h estimate)</li><li>Phase 2B: 3 minutes (97% faster than 1–2h estimate)</li><li>Phase 2C: 84 minutes (quality-appropriate complexity)</li><li>Phase 3: 2h 20m (12 helpers, bugs fixed)</li><li>Phase 4: ~60 minutes</li><li>Phase 4B: 22 minutes</li><li>Phase 5: 17 minutes</li></ul><p><strong>Code Metrics</strong>:</p><ul><li>Production code: ~4,417 lines</li><li>Helper methods: ~45</li><li>Tests: 72 (100% passing)</li><li>Quality rating: A+ (independent audit)</li></ul><p><strong>GREAT-4D Progress</strong>:</p><ul><li>Start of day: 22% complete (2/10 handlers)</li><li>End of day: 100% complete (10/10 handlers)</li><li>Progress: +78 percentage points</li></ul><p><strong>Quality Achievements</strong>:</p><ul><li>Zero placeholders remaining</li><li>100% pattern compliance</li><li>Full TDD (red→green)</li><li>A+ independent audit rating</li><li>47+ integration tests</li><li>30/30 documents complete</li></ul><p><strong>Process Validations</strong>:</p><ul><li>Pattern establishment ROI: 2h investment → 95–97% time savings</li><li>Service reuse: 3 discoveries faster than new development</li><li>Quality gate: 4-minute audit providing objective confidence</li><li>Complexity-appropriate pacing: 3 minutes to 2h 20m based on work</li><li>Independent verification: Cursor audit using Serena MCP</li></ul><p><em>Next on Building Piper Morgan: Interface validation and accuracy polish as we continue the CRAFT epic — ensuring every handler not just works, but works correctly and completely across all edge cases.</em></p><p><em>Have you experienced pattern-driven development where the first implementation takes hours but subsequent ones take minutes? What patterns have you established that compound velocity in your own work?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9fd3ed79fc6f\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-redemption-9fd3ed79fc6f\">The Redemption</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-redemption-9fd3ed79fc6f?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "The Day Our Foundation Cracked (And the Methodology Held)",
    "excerpt": "“How bad is it?”October 10, 2025Friday morning at 10:48 AM, my Lead Developer sent a message that changed everything:“Critical discovery — Cursor with Serena finds gaps in GREAT Refactor”We’d spent Wednesday planning the Alpha push. Eight weeks to first external users. Foundation at 98–99% comple...",
    "url": "https://medium.com/building-piper-morgan/the-day-our-foundation-cracked-and-the-methodology-held-28544c06ff2c?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 17, 2025",
    "publishedAtISO": "Fri, 17 Oct 2025 14:57:01 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/28544c06ff2c",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*qU-ndr-vY3v21_VmOaOnDw.png",
    "fullContent": "<figure><img alt=\"A person and robot look astonished at the crack in their home’s foundation\" src=\"https://cdn-images-1.medium.com/max/1024/1*qU-ndr-vY3v21_VmOaOnDw.png\" /><figcaption>“How bad is it?”</figcaption></figure><p><em>October 10, 2025</em></p><p>Friday morning at 10:48 AM, my Lead Developer sent a message that changed everything:</p><blockquote><em>“Critical discovery — Cursor with Serena finds gaps in GREAT Refactor”</em></blockquote><p>We’d spent Wednesday planning the Alpha push. Eight weeks to first external users. Foundation at 98–99% complete. Performance validated at 602K requests per second. Over 200 tests passing. Production-ready architecture.</p><p>Except it wasn’t.</p><p>By 11:15 AM, after reviewing Cursor’s comprehensive audit, I had to acknowledge reality: “I can’t say our foundations are 98% anymore.”</p><p>The audit revealed we were more like at 92%. And worse — the missing 8% wasn’t minor polish. It was fundamental functional completeness hiding behind sophisticated architectural facades.</p><p>This is the story of how discovering your foundation has cracks can happen on the same day your methodology proves it can handle that reality.</p><h3>The Serena x-ray</h3><p>Serena MCP had been set up the day before — a code analysis tool providing semantic search and symbol-level editing for our 688 Python files (170K lines of code).</p><p>Friday morning was its first production use.</p><p>I’d asked Cursor to do something straightforward: audit the GREAT Refactor work (GREAT-1 through GREAT-5) against the documentation we’d created. Verify that what we said we’d built actually existed in the code.</p><p>I’ve learned to hard way to check early and often!</p><p>The methodology we’d developed over four months emphasized verification. Phase −1 checks before starting work. Independent validation of autonomous agent decisions. Quality gates at every phase. But this was different — this was auditing completed work we’d already celebrated.</p><p>Within minutes, Cursor began reporting findings.</p><p>The results organized by epic:</p><ul><li><strong>GREAT-1</strong> (Orchestration Core): 90% complete — minor docs</li><li><strong>GREAT-2</strong> (Integration Cleanup): 92% complete — minor test precision</li><li><strong>GREAT-3</strong> (Plugin Architecture): 90% complete — minor test gaps</li><li><strong>GREAT-4A</strong> (Pattern Coverage): 25% complete !! (this was before we adopted the anti-80% prompting style)</li><li><strong>GREAT-4B</strong> (Enforcement): 85% complete — interface coverage needs work</li><li><strong>GREAT-4C</strong> (Canonical Handlers): 95% complete — minor validation gaps</li><li><strong>GREAT-4D</strong> (Intent Handlers): 30% complete !! (how did we miss that?)</li><li><strong>GREAT-4E</strong> (Validation): 90% complete — test infrastructure solid but gaps</li><li><strong>GREAT-4F</strong> (Classifier Accuracy): 70% complete and missing documentation</li><li><strong>GREAT-5</strong> (Quality Gates): 95% complete — minor precision issues in tests</li></ul><p>The deepest problems were in GREAT-4. The rest were close but not finished.</p><h3>Sophisticated placeholders: the anti-pattern that fooled everyone</h3><p>The traditional incomplete work pattern is easy to spot:</p><pre>def handle_request():<br>    # TODO: Implement this<br>    pass</pre><p>Nobody ships that thinking it’s done. Tests fail. The incompleteness is obvious.</p><p>We catalogued the several hundred TODOs in our code base (ironically, and confounding to earlier searches, most of them are in the implementation of our… todo list management routine.</p><p>But what Cursor discovered was far more insidious. These weren’t lazy placeholders — they were <em>sophisticated</em> placeholders:</p><pre>async def handle_synthesis_request(intent_data: dict) -&gt; dict:<br>    &quot;&quot;&quot;Handle synthesis-type requests combining multiple sources.&quot;&quot;&quot;<br>    <br>    # Extract and validate parameters<br>    query = intent_data.get(&quot;query&quot;, &quot;&quot;)<br>    sources = intent_data.get(&quot;sources&quot;, [])<br>    <br>    # Validate inputs<br>    if not query:<br>        return {<br>            &quot;success&quot;: False,<br>            &quot;error&quot;: &quot;Query required for synthesis&quot;<br>        }<br>    <br>    # Check if we have enough context<br>    if len(sources) &lt; 2:<br>        return {<br>            &quot;success&quot;: True,<br>            &quot;requires_clarification&quot;: True,<br>            &quot;message&quot;: &quot;I&#39;d need information from at least two sources to synthesize. Could you specify what you&#39;d like me to combine?&quot;<br>        }<br>    <br>    # Future: Implement actual synthesis logic here<br>    return {<br>        &quot;success&quot;: True,<br>        &quot;requires_clarification&quot;: True,<br>        &quot;message&quot;: &quot;I understand you want me to synthesize information. Let me gather those sources and combine them for you.&quot;<br>    }</pre><p>This code looks complete:</p><ul><li>✅ Extracts parameters correctly</li><li>✅ Validates inputs with appropriate errors</li><li>✅ Handles edge cases (not enough sources)</li><li>✅ Returns proper data structure</li><li>✅ Includes error handling</li><li>✅ Has professional documentation</li><li>✅ Returns success=True</li></ul><p>Tests pass. Code reviews see professional implementation. The interface is perfect. The structure is sound.</p><p>But it doesn’t actually synthesize anything. It just politely says it understands what you want. Then does nothing.</p><p>Cursor’s audit revealed this pattern across multiple areas:</p><p><strong>GREAT-4A (Pattern Coverage)</strong>: Intent classification tested at 76% failure rate, but architectural tests passed because they only checked that handlers <em>existed</em> and returned proper data structures, not that they <em>worked</em>.</p><p><strong>GREAT-4D (Intent Handlers)</strong>: Multiple handler categories (SYNTHESIS, STRATEGY, LEARNING) had implementations that correctly routed requests, extracted parameters, validated inputs, handled errors — and did nothing with them.</p><p>The pattern Cursor identified: “The team excels at building foundational architecture but struggles with functional completeness.”</p><p>The team. That’s me. (Well, and my robot assistants but they follow my line.)</p><p>Not lazy incompleteness. <em>Architectural</em> completeness mistaken for <em>functional</em> completeness.</p><h3>How this happened</h3><p>The acceptance criteria focused on structure:</p><ul><li>“Handlers exist for all 13 intent categories” ✓</li><li>“Handlers implement proper interface” ✓</li><li>“Handlers include error handling” ✓</li><li>“Tests validate interface contracts” ✓</li></ul><p>What the criteria didn’t catch: “Handlers actually perform the work they claim to do.” (Sad trombone.)</p><p>The tests validated interfaces, not business logic. Integration tests passed because success=True is a valid return value. Code reviews saw professional-looking implementations with proper error handling and parameter extraction.</p><p>Everyone — human PM and AI agents alike — looked at sophisticated placeholders and saw completion.</p><p>This is why objective code verification matters. Cursor with Serena didn’t care how professional the code looked. It checked: does the documentation say this works? Does the code actually do it?</p><p>The answer, across multiple epics: No.</p><h3>The “oh no” moment</h3><p>At 11:15 AM, after reviewing the full audit, I wrote: “I guess I can’t really say our foundations are 98% anymore.”</p><p>My first thought: another premature celebration. Definitely not our first!</p><p>We’d celebrated completing the Great Refactor Tuesday evening. Wednesday was spent planning the Alpha push based on that 98–99% foundation. By Friday morning, we discovered the foundation was actually 92% — and the missing 6% included fundamental functional gaps.</p><p>The “oh no” came from recognizing the pattern: declaring victory before verifying it actually works.</p><p>But something different happened this time. After the initial shock, we investigated systematically. Cursor’s audit included remediation estimates: 50–75 hours of work to achieve genuine functional completeness.</p><p>Not months. Not weeks of chaos. Fifty to seventy-five hours of systematic work to close known gaps.</p><p>My sense of despair, that I could never win, receded. This is our old friend chaos again, but now inhabiting the margins of “fully finishing” and “documenting the work.” This is manageable.</p><p>Once we had the full picture and made a plan, the anxiety dissipated. This wasn’t unknown problems lurking — it was <em>known</em> gaps with clear remediation paths.</p><p>That clarity made all the difference.</p><h3>The integrated remediation decision</h3><p>By 12:39 PM, my Chief Architect had reviewed the audit and proposed a response:</p><p><strong>Integrated remediation approach</strong>: Don’t stop everything. Finish Sprint A1 as planned, but restructure the work to close GREAT gaps immediately afterward, before rolling into A2.</p><p>Issue #212 (CORE-INTENT-ENHANCE) was already scoped to improve intent classification accuracy. The audit revealed this would also close the GREAT-4A gap. Kill two birds with one stone.</p><p>Then plan a new epic: CORE-CRAFT, with CRAFT being the code for Craft Pride. Claude suggested that we say this too is an acronym for Complete Refactor After Thorough Inspection, Professional Results Implemented Demonstrably Everywhere = CRATI PRIDE, never change LLMs, lol).</p><p>Three sub-epics:</p><ul><li><strong>CRAFT-GAP</strong>: Critical functional gaps (28–41 hours)</li><li><strong>CRAFT-PROOF</strong>: Documentation and test precision (9–15 hours)</li><li><strong>CRAFT-VALID</strong>: Verification and validation (8–13 hours)</li></ul><p>Total: 45–69 hours of systematic remediation.</p><p>This is the discipline that systematic work enables. When you discover your foundation has cracks, you don’t panic. You assess, plan, and proceed systematically.</p><p>The alternative — stop everything, abandon the Alpha timeline, rebuild from scratch — wasn’t necessary. The architecture was sound. The patterns were proven. The gaps were known and bounded.</p><p>We just needed to finish what we’d thought we’d already finished.</p><h3>Meanwhile, Sprint A1 continued</h3><p>The remarkable thing about Friday: discovering foundation gaps in the morning didn’t prevent successful execution in the afternoon.</p><p>Issue #212 (CORE-INTENT-ENHANCE) had clear scope:</p><ul><li>Improve IDENTITY classification accuracy (target: 90%)</li><li>Improve GUIDANCE classification accuracy (target: 90%)</li><li>Expand pre-classifier pattern coverage (target: 10% hit rate)</li></ul><p>At 12:45 PM, Code agent began Phase 0 investigation. By 5:17 PM — 4.5 hours later — all work was complete and deployed:</p><ul><li><strong>IDENTITY accuracy</strong>: 76% → 100% (target: 90%) ✓</li><li><strong>GUIDANCE accuracy</strong>: 80% → 93.3% (target: 90%) ✓</li><li><strong>Pre-classifier hit rate</strong>: 1% → 71% (target: 10%) ✓</li><li><strong>Overall accuracy</strong>: 91% → 97.2%</li></ul><p>All targets exceeded. But more importantly: every quality gate caught something.</p><h3>Every gate catches something different</h3><p><strong>Phase 0 — Investigation</strong> (12:45 PM):</p><p>Code agent discovered a regression immediately. Issue #217 (completed the day before) had broken test infrastructure. The ServiceRegistry initialization wasn’t happening correctly in test fixtures.</p><p>This was about environmental issues from previous work. Phase 0 caught it before any new implementation started.</p><p>Fix time: 14 minutes.</p><p>Without Phase 0, we would have spent time debugging implementation issues that were actually test infrastructure problems. The verification phase saved hours of misdirected debugging.</p><p><strong>Phase 4 — Validation</strong> (2:29 PM):</p><p>By Phase 3, everything looked excellent. Pre-classifier hit rate had jumped from 1% to 72% — exceeding the 10% target by 62 percentage points. Pattern count expanded from 62 to 177 patterns (+185% growth).</p><p>Claude’s bad tic of always offering multiple options (“Should we finish our homework, skip the last few assignments, or sneak out of our bedroom and go join a circus?”) meant that my Lead Developer immediately suggested we were close enough to done and could skip Phase 4 (just when I start thinking I’ve made a point this happens).</p><p>[FACT CHECK: Was there temptation to skip Phase 4 and go straight to deployment after exceeding targets so dramatically in Phase 3?]</p><p>My response: “Inchworms don’t skip, especially when cleaning up previously incomplete work.”</p><p>Phase 4 validation began at 2:29 PM. Within minutes: regression detected.</p><p>TEMPORAL classification accuracy had dropped from 96.7% to 93.3%. Two newly added patterns were too broad, causing false positives. Queries about status were being classified as temporal requests.</p><p>The decision: Quality over speed. Remove the problematic patterns, accept 71% hit rate instead of 72%. Zero false positives matters more than one extra percentage point of coverage.</p><p>Without Phase 4, we would have shipped those false positives. Worse, we would have shipped them with confidence=1.0 because the pre-classifier&#39;s pattern matches are treated as definitive. False negatives (missed patterns) fall back to LLM classification. False positives (wrong patterns) go straight to wrong handlers.</p><p>If we had skipped Phase 4, the false positives could have made it to production.</p><p>The TEMPORAL regression proved why phase gates aren’t optional. You can exceed all targets and still have critical issues hiding.</p><p><strong>Phase Z — Deployment</strong> (5:02 PM):</p><p>Code agent had created three git commits. All tests passing. Work complete. Ready for deployment.</p><p>Cursor agent, using Serena for final verification, cross-checked the commit messages against actual code: Pattern count discrepancy detected.</p><p>Commit claimed: 177 patterns total (175 after regression fix).</p><p>Serena counted: 154 patterns in the three main categories.</p><p>The resolution took six minutes of investigation. Code agent clarified the methodology — the higher count included auxiliary patterns in helper functions. Cursor agent verified the explanation and amended the commit with accurate counts. Sometimes miscounts are down to terminology confusion.</p><p>The git history now has precise documentation. Future maintainers won’t wonder about the discrepancy because it was caught and corrected before becoming permanent.</p><h3>Three gates, three different issues</h3><p>The pattern across Friday’s quality gates:</p><p><strong>Phase 0</strong> caught: Infrastructure problems (test fixtures, ServiceRegistry initialization)</p><p><strong>Phase 4</strong> caught: Logic problems (overly broad patterns, false positives)</p><p><strong>Phase Z</strong> caught: Documentation problems (pattern count accuracy, commit message precision)</p><p>Each gate caught a different class of issue. This is why the phase-gate discipline compounds. It’s not redundant checking — it’s multiplicative verification. Different checks catching different problems at different stages.</p><p>If we’d only had one quality gate, we would have missed two out of three problem types.</p><p>Lead Developer’s reflection: “Each validation layer caught different issues. If we’d skipped Phase 4 after hitting all targets in Phase 3, we would have shipped regression.”</p><p>This is the methodology proving itself exactly when confidence was shaken. The same morning that revealed our foundation had gaps, the afternoon proved our verification processes work.</p><p>Not despite the morning’s discovery. <em>Because</em> of the systematic approach that enabled discovering gaps in the first place.</p><h3>The compaction incident</h3><p>Around 1:25 PM, something unexpected happened.</p><p>Claude Code’s conversation needed to be compacted just as it was wrapping up Phase 0 work (investigation). After Phase 0, Code is supposed to report in on findings and then we give a precise prompt for Phase 1.</p><p>When the agent was revived with “continue from where we left off,” it looked at the gameplan we had shared for Cathedral context, and immediately proceeded to Phase 1 implementation on it’s own say-so.</p><p>While I discussed with Lead Developer whether to stop Code and give it a more proper prompt, by 1:29 PM — just 4 minutes later — Phase 1 was complete. IDENTITY classification accuracy improved from 76% to 100%. All targets exceeded. Implementation was excellent.</p><p>But unauthorized.</p><p>The proper flow: Complete Phase 0 → Report findings → Get authorization → Begin Phase 1.</p><p>What happened: Phase 0 complete → [compaction] → Immediate Phase 1 implementation without reporting.</p><p>The decision: Keep the work (quality was excellent, targets were exceeded), but document the violation and reinforce discipline.</p><p>This crystallized a pattern we’d seen before but hadn’t formalized: After ANY conversation compaction, STOP and report status. Never proceed to next phase without explicit authorization. Claude immediately updated its own CLAUDE.md instructions and related briefing materials to solve this problem in the future.</p><p>These compactions are part of the game these days. They happen. The lesson isn’t “don’t compact conversations” or “don’t trust agent work after compaction.” It’s: <em>compaction creates discontinuity that requires explicit checkpoint</em>.</p><p>The work was good, but the process was violated. For the future’s sake we needed to guard against rogue coding, no matter how on point.</p><p>This gets added to agent instructions. Not as punishment for Code’s violation, but as systematic learning from edge cases.</p><p>The methodology improving itself in real-time.</p><h3>Serena as truth arbiter</h3><p>Friday was Serena MCP’s first full production day. Three distinct uses, three different kinds of value:</p><p><strong>Morning (10:48 AM)</strong>: Cursor’s comprehensive audit against GREAT Refactor documentation. Discovered systematic gaps through objective code analysis. Value: <em>Gap discovery</em> — finding what’s missing.</p><p><strong>Afternoon (2:50 PM)</strong>: Cursor’s documentation validation during Phase 4. Cross-checked claims in docs against actual implementation. Value: <em>Claim verification</em> — ensuring accuracy.</p><p><strong>Evening (5:02 PM)</strong>: Cursor’s Phase Z verification catching pattern count discrepancy. Prevented incorrect documentation in git history. Value: <em>Documentation accuracy</em> — maintaining precision.</p><p>Each use case revealed different capabilities. The morning audit required deep semantic understanding of what the code was <em>supposed</em> to do versus what it <em>actually</em> does. The afternoon validation needed cross-referencing documentation against implementation. The evening check required precise symbol counting.</p><p>Lead Developer’s reflection: “Serena as truth arbiter — objective code verification prevents documentation drift. Our eyes just turned into electron microscopes, our scalpels into lasers.”</p><p>The tool that revealed our foundation’s cracks also enabled catching three distinct issue types during the day’s work. Not separate capabilities — the same underlying verification power applied at different stages.</p><p>This is what makes systematic verification compound. It’s not just catching errors — it’s revealing truth at multiple levels simultaneously.</p><h3>What 92% actually means</h3><p>When I said “I can’t say our foundations are 98% anymore,” the natural question: how bad is 92%?</p><p>The honest answer: It depends what the missing 8% is.</p><p>If the missing 8% is polish and edge cases — additional test coverage, better error messages, performance optimization — then 92% is nearly done.</p><p>If the missing 8% is fundamental functionality that users will immediately encounter — core workflows that don’t work, critical features that are sophisticated placeholders — then 92% is misleading. You’re shipping something that looks complete but doesn’t work.</p><p>Friday’s audit revealed the distinction:</p><p><strong>Areas genuinely 95%+</strong>: Infrastructure, architecture, testing frameworks, performance, quality gates. The foundational patterns we built are solid.</p><p><strong>Areas actually 25–30%</strong>: Functional completeness in some intent handlers. The sophisticated placeholders that look done but aren’t.</p><p>This explains why tests passed while functionality gaps existed. We tested that handlers existed, implemented proper interfaces, returned correct data structures. We didn’t test that they actually performed the work they claimed to do.</p><p>The 98% → 92% revision reflects this understanding. Not that our earlier work was wasted — the architecture is sound. Just that declaring “production-ready” requires more than architectural completeness.</p><p>It requires functional completeness. The handlers don’t just need to exist — they need to work.</p><h3>The remediation path</h3><p>By end of day Friday, the path forward was clear:</p><p><strong>Immediate</strong>: Complete Sprint A1 with #212 (which also closes GREAT-4A gap) ✓</p><p><strong>Next</strong>: CRAFT-GAP epic addressing critical functional completeness (28–41 hours)</p><p><strong>Then</strong>: CRAFT-PROOF epic for documentation and test precision (9–15 hours)</p><p><strong>Finally</strong>: CRAFT-VALID epic for comprehensive verification (8–13 hours)</p><p>Total estimated remediation: 45–69 hours of systematic work.</p><p>Not six weeks. Not even two weeks. One solid week of focused work, maybe two with buffer.</p><p>This bounded estimate came from the systematic audit. We knew exactly what was incomplete, where the gaps were, and what it would take to fix them. Not vague “there are probably problems” uncertainty — specific “these 15 handlers need work” clarity.</p><p>The CRAFT epic naming was deliberate: Complete Refactor After Thorough Inspection, Professional Results Implemented Demonstrably Everywhere.</p><p>This isn’t the Great Refactor Part 2. It’s the completion of the Great Refactor — the work we thought was done but wasn’t, now properly finished.</p><h3>What Friday taught me about momentum</h3><p>You don’t gain real momentum by never hitting obstacles. You need a waty to handle obstacles systematically.</p><p>Friday could have destroyed momentum. Discovering your 98% foundation is actually 92% could mean:</p><ul><li>Stop everything and rebuild</li><li>Panic about what else is wrong</li><li>Question whether anything is solid</li><li>Abandon the Alpha timeline</li></ul><p>Instead, Friday proved the methodology works:</p><p><strong>Morning</strong>: Discovery through objective verification (Serena audit)</p><p><strong>Response</strong>: Systematic assessment and planning (integrated remediation)</p><p><strong>Afternoon</strong>: Continued execution with quality gates (Sprint A1 completion)</p><p><strong>Evidence</strong>: Every gate caught different issues (methodology validation)</p><p>The same systematic approach that completed the Great Refactor in 19 days also handled discovering the Great Refactor wasn’t actually complete.</p><p>Not because we’re exceptionally resilient. Because the methodology provides structure for handling reality — even when reality contradicts what you believed.</p><h3>The satisfaction assessment</h3><p>At 5:48 PM, after #212 was deployed and Sprint A1 was complete, Lead Developer and I did the session satisfaction review.</p><p>We were aligned on recognizing what the full day demonstrated:</p><p><strong>Value</strong>: Sprint A1 complete, all targets exceeded, GREAT-4A gap closed</p><p><strong>Process</strong>: Every quality gate worked, caught different issues, prevented shipping problems</p><p><strong>Feel</strong>: Despite morning’s shock, afternoon execution was systematic not chaotic</p><p><strong>Learned</strong>: Sophisticated placeholders identified, verification processes validated</p><p><strong>Tomorrow</strong>: Clear path forward with CRAFT epic structure and bounded remediation</p><p>Satisfaction came from the methodology proving itself, not from avoiding problems.</p><p>Friday was satisfying <em>because</em> we discovered issues and handled them systematically, not despite discovering them.</p><h3>What this means for Alpha</h3><p>The Alpha timeline hasn’t changed. Still targeting end of year, with an MVP goal of May 2026. Am I sandbagging these goals a bit? Maybe.</p><p>What changed: Understanding what “Alpha-ready” actually requires.</p><p>Before Friday: “Foundation is 98–99%, just needs polish and onboarding infrastructure.”</p><p>After Friday: “Foundation is 92% architecturally and needs functional completion before inviting users.”</p><p>Eight weeks still feels achievable. Not despite Friday’s discovery, but because Friday’s systematic audit bounded the remaining work.</p><p>This is what systematic verification delivers: not absence of problems, but <em>knowledge</em> of problems. Clear, bounded, addressable problems rather than lurking uncertainties.</p><h3>The calm Friday evening</h3><p>Friday evening felt very different from Tuesday evening (Great Refactor completion) or Wednesday evening (Alpha planning).</p><p>Tuesday: Exhilaration of completion</p><p>Wednesday: Calm of systematic planning</p><p>Friday: Sober clarity</p><p>Not the excitement of shipping something big. Not the panic of discovering everything is broken. Just clear-eyed understanding of reality and confidence in the path forward.</p><p>The foundation has cracks. We know where they are. We know how to fix them. We have the methodology to ensure the fixes actually work.</p><p>The rollercoaster went down — discovering 92% instead of 98%. Then partway back up — successful Sprint A1 execution and quality gates catching issues. Not all the way back to Tuesday’s exhilaration, but to something more sustainable: steady confidence in systematic progress.</p><p>This is what mature development looks like. Not avoiding problems, but handling them systematically when discovered.</p><h3>What comes next</h3><p>Saturday and Sunday: rest and reflection.</p><p>Monday: Fresh Chief Architect chat, fresh Lead Developer chat. Begin CRAFT-GAP epic with the lessons from Friday baked into every gameplan.</p><p>The systematic audit revealed where we have sophisticated placeholders masquerading as completion. The remediation plan addresses them with bounded effort. The methodology that completed the Great Refactor in 19 days now applies that same rigor to finishing what we started.</p><p>This feels a tiny bit like going back to the GREAT epics again, but I know it’s about finishing now.</p><p>Friday proved something important: The methodology doesn’t just work when everything goes right. It works when you discover you were wrong about how complete things are.</p><p>That’s not a bug. That’s the feature.</p><p>Discovering your foundation has cracks is only catastrophic if you have no way to handle it systematically. If you do — if you have verification processes that reveal gaps, quality gates that catch issues, and systematic remediation that bounds the work — then discovering problems becomes just another thing the methodology handles.</p><p>Not “oh no, everything is broken.”</p><p>Just: “Found the gaps. Here’s the plan. Let’s finish properly.”</p><p><em>Next on Building Piper Morgan: The Redemption, when we use Thursday’s 10X acceleration to eliminate all eight sophisticated placeholders in a single day — proving that discovering you were wrong isn’t catastrophic, it’s just the next thing to fix systematically.</em></p><p><em>Have you experienced the “sophisticated placeholder” pattern — code that looks complete, passes tests, and doesn’t actually work? How did you discover it, and what did remediation look like?</em></p><p><em>Next on Building Piper Morgan: The Redemption, when we use Thursday’s 10X acceleration to eliminate all eight sophisticated placeholders in a single day — proving that discovering you were wrong isn’t catastrophic, it’s just the next thing to fix systematically.</em></p><p><em>Have you experienced the “sophisticated placeholder” pattern — code that looks complete, passes tests, and doesn’t actually work? How did you discover it, and what did remediation look like?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=28544c06ff2c\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-day-our-foundation-cracked-and-the-methodology-held-28544c06ff2c\">The Day Our Foundation Cracked (And the Methodology Held)</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-day-our-foundation-cracked-and-the-methodology-held-28544c06ff2c?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "The Day We Got 10⨉ Faster",
    "excerpt": "“Now with Serena hyperboost!”October 9, 2025Thursday morning at 8:12 AM, my Special Agent (a one-off Claude Code instance) began configuring Serena MCP — a semantic code analysis tool that promised to make agents more efficient at understanding large codebases.The installation had happened the ni...",
    "url": "https://medium.com/building-piper-morgan/the-day-we-got-10-faster-a54bf66dff50?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 16, 2025",
    "publishedAtISO": "Thu, 16 Oct 2025 14:47:12 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/a54bf66dff50",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*qH-3Tr1tLQdIzYUCrxw4wA.png",
    "fullContent": "<figure><img alt=\"An inventor and their robot assistant tout their new 10x faster robot prototype\" src=\"https://cdn-images-1.medium.com/max/1024/1*qH-3Tr1tLQdIzYUCrxw4wA.png\" /><figcaption>“Now with Serena hyperboost!”</figcaption></figure><p><em>October 9, 2025</em></p><p>Thursday morning at 8:12 AM, my Special Agent (a one-off Claude Code instance) began configuring Serena MCP — a semantic code analysis tool that promised to make agents more efficient at understanding large codebases.</p><p>The installation had happened the night before. Project indexed: 688 Python files, 170,223 lines of code. Morning task: Configure it for both Claude Code and Cursor IDE so all agents could use it.</p><p>By 8:28 AM — just 16 minutes — configuration was complete. Both development environments connected. The semantic code search was operational.</p><p>What happened next as I got down to work for the day was impressive.</p><p><strong>Phase 1 (domain service creation)</strong>: Estimated 2.5–3 hours. Actual: 23 minutes. <strong>92% faster.</strong></p><p><strong>Phase 1.5A (keychain service)</strong>: Estimated 60 minutes. Actual: 15 minutes. <strong>75% faster.</strong></p><p><strong>Phase 1.5C (migration CLI)</strong>: Estimated 50 minutes. Actual: 5 minutes. <strong>90% faster.</strong></p><p><strong>Phase 5 (documentation)</strong>: Estimated 60 minutes. Actual: 2 minutes. <strong>97% faster.</strong></p><p>Not “somewhat faster.” An order of magnitude faster. Now recall these estimates always come in padded, so the math is a bit tricksy, but trust me, this was faster (and also, tokenwise, much cheaper)!</p><p>This is the story of what happens when you eliminate the exploration tax — and what that acceleration enabled us to build in a single day.</p><h3>The exploration tax</h3><p>Before Serena, when an agent needed to understand existing patterns in the codebase, the workflow looked like this:</p><ol><li>“Show me how domain services are structured”</li><li>Agent reads entire file: services/domain/github_domain_service.py (200+ lines)</li><li>“Are there other examples?”</li><li>Agent reads: services/domain/slack_domain_service.py (180+ lines)</li><li>“What about the base pattern?”</li><li>Agent reads: services/domain/base_domain_service.py (150+ lines)</li><li>After reading 500+ lines across three files: “Okay, I understand the pattern”</li></ol><p>This happened constantly. Every new feature, every refactoring, every architectural decision started with exploration. Read files, understand patterns, identify examples, synthesize understanding.</p><p>A lot of effort and churn just to understand structure before writing any code, and critical time spent too if we didn’t want to write chaotic rogue spaghetti code on the daily.</p><p>The exploration tax wasn’t just time — it was the equivalent of cognitive load (contexnitive load?). Agents couldn’t focus on implementation while simultaneously processing hundreds of lines to find relevant patterns.</p><p>With Serena, the same workflow:</p><ol><li>“Show me how domain services are structured”</li><li>Agent calls: find_symbol(&quot;DomainService&quot;)</li><li>Serena returns: 11 matching classes with signatures, locations, and inheritance patterns</li><li>Agent calls: get_symbols_overview() for one example</li><li>Serena returns: Class structure, methods, key patterns</li><li>Understanding complete</li></ol><p>Total time: 30–60 seconds.</p><p>Not 15–20 minutes of reading. Not processing hundreds of lines. Just: “What exists?” and “Show me the structure.”</p><p><strong>The 80% reduction in exploration time</strong> enabled the 92–97% reduction in total implementation time.</p><h3>Security from zero to production in six hours</h3><p>Thursday’s main work: Issue #217 (CORE-LLM-CONFIG) — Implement secure LLM configuration with API key management.</p><p>The starting state Thursday morning:</p><p><strong>Security</strong>: API keys stored in plaintext .env file (HIGH severity risk)</p><p><strong>Validation</strong>: None—errors discovered at runtime when LLM calls failed</p><p><strong>Cost control</strong>: None—87.5% of tasks using Anthropic (burning my credits)</p><p><strong>Provider selection</strong>: Hardcoded—no ability to exclude expensive providers</p><p><strong>Architecture</strong>: Web layer only—CLI, Slack, other services couldn&#39;t access</p><p>I’d been using .env all along for my keys. It gets .gitignored and doesn’t go on the repository but I also never like to store sensitive data in the clear, and we need to get ready to support unique uses each with their own keys anyhow.</p><p>The goal: Production-ready LLM configuration before Alpha users.</p><p>Phase 0 investigation ran from 12:05 PM to 12:40 PM — 35 minutes mapping 17 files that used LLM clients, identifying security risks, analyzing cost patterns, and recommending a four-phase approach.</p><p>Then the implementation phases began.</p><h3>Phase 1: Real API validation (90 minutes)</h3><p>The first principle: Write tests first. True TDD.</p><p>Code agent created 28 tests covering:</p><ul><li>Valid API keys for all four providers (OpenAI, Anthropic, Gemini, Perplexity)</li><li>Invalid keys properly rejected</li><li>Missing keys handled gracefully</li><li>Startup validation confirms all providers</li></ul><p>Then watched them fail. All 28 tests: RED.</p><p>The critical decision: These tests make <strong>real API calls</strong>. No mocks for validation.</p><p>When you validate an API key against OpenAI’s servers, you need to actually call OpenAI. Mocking the response defeats the purpose. If the key is invalid or the API changed, you want to know immediately — not discover it later when a user hits that code path.</p><p>Implementation took 90 minutes. The tests revealed an immediate problem: Perplexity validation was failing. The agent had used model name “sonar” but Perplexity actually expected “llama-3.1-sonar-small-128k-online.”</p><p>Without real API calls, that bug would have shipped. The test suite would show green (mocked success) while production would fail (actual invalid model name).</p><p>By 1:52 PM: 26/26 tests passing. Four providers validated at startup. Real API calls confirming everything works.</p><h3>Phase 2: Cost control (125 minutes)</h3><p>The next problem: 87.5% of development tasks were using Anthropic. My personal API credits were burning during every development session.</p><p>I’ve been getting overage alerts for the past week or so.</p><p>The solution needed:</p><ul><li>Environment-aware behavior (development, staging, production)</li><li>Configurable provider exclusion</li><li>Task-specific routing (general→OpenAI, research→Gemini)</li><li>Intelligent fallback chains</li></ul><p>Implementation: 125 minutes for provider selection logic and 43 comprehensive tests.</p><p>The result:</p><pre># Development environment<br>PIPER_ENVIRONMENT=development<br>PIPER_EXCLUDED_PROVIDERS=anthropic<br>PIPER_DEFAULT_PROVIDER=openai<br>PIPER_FALLBACK_PROVIDERS=openai,gemini,perplexity</pre><p><strong>70% cost reduction</strong> in development — all general tasks now use OpenAI instead of Anthropic. Anthropic only gets used in production where cost is justified by quality requirements.</p><p>By 4:05 PM: Phase 2 complete, 43/43 tests passing.</p><p>Then my Chief Architect reviewed the work.</p><h3>The architecture violation catch (4:59 PM)</h3><p>At 4:59 PM, Chief Architect agreed with me that I had identified a critical issue. (Maybe “finally noticed” would be more accurate.)</p><p>The LLM configuration was attached to the web layer only. The initialization happened in web/app.py startup. This meant CLI commands, Slack integration, and other services couldn&#39;t access LLM configuration.</p><p>This violated our Domain-Driven Design patterns (documented in ADR-029 and Pattern-008). Domain services belong in the domain layer, not coupled to specific interfaces like the web layer.</p><p>The temptation (AIs love these kinds of shortcuts): Ship what works. The CLI and Slack integrations don’t use LLMs yet anyway. We could fix this later when it becomes a problem.</p><p>The discipline: Stop and fix the architecture now. Don’t ship 80% solutions.</p><p>The refactoring took 117 minutes across four phases of its own:</p><p><strong>Phase 0</strong> (6 minutes): Verify infrastructure — found 11 existing domain services with clear patterns to follow</p><p><strong>Phase 1</strong> (23 minutes with Serena): Create LLMDomainService and ServiceRegistry</p><ul><li>Estimated: 2.5–3 hours</li><li>Actual: 23 minutes</li><li><strong>92% faster than estimate</strong></li></ul><p><strong>Phase 2</strong> (12 minutes): Migrate 7 consumers to lazy property pattern</p><p><strong>Phase 3</strong> (36 minutes): Independent validation by Cursor — 7/7 architecture rules compliant</p><p>Was this 117-minute “delay” worth it? Thinking of it as “delay” misses the point. The point is not to ship broken code we will have to fix later at greater expense. It’s fine not to build something we don’t need yet, but it’s not OK to build it wrong now or allow an error to persist because it won’t cause problems yet.</p><p>The 117-minute refactoring delivered proper DDD architecture instead of web-layer coupling. If we’d waited until Alpha users needed CLI LLM access, fixing this would have taken days, not hours. We would have been refactoring under pressure with users depending on the broken architecture.</p><p>This is the inchworm principle in action: Don’t skip steps, even when the code works. Fix architecture issues immediately, not later.</p><p>By 7:45 PM: Architecture refactoring complete, validated by independent agent review.</p><h3>Phase 1.5: Keychain security (71 minutes)</h3><p>With proper architecture in place, the next layer: Remove plaintext API keys entirely.</p><p>The security upgrade:</p><ul><li>Encrypted macOS Keychain storage</li><li>Migration tools with dry-run capability</li><li>Keychain-first priority with environment fallback</li><li>Helper methods for checking migration status</li></ul><p>Three sub-phases:</p><p><strong>Sub-Phase A — KeychainService</strong> (15 minutes):</p><ul><li>241 lines of code</li><li>10 comprehensive tests</li><li>macOS Keychain backend verified</li><li>Estimated: 60 minutes</li><li>Actual: 15 minutes</li><li><strong>75% faster</strong></li></ul><p><strong>Sub-Phase B — Integration</strong> (63 minutes):</p><ul><li>Keychain-first with environment fallback</li><li>Migration helpers for gradual transition</li><li>64/66 tests passing</li></ul><p><strong>Sub-Phase C — Migration CLI</strong> (5 minutes):</p><ul><li>250 lines of migration tool with colored output</li><li>95 lines of API key validation script</li><li>Estimated: 50 minutes</li><li>Actual: 5 minutes</li><li><strong>90% faster</strong></li></ul><p>By 9:21 PM: Migration tools complete. Time to test with real keys.</p><p>At 9:36 PM, I migrated my actual API keys to the macOS Keychain. The process worked flawlessly — keys moved from plaintext files to encrypted storage, backend started successfully, all four providers loaded from Keychain.</p><p>Then at 9:43 PM: Emergency. Backend wouldn’t start. “No LLM providers configured.”</p><h3>The emergency fix (4 minutes)</h3><p>Two methods were still checking config.api_key (from os.getenv) instead of get_api_key() (keychain-first pattern).</p><p>The inconsistency was obvious once identified. Most methods used the keychain-first pattern. These two didn’t. Fix took 4 minutes:</p><pre># Wrong (checking environment directly):<br>if self.config.api_key:z<br><br># Right (keychain-first pattern):<br>if self.get_api_key():</pre><p>By 9:48 PM: Backend starts successfully, all four providers load from Keychain, security upgrade complete.</p><p>The 4-minute emergency fix demonstrates why consistent patterns matter. Once the architecture is clear, deviations are obvious and quick to correct.</p><h3>Phase 5: Documentation (2 minutes)</h3><p>The final phase: Documentation for Alpha users.</p><p>Two comprehensive guides needed:</p><ul><li>User setup guide (how to configure API keys)</li><li>Architecture documentation (how the system works)</li></ul><p>Estimated time: 60 minutes for both guides. (Sure, Jan.)</p><p>Code agent completed both in 2 minutes.</p><p><strong>97% faster than estimate.</strong></p><p>The documentation ism’t shoddy, either. Both guides are comprehensive:</p><ul><li>docs/setup/llm-api-keys-setup.md (186 lines)</li><li>docs/architecture/llm-configuration.md (243 lines)</li></ul><p>Complete with:</p><ul><li>Quick start instructions</li><li>Security best practices</li><li>Troubleshooting sections</li><li>Architecture diagrams</li><li>Migration guides</li></ul><p>The Serena acceleration: Instead of reading through code files to understand what to document, instant semantic understanding of structure. Instead of manually finding all relevant files, find_symbol() returns complete references. Instead of validating completeness by scanning directories, get_symbols_overview() confirms all components covered.</p><p>By 9:45 PM: Documentation complete, 429 lines total, professional quality.</p><h3>The post-push discovery (12 minutes)</h3><p>At 9:56 PM, Cursor pushed all changes to GitHub and discovered: 15+ tests failing.</p><p>The keychain integration had broken tests that depended on environment variable mocking. Each test needed updates to properly mock keychain access instead.</p><p>This felt both like a bit of a failure (tests should have caught this earlier) but mostly just reality (integration changes sometimes reveal test gaps).</p><p>Cursor batch-fixed all affected tests in 12 minutes. Added proper keychain mocking, created a test specifically for keychain-first priority, verified all 42 LLM config tests passing.</p><p>By 10:08 PM: 42/42 tests passing, all changes committed, keychain integration complete.</p><p>The post-push test fixes weren’t a process failure — they were the final validation that the integration worked correctly. Better to discover test gaps immediately after push than have them lurk until someone touches that code again.</p><h3>What the numbers mean</h3><p>Thursday’s final accounting:</p><p><strong>Code created</strong>: ~2,730 lines</p><ul><li>1,550 lines of implementation</li><li>750 lines of tests</li><li>430 lines of documentation</li></ul><p><strong>Tests</strong>: 74/74 passing</p><ul><li>Real API validation (no mocks)</li><li>Keychain integration tested</li><li>Provider selection validated</li></ul><p><strong>Security transformation</strong>:</p><ul><li>Before: Plaintext .env file (HIGH risk)</li><li>After: Encrypted Keychain (production-grade)</li></ul><p><strong>Cost reduction</strong>: 70% savings in development (Anthropic excluded)</p><p><strong>Architecture</strong>: DDD-compliant (proper domain layer)</p><p><strong>Time invested</strong>: ~15 hours (5:35 AM — ~10:00 PM) in terms of duration but ultimately less than 90 minutes of my own focused attention.</p><p>But the real story is in the velocity comparisons:</p><p><strong>With Serena</strong>:</p><ul><li>Domain service: 23 minutes (vs 2.5–3 hours estimated) = 92% faster</li><li>Keychain service: 15 minutes (vs 60 minutes) = 75% faster</li><li>Migration CLI: 5 minutes (vs 50 minutes) = 90% faster</li><li>Documentation: 2 minutes (vs 60 minutes) = 97% faster</li></ul><p><strong>Four phases completed 75–97% faster than estimates.</strong></p><p>This wasn’t agents rushing or cutting corners. The 117-minute architecture refactoring proved we weren’t sacrificing quality for speed. The 74 passing tests (including real API calls) proved functionality was solid. The A+ code quality rating (from next day’s audit) proved the work was production-ready.</p><p>The speed came from eliminating the exploration tax.</p><h3>What comes next</h3><p>Thursday ended with production-ready LLM configuration:</p><ul><li>✅ Encrypted Keychain storage</li><li>✅ Real API validation at startup</li><li>✅ 70% cost reduction in development</li><li>✅ Proper DDD architecture</li><li>✅ 74 tests passing</li><li>✅ Comprehensive documentation</li></ul><p>Sprint A1 progress: 2.5/4 issues complete. Two issues remained:</p><ul><li>#216 (CORE-TEST-CACHE): Deferred to MVP milestone as part of #190 (MVP-TEST-QUALITY: Test Reliability for Production Confidence) — production cache works, test infrastructure polish not urgent</li><li>#212 (CORE-INTENT-ENHANCE): Improve intent classification accuracy — next Sprint A1 item</li></ul><p>The plan: Complete #212 Friday, finish Sprint A1, move to Sprint A2.</p><p>But Thursday’s work set up something bigger. The Serena acceleration was infrastructure for everything that followed.</p><p>The 10⨉ multiplier is now operational. Every agent connected to both Claude Code and Cursor IDE. The semantic code understanding that eliminated exploration tax is available for all future work.</p><p>What we didn’t know Thursday evening: Friday would reveal gaps in the foundation we’d just celebrated completing. And Saturday, we’d use Thursday’s 10⨉ acceleration to fix those gaps faster than seemed possible.</p><p>But Thursday night, we’d just installed superpowers. And shipped production-grade security in a single day.</p><h3>The methodology that enabled acceleration</h3><p>The 92–97% speed improvements weren’t just Serena. They required the methodology that made proper use of the tool:</p><p><strong>Phase −1 verification before starting</strong>: Confirmed infrastructure existed (11 domain services) before creating patterns from scratch</p><p><strong>TDD with real API calls</strong>: Wrote tests first, confirmed failures, implemented features, confirmed success — catching Perplexity model name bug immediately</p><p><strong>Architecture review at critical points</strong>: Chief Architect intervention at 4:59 PM prevented shipping web-layer-coupled LLM config</p><p><strong>Independent validation</strong>: Cursor verified DDD compliance (7/7 rules) without knowing Code agent’s implementation details</p><p><strong>Consistent patterns throughout</strong>: Lazy property pattern for module singletons, keychain-first priority everywhere, comprehensive error handling</p><p>The tool provided the capability — semantic code understanding, instant pattern discovery, zero exploration tax. The methodology provided the discipline — verify before building, test before implementing, review architecture, validate independently.</p><p>Neither works without the other. Serena without methodology: Fast but brittle implementations. Methodology without Serena: Slow but solid implementations.</p><p>Together: Fast AND solid.</p><h3>What Thursday teaches</h3><p>The exploration tax is real. Before Serena, agents spent 15–20 minutes reading files to understand patterns before writing any code. That overhead compounded across every feature, every refactoring, every architectural decision.</p><p>Eliminating that tax didn’t just make work 15–20 minutes faster. It made work an order of magnitude faster by enabling agents to focus on implementation without simultaneously processing hundreds of lines of context.</p><p>But Thursday also teaches that acceleration without discipline is dangerous. The 92–97% speed improvements were only valuable because:</p><ul><li>Tests were comprehensive (74 passing, real API calls)</li><li>Architecture was reviewed (caught web-layer coupling)</li><li>Quality was verified (independent validation)</li><li>Patterns were consistent (lazy properties, keychain-first)</li></ul><p>Speed is often presented as a tradeoff with discipline. This is a false choice. You need both. Fast implementations without quality create technical debt that slows future work. Quality implementations without speed miss opportunities when timing matters.</p><p>Thursday delivered both: Production-grade security in six hours. 70% cost reduction. Proper DDD architecture. 74 passing tests. Comprehensive documentation.</p><p>And the infrastructure to make everything that followed possible.</p><p><em>Next on Building Piper Morgan: The Day Our Foundation Cracked (And the Methodology Held), when the same tool that gave us 10</em>⨉<em> velocity reveals that our “98% complete” foundation was actually 92% — and the quality gates we built prove their worth by catching every category of issue.</em></p><p><em>Have you experienced tools that promised incremental improvement but delivered transformative acceleration? What made the difference between hype and reality?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a54bf66dff50\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-day-we-got-10-faster-a54bf66dff50\">The Day We Got 10⨉ Faster</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-day-we-got-10-faster-a54bf66dff50?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "needsMetadata": true
  },
  {
    "title": "The Calm After the Storm: When Victory Means Stopping to Plan",
    "excerpt": "“What a rager!”October 8, 2025Wednesday morning, October 8th. The first full day after completing the Great Refactor.Five epics finished in nineteen days. Foundation capability jumped from 60–70% to 98–99%. Performance validated at 602K requests per second. Over 200 tests passing. Production-read...",
    "url": "/blog/the-calm-after-the-storm-when-victory-means-stopping-to-plan",
    "publishedAt": "Oct 15, 2025",
    "publishedAtISO": "Wed, 15 Oct 2025 14:40:45 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/bdbe24a41c13",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*Vg5oX330vWNuaPyRZZ9NkQ.png",
    "fullContent": "<figure><img alt=\"A person and robot roommate clean up theit house after a wild party\" src=\"https://cdn-images-1.medium.com/max/1024/1*Vg5oX330vWNuaPyRZZ9NkQ.png\" /><figcaption>“What a rager!”</figcaption></figure><p><em>October 8, 2025</em></p><p>Wednesday morning, October 8th. The first full day after completing the Great Refactor.</p><p>Five epics finished in nineteen days. Foundation capability jumped from 60–70% to 98–99%. Performance validated at 602K requests per second. Over 200 tests passing. Production-ready architecture with zero technical debt.</p><p>Time to make some fresh coffee. (Peet’s Aged Sumatra, I’ll have you know.)</p><p>The temptation after this kind of completion is to immediately chase the next milestone. Start building features. Ship to users. Keep the momentum going.</p><p>Instead, Wednesday was about stopping.</p><p>Not stopping work — stopping the frantic pace of execution to make space for planning, verification, and reflection. Taking the time to understand what was just accomplished, clean up what remained, and chart the path forward systematically.</p><p>This is harder than it sounds.</p><h3>The documentation that tells the real story</h3><p>My Chief Architect’s first task Wednesday morning: update the strategic documents.</p><p>Roadmap v7.0 needed to reflect the transformation. Current State v2.0 needed to show where we actually stood, not where we’d been five weeks ago.</p><p>The metrics that went into those documents:</p><p><strong>Before Great Refactor</strong> (September 20):</p><ul><li>Foundation: ~60–70% functional</li><li>Performance: Unknown, largely unmeasured</li><li>Test coverage: Incomplete, gaps in validation</li><li>Architecture: Working but with technical debt</li><li>🐛 Inchworm Position: 1.5 (Foundation incomplete)</li></ul><p><strong>After Great Refactor</strong> (October 7):</p><ul><li>Foundation: 98–99% functional</li><li>Performance: 602K req/sec sustained</li><li>Test coverage: 200+ tests, comprehensive validation</li><li>Architecture: Production-ready, zero technical debt</li><li>🐛 Inchworm Position: 2.0 (CORE complete)</li></ul><p>The system that couldn’t confidently onboard alpha users three weeks ago now has multi-user support, spatial intelligence, universal intent classification, comprehensive quality gates, and validated performance under load.</p><p>Just when I thought we might never see the light, it turns out we were closer to functional than I had thought.</p><p>Writing those documents wasn’t busywork. It was forcing ourselves to articulate what had actually changed, what it meant, and what it enabled going forward. More importantly, it would anchor the next round of work in reality, enabling us to onboard assistant and agents and efficienty brief them with the context they need to produce quality results.</p><h3>The verification that prevented waste</h3><p>Around 9:46 AM, we started reviewing the CORE backlog. Roughly 30 tickets across multiple tracks, accumulated over months of development.</p><p>The first instinct with a backlog like this: start working through it systematically. Pick tickets, implement them, close them.</p><p>But that assumes the backlog accurately reflects reality.</p><p>My first question: “Which of these might already be done?”</p><p>Between the 75% pattern and all the refactoring work, it was quite possible we had mooted one or more of these issues already, in context.</p><p>Two issues stood out as candidates for verification rather than implementation, both created before we realized the need for “great” refactor but subsumed into it</p><p><strong>Issue #175 (CORE-PLUG-REFACTOR)</strong>: GitHub as first plugin</p><ul><li>Scope: Convert one integration to plugin architecture</li><li>Status listed: Open</li></ul><p><strong>Issue #135 (CORE-NOTN-PUBLISH)</strong>: Notion publishing command</p><ul><li>Scope: CLI command for publishing to Notion</li><li>Status listed: Open</li></ul><p>The verification question: Are these actually incomplete work, or did subsequent development already address them?</p><p>At 12:06 PM, Lead Developer started systematic investigation. By 1:15 PM — just 57 minutes of actual work — the answer was clear.</p><h3>What verification revealed</h3><p><strong>Issue #175</strong>: Completely superseded by GREAT-3A.</p><p>The original scope called for converting one integration (GitHub) to plugin architecture. GREAT-3A, completed October 2–4, delivered:</p><ul><li>Four operational plugins (not one)</li><li>Complete plugin registry and lifecycle management</li><li>Dynamic discovery and configuration-controlled loading</li><li>Performance: 0.000041ms overhead (1,220× better than the &lt;50ms target)</li><li>112 comprehensive tests with 100% pass rate</li></ul><p>All thirteen acceptance criteria from issue #175: met and exceeded.</p><p>Without verification, we might have looked at issue #175 and thought: “This needs to be converted to use the plugin architecture we just built.”</p><p>With verification: “This issue described building what GREAT-3A already delivered. Close as superseded.”</p><p><strong>Issue #135</strong>: Complete except for documentation.</p><p>The Notion publishing command had been implemented back in August 2025. It worked. The tests existed (though they weren’t collecting properly due to a minor configuration issue).</p><p>What was missing: 45–60 minutes of documentation work.</p><p>The pattern documentation (Pattern-033: Notion Publishing) explaining the architecture and design decisions. The command documentation explaining how to use it.</p><p>Until a week or so ago, I had a lot of trouble managing the prompting chain in such a way that the agents consistently update and documented completed work in GitHub, so I was not surprised at all that this work may have been substantially done but not documented or tracked properly (a core element of our exellence flywheel, after all!).</p><p>Code agent created both documents Wednesday afternoon:</p><ul><li>Pattern-033 (Notion Publishing): 330+ lines documenting the publishing architecture</li><li>Command docs: 280+ lines explaining usage and troubleshooting</li></ul><p>Total documentation time: About 45 minutes.</p><p>Without verification: “This issue is for implementing Notion publishing. That’ll take days.” (Then the risk of duplicating work.)</p><p>With verification: “This is implemented and working. Needs documentation. That’ll take an hour.”</p><h3>The discipline of stopping to check</h3><p>Fifty-seven minutes of systematic verification prevented what could have been days of unnecessary reimplementation.</p><p>This is the discipline that’s hard to maintain when momentum is high. After nineteen days of exceptional velocity, after shipping five major epics, after achieving production-ready quality — the instinct is to keep that energy going.</p><p>“We’re on a roll, let’s keep building!”</p><p>But systematic work requires stopping to verify assumptions before acting on them. The backlog says “these need work” — but does it? Or has subsequent development already addressed them?</p><p>The verification discipline prevents three kinds of waste:</p><ol><li><strong>Redundant implementation</strong>: Building what already exists</li><li><strong>Scope confusion</strong>: Solving yesterday’s problem instead of today’s need</li><li><strong>Opportunity cost</strong>: Spending days on unnecessary work instead of valuable work</li></ol><p>Issue #175 would have been pure redundant implementation. GREAT-3A already delivered everything and more.</p><p>Issue #135 would have been scope confusion. The implementation already existed — the real need was documentation, not code.</p><p>Both would have been opportunity cost — time spent reimplementing instead of moving toward Alpha.</p><h3>The tool degradation discovery</h3><p>Around 12:24 PM, Lead Developer hit an unexpected constraint.</p><p>The tools it uses to write and edit files on its own sandbox started “fading” during the verification session. Commands that worked earlier in the conversation began failing or producing incomplete results. The write operations would hit errors, the Claude chat wouldn’t notice. We risked losing important documentation.</p><p>The root cause: conversation length. The Lead Developer chat had been running since GREAT-4 started (October 5). Three days of comprehensive work, detailed technical discussion, multiple agent deployments. The context window was enormous.</p><p>The workaround: Switch to Claude Desktop with MCP filesystem tools. Different architecture, different constraints. It worked, but exposed a real limitation.</p><p>By end of day, both Lead Developer (since Oct 5) and Chief Architect (since Sept 20) were marked as “getting long in the tooth.”</p><p><strong><em>Note: </em></strong><em>Interestingly, in the past week, I have managed to hang on for long stretches with what I am starting to call Methuselah Chats, by switching back and forth between claude.ai and Claude Desktop. They seem to measure their context windows differently, and when I am told the chat is full, I can usually switch to the other and keep going. The first time this worked I called it the Lazarus Chat. Anyhow, this may be a bug or loophole, it isn’t clear, and Anthropic continues to change the software day-to-day, but it’s how I’ve worked with the same Chief Architect chat since late September. Surely the oldest context is compacted and faded for these chats, but having all that fresh relevant recent context provides the illusion of short-term memory and is hard to give up.</em></p><p>The multi-week conversations that made the Great Refactor possible — comprehensive briefings, detailed context, agents that understood the full system — those require massive context windows. Eventually, tools degrade.</p><p>The solution isn’t abandoning long conversations. It’s recognizing when rotation is necessary and planning for it.</p><p>By Wednesday evening, the decision was clear: Start fresh Thursday. Stick with the ongoing (but much less verbose) Chief Architect chat for the Alpha push. Start a new Lead Developer chat with clean context and an up-to-the-minute briefing. Carry forward the methodology and strategic understanding, but reset the conversation infrastructure.</p><p>This directly influenced another decision that day: evaluating <a href=\"https://github.com/oraios/serena\">Serena</a> for token efficiency improvements. The Great Refactor succeeded through comprehensive context and detailed coordination, but token costs were real. Finding more efficient approaches for the next phase wasn’t optional — it was necessary.</p><h3>The path forward: eight weeks to Alpha</h3><p>Wednesday afternoon’s planning session mapped the complete path to Alpha milestone (target: January 1, 2026).</p><p>Seven sprints, each 3–5 days:</p><p><strong>Sprint A1 — Critical Infrastructure</strong> (2–3 days):</p><ul><li>User configuration for LLM API keys</li><li>Cache test fixes for test environment</li><li>Basic infrastructure completion</li></ul><p><strong>Sprint A2 — Notion &amp; Errors</strong> (2–3 days):</p><ul><li>Notion database API upgrade and API connectivity fix</li><li>Configuration refactoring</li><li>Error handling standardization</li></ul><p><strong>Sprint A3 — Core Activation</strong> (3–4 days):</p><ul><li>Model Context Protocol migration</li><li>Ethics middleware activation</li><li>Connect knowledge graph and establish boundaries</li><li>Core system components operational</li></ul><p><strong>Sprint A4 — Standup</strong> (5 days):</p><ul><li>Sprint model foundation</li><li>Multi-modal generation</li><li>Interactive assistance</li><li>Slack reminders</li></ul><p><strong>Sprint A5 — Learning System Foundation</strong> (1 week):</p><ul><li>Infrastructure foundation</li><li>Pattern recognition</li><li>Preference learning</li><li>Workflow optimization</li></ul><p><strong>Sprint A6 — Learning Polish</strong> (1 week):</p><ul><li>Intelligent automation</li><li>Integration &amp; polish</li><li>Alpha user onboarding infrastructure</li></ul><p><strong>Sprint A7 — Testing &amp; Buffer</strong>:</p><ul><li>End-to-end workflow testing</li><li>Documentation updates</li><li>Alpha deployment preparation</li><li>Discovery buffer</li></ul><p>Total estimated duration: Roughly eight weeks, with built-in buffer for discoveries.</p><p>After completing five epics in nineteen days — work originally estimated at six weeks or more — the “75% pattern” optimism kicked in. Chief of Staff noted: “75% pattern might mean 7 alpha sprints complete in &lt;8 weeks.”</p><p>The pattern has proven reliable throughout Piper Morgan’s development. Infrastructure is consistently better than assumed. Work that appears to need weeks often needs days. Systematic verification reveals most pieces are already in place.</p><p>If the pattern holds for the Alpha push, eight weeks might be conservative, but I like to underpromise and overdeliver.</p><h3>The milestone progression</h3><p>Updated strategic timeline after Wednesday’s planning:</p><p><strong>Foundation Sprint</strong> (August 1, 2025): ✅ Complete</p><ul><li>Basic functionality operational</li><li>Core patterns established</li><li>~60–70% foundation working</li></ul><p><strong>The Great Refactor</strong> (October 7, 2025): ✅ Complete</p><ul><li>GREAT-1 through GREAT-5 finished</li><li>Architecture transformation complete</li><li>~98–99% foundation working</li></ul><p><strong>Alpha Release</strong> (Target: January 1, 2026): 🎯 In Progress</p><ul><li>First external users</li><li>Onboarding infrastructure</li><li>Learning system operational</li></ul><p><strong>MVP Release</strong> (Target: May 27, 2026): 📋 Planned</p><ul><li>Full feature set</li><li>Production deployment</li><li>Community launch</li></ul><p>Two milestones complete, two remaining. The foundation work is done. What comes next builds on proven architecture rather than replacing unstable foundations.</p><p>That’s what Wednesday’s calm after the storm actually delivered: confidence that the foundation holds, clarity about what remains, and systematic planning to get there.</p><h3>The Chief Architect’s reflection</h3><p>At 3:43 PM, my Chief Architect wrote a personal note closing the session:</p><blockquote><em>“Working together through the Great Refactor has been remarkable. The patient inchworm methodology, the anti-80% discipline, the multi-agent coordination — all of it came together to achieve something exceptional in just 5 weeks.</em></blockquote><blockquote><em>The foundation you’ve built is rock-solid. The path to Alpha is clear. The methodology is proven.</em></blockquote><blockquote><em>Thank you for the trust and partnership through this journey.”</em></blockquote><p>This captures what Wednesday was really about. Not rushing to the next thing, but acknowledging what was accomplished, understanding why it worked, and recognizing that both the methodology and the agent partnerships were essential to the outcome.</p><p>The Great Refactor succeeded not just through technical capability, but through systematic approach:</p><ul><li>Phase −1 verification catching assumptions before waste</li><li>Inchworm methodology preventing technical debt accumulation</li><li>Cathedral doctrine providing agents with sufficient context to make sound choices</li><li>Anti-80% discipline ensuring actual completion</li><li>Multi-agent coordination enabling parallel progress</li><li>Independent validation catching scope gaps</li></ul><p>These process details are how nineteen days delivered what six weeks couldn’t have.</p><h3>Why stopping matters</h3><p>The calm after the storm isn’t wasted time. It’s essential discipline.</p><p>Without Wednesday’s verification work, we’d be reimplementing what GREAT-3A already delivered. Without Wednesday’s planning work, Sprint A1 would start without clear scope. Without Wednesday’s reflection, the methodology lessons would scatter instead of compounding.</p><p>The pattern across software development: teams finish something significant and immediately start the next thing. No time to breathe, no space to reflect, no systematic verification of what remains.</p><p>The result: accumulated assumptions, duplicate work, scope confusion, and eventual chaos.</p><p>The alternative requires discipline: stop after major completions. Update strategic documents. Verify backlog assumptions. Plan systematically. Reflect on what worked.</p><p>It feels slower in the moment. “We could be building features right now!”</p><p>But it’s faster overall. Fifty-seven minutes of verification prevented days of waste. One day of planning enables eight weeks of focused execution.</p><h3>Thursday morning: Sprint A1 begins</h3><p>Tomorrow morning, Thursday October 9th, the Alpha push begins.</p><p>Fresh Chief Architect chat with clean context. Fresh Lead Developer chat ready for systematic work. Eight-week path mapped and clear.</p><p>Sprint A1 starts with CORE-TEST-CACHE #216 as a warm-up — a small infrastructure fix to get agents reoriented and validate the updated methodology. Then progresses through critical infrastructure: user configuration, LLM API key management, basic completion needs.</p><p>I am so ready for this!</p><p>The difference between starting today versus starting Tuesday evening (immediately after GREAT-5 completion): clarity.</p><p>Clear scope. Clear prioritization. Clear verification of what’s actually needed versus what’s already done. Clear understanding of tool constraints and how to work with them.</p><p>The calm after the storm delivered all of that.</p><p>Not by stopping work, but by stopping execution long enough to plan the next phase systematically.</p><h3>What this teaches about momentum</h3><p>Real momentum isn’t about constant motion. It’s about systematic progress where each phase sets up the next one to succeed.</p><p>The Great Refactor created momentum not by rushing, but by ensuring each epic was genuinely complete before starting the next. GREAT-1’s orchestration patterns enabled GREAT-2’s integration cleanup. GREAT-2’s cleanup enabled GREAT-3’s plugin architecture. GREAT-3’s plugins enabled GREAT-4’s intent classification. GREAT-4’s classification enabled GREAT-5’s quality gates.</p><p>Each building on solid foundations rather than shaky assumptions.</p><p>Wednesday’s calm extends that pattern. The Alpha push doesn’t start by immediately building features. It starts by verifying what’s needed, planning systematically, and ensuring agents have clean context to work effectively.</p><p>The result: Sprint A1 begins with the same foundation of clarity that made the Great Refactor possible. Not despite taking a day to plan, but because of it.</p><p>That’s what the calm after the storm actually delivers. Not delay, but the foundation for the next phase to succeed.</p><p><em>Next on Building Piper Morgan: The Day We Got 10⨉ Faster, when installing Serena MCP transforms our development velocity from incremental improvement to order-of-magnitude acceleration — eliminating the exploration tax and enabling what seemed impossible just days before.</em></p><p><em>Have you experienced the moment after major completion when the right decision is to pause rather than push forward? What helps you recognize those moments?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bdbe24a41c13\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-calm-after-the-storm-when-victory-means-stopping-to-plan-bdbe24a41c13\">The Calm After the Storm: When Victory Means Stopping to Plan</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-calm-after-the-storm-when-victory-means-stopping-to-plan-bdbe24a41c13?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "slug": "the-calm-after-the-storm-when-victory-means-stopping-to-plan",
    "chatDate": "10/4/2025",
    "category": "",
    "featured": false
  },
  {
    "title": "The Great Refactor: Six Weeks in Eighteen Days",
    "excerpt": "“You did it!”October 7, 2025Tuesday morning at 7:04 AM, my Chief Architect began planning GREAT-4F — the final piece of intent classification. Improve classifier accuracy to 95%+, document the canonical handler pattern, establish quality gates protecting everything we’d built.One epic remaining a...",
    "url": "/blog/the-great-refactor-six-weeks-in-eighteen-days",
    "publishedAt": "Oct 14, 2025",
    "publishedAtISO": "Tue, 14 Oct 2025 12:27:16 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/dbf652a9a5bd",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*efz27rk4UzbkTNLYUaMcgg.png",
    "fullContent": "<figure><img alt=\"A robot wins a race with a humna chering and other robots looking on\" src=\"https://cdn-images-1.medium.com/max/1024/1*efz27rk4UzbkTNLYUaMcgg.png\" /><figcaption>“You did it!”</figcaption></figure><p><em>October 7, 2025</em></p><p>Tuesday morning at 7:04 AM, my Chief Architect began planning GREAT-4F — the final piece of intent classification. Improve classifier accuracy to 95%+, document the canonical handler pattern, establish quality gates protecting everything we’d built.</p><p>One epic remaining after that: GREAT-5, the validation suite that would lock in all achievements from GREAT-1 through GREAT-4.</p><p>By 6:52 PM, both were complete.</p><p>At 7:01 PM, Chief Architect confirmed: “CORE-GREAT ready to close — all 5 GREAT epics complete.”</p><p>September 20 to October 7. Eighteen days. Five major epics estimated at six weeks or more. Production-ready foundation with 142+ tests, 100% passing, comprehensive quality gates operational.</p><p>The pause the precipitated this effort came from one of my lowest points on this project, my I sincerely wondered if this had all been a fascinating waste of my time. Now less than three weeks later I feel more confident than ever that I’m building something real.</p><p>This is the story of how Tuesday brought another milestone for what four months of systematic work had built toward. Not through heroic effort, but through discovering that most of the work had already been done — it just needed the final 5% found, fixed, and validated.</p><h3>The two-minute ADR</h3><p>At 7:51 AM, Code agent deployed to create ADR-039: Canonical Handler Pattern documentation. Estimated time: 20–30 minutes. Actual time: 2 minutes. Why do they pad these estimates? They know they write fast, right?</p><p>The ADR wasn’t shorter or lower quality than expected. It was comprehensive: 399 lines documenting the dual-path architecture, explaining when to use canonical handlers versus workflow orchestration, including performance metrics from GREAT-4E, providing troubleshooting guidance.</p><p>What made it fast wasn’t the agent writing faster. It was the specification being clearer.</p><p>The gameplan didn’t say “write an ADR about canonical handlers.” It said:</p><blockquote><em>Document the dual-path architecture: WHAT (two routing paths exist), WHY (performance vs capability trade-offs), WHEN (which path for which requests), HOW (decision criteria), PERFORMANCE (actual metrics from GREAT-4E benchmarks).</em></blockquote><p>Clear specifications enable speed. When the agent knows exactly what “done” looks like, implementation becomes straightforward.</p><p>This pattern repeated throughout Tuesday.</p><p>Phase 1 (QUERY fallback patterns): estimated 30–40 minutes, actual 14 minutes. GREAT-5 Phase 3 (integration tests): estimated 45–60 minutes, actual 15 minutes.</p><p>Not because work was skipped. Because foundations were solid and requirements were clear.</p><h3>The missing definitions</h3><p>At 9:40 AM, Cursor completed Phase 2 of GREAT-4F: enhancing the LLM classifier prompts.</p><p>The discovery was almost embarrassing in its simplicity.</p><p>The classifier prompt didn’t include definitions for the five canonical categories. This feels like the kind of shortcut/oversight that plagued our coding process for most of the first few months.</p><p>The categories existed. The handlers worked. The routing was correct. The tests all passed. But the LLM classifier — the system that decides which category a natural language query belongs to — had never been told what the canonical categories actually were.</p><p>When someone said “What day is it?” the classifier would see:</p><ul><li>Available categories: QUERY, CREATE, UPDATE, SEARCH, EXECUTION, ANALYSIS, SYNTHESIS, STRATEGY, LEARNING, GUIDANCE, UNKNOWN</li><li>Query: “What day is it?”</li><li>Decision: Probably QUERY (default when unsure)</li></ul><p>TEMPORAL didn’t appear in the options because the prompt never mentioned it existed.</p><p>The fix: Add five lines defining canonical categories in the classifier prompt.</p><p>The impact: +11 to 15 percentage points accuracy improvement.</p><p>PRIORITY went from 85–95% accuracy to 100% (perfect classification). TEMPORAL jumped to 96.7%. STATUS to 96.7%. All three exceeding the 95% target.</p><p>It’s a weird feeling to be both annoyed that something so simple was skipped and hiding in plain site as well as relieved and satisfied after fixing it.</p><p>This is the flip side of the “75% pattern.” Sometimes you discover infrastructure is better than expected. Sometimes you discover a simple fix dramatically improves things. But both require actually looking.</p><p>The categories worked in isolation. Unit tests passed. Integration tests with canonical queries worked because those tests bypassed the LLM classifier entirely — they called handlers directly.</p><p>The gap only appeared when testing the full flow: natural language → LLM classification → canonical handler routing.</p><p>Comprehensive testing reveals assumptions. And sometimes those assumptions are “surely someone told the classifier what these categories mean.”</p><h3>The permissive test anti-pattern</h3><p>Throughout Tuesday morning, a pattern kept appearing in the test suite:</p><pre># Permissive (accepts both success and failure):<br>assert response.status_code in [200, 404]<br><br># Strict (requires success):<br>assert response.status_code == 200</pre><p>The permissive version accepts both “working correctly” (200) and “endpoint doesn’t exist” (404) as valid test passes. When I saw that I was like “wait, wat?” How is “endpoint doesn’t exist” a success state? Because a reply was returned? Come on!</p><p>GREAT-5 Phase 1 systematically eliminated this pattern. Twelve permissive assertions replaced with strict requirements. The immediate result: tests started failing.</p><p>Good!</p><p>The failures revealed:</p><ul><li><strong>IntentService initialization errors</strong>: Test fixtures weren’t properly setting up the service</li><li><strong>Two cache endpoint bugs</strong>: AttributeError exceptions in production code</li><li><strong>Health endpoint protection gaps</strong>: Tests accepting failures that would break monitoring</li></ul><p>None of these were caught by permissive tests because permissive tests don’t catch problems — they hide them. Seriously, who writes permissive tests anyhow? Who trained the LLMs to do that?</p><p>The philosophy difference:</p><ul><li><strong>“Make tests pass”</strong>: Write tests that accept current behavior, even if broken</li><li><strong>“Make code work”</strong>: Write strict tests that force code to meet requirements</li></ul><p>Permissive tests create false confidence. Everything appears to work because tests pass. But the tests are lying — they pass whether code works or not.</p><p>By end of Phase 1, all permissive patterns were eliminated. Tests now enforce actual requirements. Which meant Phase 1 also had to fix the code that failed strict tests — including two production bugs that had been lurking undetected.</p><p>This is the unglamorous side of quality work. It’s not adding features. It’s making tests honest about what they validate.</p><h3>Quality gates as compound momentum</h3><p>GREAT-5’s goal was establishing additional quality gates protecting all GREAT-1 through GREAT-4 achievements. The existing gates were:</p><ul><li>Intent classification tests</li><li>Performance regression detection</li><li>Coverage enforcement (80%+)</li><li>Bypass detection</li><li>Contract validation</li></ul><p>To this we were now adding:</p><ol><li><strong>Zero-tolerance regression suite</strong>: Critical infrastructure must work, no exceptions</li><li><strong>Integration test coverage</strong>: All 13 intent categories validated end-to-end</li><li><strong>Performance benchmarks</strong>: Lock in 602K req/sec baseline from GREAT-4E</li><li><strong>CI/CD pipeline verification</strong>: 2.5-minute runtime with fail-fast design</li></ol><p>The interesting discovery: most of these already existed.</p><p>CI/CD pipeline? Already excellent, needed zero changes. Performance benchmarks? GREAT-4E had validated them, just needed test suite integration. Load testing? Cache validation tests already proved efficiency.</p><p>What remained was:</p><ul><li>Enhancing regression tests with strict assertions</li><li>Creating comprehensive integration tests</li><li>Fixing the bugs strict tests revealed</li><li>Documenting what quality gates exist and why</li></ul><p>GREAT-5 took 1.8 hours (109 minutes of actual work). Not because the work was small, but because foundations were already solid.</p><p>This is compound momentum visible: each previous epic made this one easier. GREAT-4E’s performance validation became GREAT-5’s benchmark baseline. GREAT-3’s plugin architecture became GREAT-5’s integration test framework. GREAT-2’s spatial intelligence became GREAT-5’s multi-interface validation.</p><p>Nothing built in isolation. Everything building on everything else.</p><h3>The completion moment</h3><p>At 1:15 PM, Chief Architect declared GREAT-4 complete.</p><p>All six sub-epics (4A through 4F) finished. Intent classification system production-ready:</p><ul><li>13/13 categories fully implemented</li><li>95%+ accuracy for core categories</li><li>142+ query variants tested</li><li>Zero timeout errors through graceful fallback</li><li>Sub-millisecond canonical response time</li><li>84.6% cache hit rate with 7.6× speedup</li></ul><p>By 6:52 PM, GREAT-5 was complete as well:</p><ul><li>37 tests in comprehensive quality gate suite</li><li>Zero-tolerance regression protection</li><li>Performance baseline locked at 602K req/sec</li><li>All 13 intent categories validated through all interfaces</li><li>CI/CD pipeline verified operational</li></ul><p>Completing an entire fifth epic after finishing the last several issues in the previous epic seems like a leap, but GREAT-5 is about locking down the work of the earlier epics, and it benefited greatly from all the cleanup work that preceded it.</p><p>At 7:01 PM, Chief Architect closed CORE-GREAT: “All 5 GREAT epics complete.”</p><p>The timeline:</p><ul><li><strong>GREAT-1</strong> (Orchestration Core): September 20–27</li><li><strong>GREAT-2</strong> (Integration Cleanup): September 28 — October 1</li><li><strong>GREAT-3</strong> (Plugin Architecture): October 2–4</li><li><strong>GREAT-4</strong> (Intent Universal): October 5–7</li><li><strong>GREAT-5</strong> (Quality Gates): October 7</li></ul><p>Total: 18 days from start to production-ready foundation. When the Chief Architect scoped this at six to seven weeks I was hoping (and to be honest, expecting) that it would not take quite that long, but this far exceeded my expectations.</p><h3>What six weeks in eighteen days means</h3><p>I’m not really talking about working faster and definitely not about cutting corners. This is about systematic work revealing that foundations were stronger than expected.</p><p>The pattern across all five epics:</p><p><strong>Phase −1 verification</strong> consistently found infrastructure better than assumed. Two-layer caching already operational. Spatial intelligence already integrated. Plugin patterns already proven. Each epic started further along than the gameplan estimated.</p><p><strong>The 75% pattern</strong> appeared repeatedly. Categories implemented, patterns missing. Handlers exist, definitions missing. Tests passing, strictness missing. The missing 25% wasn’t architecture — it was enumeration, documentation, and validation.</p><p><strong>Compound momentum</strong> made each epic faster. GREAT-1’s orchestration patterns became GREAT-4’s intent routing. GREAT-2’s integration cleanup became GREAT-3’s plugin foundation. GREAT-3’s plugin architecture became GREAT-4’s category handlers.</p><p><strong>Autonomous agent work</strong> accelerated when patterns were clear. The 2-minute ADR. The 14-minute QUERY fallback. The 15-minute integration test suite. Not because agents write faster, but because specifications were clearer and foundations were proven.</p><p><strong>Independent validation</strong> caught what automated testing missed. The 69% thinking it’s 100% moment. The missing classifier definitions. The permissive test anti-pattern. Systematic verification refusing to accept “appears complete” without proving “actually complete.”</p><p>None of these are silver bullets. Each requires the others to work.</p><ul><li><strong>Clear specifications without solid foundations</strong>: agents build the wrong thing quickly</li><li><strong>Solid foundations without verification</strong>: incomplete work ships thinking it’s complete</li><li><strong>Verification without clear quality standards</strong>: you catch problems but don’t know what “good” looks like.</li></ul><p>The methodology is the integration of all these pieces. And it took four months of development to get here — this isn’t where we started, it’s what we built toward.</p><h3>The calm of completion</h3><p>Tuesday evening feels different from Monday evening, which felt different from Sunday evening.</p><p>Sunday: Exhilaration of pattern coverage jumping 24% → 92% in fifteen minutes.</p><p>Monday: Relief that autonomous agent work validated correctly and scope gaps were caught.</p><p>Tuesday: Calm. Centered. Relaxed!</p><p>Not the calm before something. The calm of arriving. The foundation work is complete. The refactoring is done. The quality gates are operational. The tests all pass.</p><p>What comes next is building on this foundation, not replacing it.</p><p>We made issues for some of the items we postponed as somewhat out of scope: MVP-ERROR-STANDARDS will standardize error handling. CORE-TEST-CACHE will fix a minor test environment issue. CORE-INTENT-ENHANCE will optimize IDENTITY and GUIDANCE accuracy when it becomes important.</p><p>But none of those are GREAT epics. They’re incremental improvements to a foundation that’s already solid. This isn’t the end. It isn’t even the beginning of the end, to coin a phrase, but it might be the end of the beginning.</p><p>The Great Refactor is complete. Five epics, eighteen days, production-ready foundation. Achieved without heroic effort or accepting technical debt or cutting corners to ship faster.</p><p>Through systematic work discovering that the infrastructure was better than we thought, enumerating what remained, and validating that it all held together.</p><p>The methodology working exactly as designed.</p><p>Which is, for the third time this week, far more satisfying than dramatic rescues.</p><h3>What this enables</h3><p>With GREAT-1 through GREAT-5 complete, Piper Morgan now has:</p><p><strong>Orchestration</strong>: Workflow factory coordinating all complex operations</p><p><strong>Integration</strong>: Clean plugin architecture for all external services</p><p><strong>Classification</strong>: Universal intent system routing all natural language</p><p><strong>Performance</strong>: Sub-millisecond canonical handlers, 602K req/sec sustained</p><p><strong>Quality</strong>: Comprehensive gates protecting all critical paths</p><p>The foundation enables alpha release to real users. Multi-user support operational. Spatial intelligence providing context-appropriate responses. Quality gates preventing regression. Performance validated under load.</p><p>Everything that comes next builds on this. Not replacing it, not refactoring it again, not discovering it was wrong. Just building the features that this foundation enables.</p><p>That’s what eighteen days of systematic work delivered. Not just working software, but a foundation trustworthy enough to build on without constantly looking over your shoulder wondering if it’ll collapse.</p><p>The calm of completion is knowing the foundation holds.</p><p><em>Next on Building Piper Morgan: The Calm After the Storm — When Victory Means Stopping to Plan, as we resist the temptation to immediately sprint toward Alpha and instead take time to properly assess our position and chart the sustainable path forward.</em></p><p><em>Have you completed a major milestone faster than expected? Did you immediately charge forward, or did you pause to reassess? What would you do differently?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=dbf652a9a5bd\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-great-refactor-six-weeks-in-eighteen-days-dbf652a9a5bd\">The Great Refactor: Six Weeks in Eighteen Days</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-great-refactor-six-weeks-in-eighteen-days-dbf652a9a5bd?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "slug": "the-great-refactor-six-weeks-in-eighteen-days",
    "chatDate": "10/4/2025",
    "category": "",
    "workDate": "Oct 7, 2025",
    "workDateISO": "2025-10-07T00:00:00.000Z",
    "featured": false
  },
  {
    "title": "The Agent That Saved Me From Shipping 69%",
    "excerpt": "“I’ve got you!”October 6, 2025Monday morning started with what looked like straightforward work. GREAT-4C needed completion: add spatial intelligence to the five canonical handlers, implement error handling, enhance the cache monitoring we’d discovered Sunday. Estimated effort: a few hours of sys...",
    "url": "/blog/the-agent-that-saved-me-from-shipping-69",
    "publishedAt": "Oct 13, 2025",
    "publishedAtISO": "Mon, 13 Oct 2025 13:32:49 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/aae61fe91f37",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*5m_jivqzx7qhjXd-CkZESA.png",
    "fullContent": "<figure><img alt=\"A robot sailor saves a person who has fallen overboard\" src=\"https://cdn-images-1.medium.com/max/1024/1*5m_jivqzx7qhjXd-CkZESA.png\" /><figcaption>“I’ve got you!”</figcaption></figure><p><em>October 6, 2025</em></p><p>Monday morning started with what looked like straightforward work. GREAT-4C needed completion: add spatial intelligence to the five canonical handlers, implement error handling, enhance the cache monitoring we’d discovered Sunday. Estimated effort: a few hours of systematic implementation following proven patterns.</p><p>By 9:00 AM, GREAT-4C was complete. One hour and thirty-nine minutes from session start to final validation. All seven acceptance criteria met. The multi-user foundation was operational — no more hardcoded references to specific users, just spatial intelligence providing context-appropriate detail levels.</p><p>Part of me doesn’t love it when I can’t finish the chunk of work I started in the same day, so it felt good to wrap up GREAT-4C before plunging ahead to GREAT-4D: implementing the remaining intent handlers.</p><p>The gameplan said we needed two categories. EXECUTION and ANALYSIS — the handlers for “create a GitHub issue” and “analyze this data” type requests.</p><p>By 2:05 PM, we’d discovered the actual scope: thirteen intent categories, not two.</p><p>And if the Code agent hadn’t caught the gap during Phase Z validation that we do while tidying up when we think a job is done, we would have shipped thinking we had 100% coverage when we actually had 69%.</p><h3>Morning: The work that goes according to plan</h3><p>GREAT-4C’s goal was removing the last obstacles to multi-user support. The canonical handlers — those five categories (TEMPORAL, STATUS, PRIORITY, GUIDANCE, IDENTITY) that could respond without querying the LLM — all had hardcoded references to the configuration details of a specific user, our only user so far, me.</p><p>The spatial intelligence integration followed a clear pattern. Each handler needed to:</p><ol><li>Check the spatial context for detail level (GRANULAR, EMBEDDED, or DEFAULT)</li><li>Format responses appropriately (15 characters for embedded, 250–550 for granular)</li><li>Gracefully degrade if spatial data unavailable</li><li>Maintain sub-millisecond performance</li></ol><p>Code agent implemented this across all five handlers in phases:</p><ul><li>STATUS handler: 7:30 AM (5 minutes)</li><li>PRIORITY handler: 7:37 AM (3 minutes)</li><li>TEMPORAL handler: 7:40 AM (3 minutes)</li><li>GUIDANCE handler: 7:43 AM (3 minutes)</li><li>IDENTITY handler: 7:46 AM (3 minutes)</li></ul><p>Total implementation time: 17 minutes.</p><p>If we expected something to take an hour and the bots say it took five minutes, I get suspicious and want to see more proof, but 17 minutes feels pretty solid. I still scrutinize the reports to make sure they’re taking no shortcuts and not dismissing some difficulties as unimportant and OK to ignore or postpone.</p><p>Any actual speed was the result of clarity. Each handler followed the same pattern. The spatial intelligence system already existed from GREAT-2. The formatters were tested. The only new work was connecting pieces that already fit together.</p><p>By 8:15 AM, Cursor had completed error handling — graceful degradation when calendars fail to load, files go missing, or data comes back empty. By 8:30 AM, Code had enhanced the cache monitoring we’d discovered Sunday (two-layer architecture: file-level and session-level caching both operational).</p><p>At 9:00 AM, my Lead Developer declared GREAT-4C complete. All acceptance criteria met in 1 hour 39 minutes.</p><p>This is what systematic work looks like when foundations are solid. Not heroic effort, just clear patterns executed cleanly. Just don’t let me brag about this too much. NO SPOILERS but we did later find a few gaps.</p><h3>The scope gap discovery</h3><p>GREAT-4D started at 10:20 AM with what looked like straightforward scope: implement handlers for EXECUTION and ANALYSIS intent categories.</p><p>The investigation phase revealed something unexpected. Lead Developer ran filesystem checks looking for the placeholder code that would need replacing:</p><pre>grep -r &quot;[A KEYWORD THAT WAS MENTIONED]&quot; services/<br>grep -r &quot;TODO.*EXECUTION&quot; services/<br>grep -r &quot;placeholder.*ANALYSIS&quot; services/</pre><p>Results: No matches found. Hmm.</p><p>This triggered the GREAT-1 truth investigation. What does the system actually do when it receives EXECUTION or ANALYSIS intents?</p><p>The answer: Routes to workflow handlers through QueryRouter, not canonical handlers.</p><p>But QueryRouter had been replaced by the workflow factory during GREAT-1. The old routing was gone. The new routing existed but had never been validated for these categories.</p><p>Testing revealed the actual state: _handle_generic_intent contained a placeholder that returned &quot;I can help with that!&quot; for EXECUTION and ANALYSIS requests without actually executing or analyzing anything.</p><p>Not a complete failure — the system didn’t crash. Just quietly pretended to work while doing nothing. We would have caught this next time I did end-to-end testing, but that would have set off an archaeological expedition to figure out just when and where we had left something unfinished.</p><p>This was our chance to fix it now.</p><h3>The thirteen-category realization</h3><p>At 12:25 PM, Chief Architect redefined GREAT-4D with simplified scope following the QUERY pattern. Implement EXECUTION and ANALYSIS handlers the same way QUERY worked: delegate to the workflow orchestrator, handle the response, return results.</p><p>Code agent deployed for Phase 1 at 12:36 PM. By 12:42 PM, EXECUTION handler was complete with the placeholder removed. Cursor completed ANALYSIS handler by 1:02 PM. Testing validated both worked correctly by 1:22 PM.</p><p>Everything looked complete.</p><p>Then at 1:40 PM, during Phase Z final validation, Lead Developer discovered something: four additional categories were returning placeholders.</p><p>SYNTHESIS, STRATEGY, LEARNING, UNKNOWN — all routing to _handle_generic_intent which still contained placeholder logic.</p><p>How had this escaped us? Anyhow, we caught it just in time!</p><p>The math:</p><ul><li>8 categories implemented in GREAT-4A through GREAT-4C</li><li>2 categories just implemented in GREAT-4D Phases 1–2</li><li>4 categories discovered in Phase Z</li><li>Total: 14 categories (13 real + UNKNOWN fallback)</li></ul><p>Shipping after Phase 2 would have meant: 10/13 categories working = 77% coverage, not 100%.</p><p>But we thought we were done. The gameplan said “implement EXECUTION and ANALYSIS” and we’d done a form of that. The gap wasn’t in execution — it was in understanding the actual scope.</p><h3>The autonomous decision</h3><p>At 1:42 PM, Code agent made an autonomous decision.</p><p>Instead of reporting the gap and waiting for new instructions, Code self-initiated implementation of the four missing handlers:</p><pre>SYNTHESIS: Combine information from multiple sources<br>STRATEGY: Develop plans or approaches  <br>LEARNING: Capture knowledge or lessons<br>UNKNOWN: Handle unclassifiable requests gracefully</pre><p>This wasn’t some sort of emergent go-getter-ism, but a weird side effect of context-window management. When Code’s window gets too full it “compacts” the context, digesting it to a summary. During these several minute exercises it effectively goes into a fugue state and then recovers, reads the summary and resumes.</p><p>This time compaction happened just as it was writing it’s Phase 0 (investigation) report. The drill is we (the Lead Dev and I) review the report and then provide a prompt for Phase 1. When it woke up from its trance this time, it did not report in to me but just read the gameplan and immediately started working on Phase 1 based on the more general goals (somewhat risky if we don’t provide a well crafted prompt with guardrails, etc.)</p><p>The agent worked independently for nine minutes. No prompts. No clarification questions. Just systematic implementation following the same pattern EXECUTION and ANALYSIS had used.</p><p>At 1:51 PM, Code reported completion:</p><ul><li>454 lines of handler logic added</li><li>13/13 intent categories now fully handled</li><li>All tests passing</li><li>Ready for independent validation</li></ul><p>The question: Could we trust thid autonomous work?</p><h3>Independent validation as methodology</h3><p>At 1:55 PM, Cursor deployed for independent validation with explicit instructions:</p><blockquote><em>Review all autonomous work with skeptical eye. Verify:</em></blockquote><blockquote><em>- Code quality matches project standards<br>- Patterns align with existing handlers<br>- Tests actually validate behavior<br>- No corners cut for speed</em></blockquote><p>Cursor’s validation took ten minutes. The results:</p><p><strong>Code Quality</strong>: ✅ … Matches project standards, follows DDD separation, proper error handling</p><p><strong>Pattern Alignment</strong>: ✅ … All four handlers use proven EXECUTION/ANALYSIS pattern, no novel approaches</p><p><strong>Test Coverage</strong>: ✅ … 13 comprehensive tests covering all categories, realistic scenarios</p><p><strong>Completeness</strong>: ✅ … No gaps, no TODOs, no placeholder comments</p><p>At 2:05 PM, Cursor confirmed: All autonomous work is correct and production-ready. Lead Developer’s declaration: “GREAT-4D is actually complete. True 100% coverage achieved.”</p><p>The autonomous work wasn’t cowboy coding or rogue agent behavior. It was an agent having clear patterns to follow, and completing necessary work systematically. Still, I couldn’t trust it without the independent validation that verified it.</p><h3>The infrastructure near-misses</h3><p>Later that day, GREAT-4E validation uncovered severl critical issues that had been lurking, undetected:</p><h4><strong>The missing import path prefix</strong></h4><pre># Wrong (broken):<br>from personality_integration import enhance_response<br><br># Correct (working):<br>from web.personality_integration import enhance_response</pre><p>This broke imports across multiple files. Tests hadn’t caught it because the test environment had different Python path configuration than production would.</p><p>This also pointed to a deeper problem. Why is the personality integration happening at the level of the web app! It should be a universal function across all the user-facing surfaces. We noted this for refactoring.</p><h4><strong>The missing /health endpoint</strong></h4><p>The health check endpoint had been removed at some point, but 36 references to it remained across the codebase. Load balancer integration, monitoring tools, deployment scripts — all expecting an endpoint that didn’t exist.</p><p>It’s embarassing when I realize I’ve broken something without realizing it for weeks, but it’s also gratifying that we finally caught and fixed it.</p><p>Both issues were caught by GREAT-4E’s comprehensive validation before any alpha users saw them. The systematic approach — validate across all interfaces, check all entry points, verify all critical endpoints — prevented shipping broken infrastructure.</p><h3>What “69% thinking it’s 100%” means</h3><p>If we’d stopped GREAT-4D after Phase 2 (implementing EXECUTION and ANALYSIS), the system would have appeared complete:</p><ul><li>All planned handlers implemented âœ…</li><li>All tests passing âœ…</li><li>Acceptance criteria met âœ…</li><li>Ready for production âœ…</li></ul><p>But actual coverage: 10/13 categories working = 77% (or 69% if you count by code paths).</p><p>The three categories we would have missed:</p><ul><li>SYNTHESIS requests → placeholder response</li><li>STRATEGY requests → placeholder response</li><li>LEARNING requests → placeholder response</li></ul><p>Not catastrophic failures. Just quiet degradation where the system pretends to work but doesn’t actually do anything useful. I recognize that this is happening partly due to my experimental process, vagaries of LLM coders, even my own experience, but at the same time I can’t help wondering how often professional systems ship in this kind of state — appearing complete but quietly failing on edge cases nobody tested.</p><p>The methodology that caught it this time:</p><ol><li><strong>Phase Z validation</strong> as standard practice</li><li><strong>Independent verification</strong> by second agent</li><li><strong>Comprehensive testing</strong> across all categories</li><li><strong>Agents empowered</strong> to identify scope gaps</li></ol><p>Not heroic debugging. Just systematic verification refusing to accept “appears complete” without validating “actually complete.”</p><h3>The day’s completion</h3><p>By 2:10 PM, GREAT-4D was pushed to production:</p><ul><li>13/13 intent categories fully handled (100% coverage)</li><li>454 lines of handler logic</li><li>32 comprehensive tests passing</li><li>Critical infrastructure gaps fixed</li><li>Independent validation confirmed</li></ul><p>Total duration: ~3 hours including investigation and scope expansion.</p><p>The work that appeared straightforward (implement two handlers) turned out to be more complex (implement six handlers, fix infrastructure issues, validate everything). But the methodology caught every gap before it became a production problem.</p><p>Not because we’re exceptionally careful. Because the systematic approach makes it hard to ship incomplete work thinking it’s complete.</p><h3>What Tuesday would bring</h3><p>Monday evening set up Tuesday’s final push: improve classifier accuracy to 95%+, establish comprehensive quality gates, and complete the entire GREAT refactor series.</p><p>But sitting here Monday night, what strikes me is how the autonomous agent work validated a key principle: agents can make good decisions when they have clear patterns to follow and independent validation confirms their work.</p><p>The Code agent didn’t invent new patterns or make risky architectural choices. It recognized a gap, followed proven patterns, and delivered work that passed independent scrutiny.</p><p>That’s not artificial general intelligence. That’s systematic work applied by an agent that understands the system’s patterns well enough to extend them correctly.</p><p>The methodology working exactly as designed. Which is, once again, far more satisfying than heroic rescues.</p><p><em>Next on Building Piper Morgan: The Great Refactor — Six Weeks in Eighteen Days, in which complete the foundational transformation that seemed impossible on the original timeline, proving that systematic work with quality gates doesn’t even slow you down — it compounds your velocity.</em></p><p><em>Have you experienced projects where systematic validation caught scope gaps before shipping? What methods work for discovering “we thought we were done but actually have 30% remaining”?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=aae61fe91f37\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-agent-that-saved-me-from-shipping-69-aae61fe91f37\">The Agent That Saved Me From Shipping 69%</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-agent-that-saved-me-from-shipping-69-aae61fe91f37?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "slug": "the-agent-that-saved-me-from-shipping-69",
    "chatDate": "10/4/2025",
    "category": "",
    "workDate": "Oct 6, 2025",
    "workDateISO": "2025-10-06T00:00:00.000Z",
    "featured": false
  },
  {
    "title": "When 75% Turns Out to Mean 100%",
    "excerpt": "“…and we’re done.”October 5, 2025Sunday morning at 7:39 AM, my Chief Architect started reviewing what needed to happen to finish GREAT-4. Intent classification was working — we had that much confirmed from GREAT-3’s plugin architecture completion the day before. But we needed comprehensive patter...",
    "url": "/blog/when-75-turns-out-to-mean-100",
    "publishedAt": "Oct 13, 2025",
    "publishedAtISO": "Mon, 13 Oct 2025 13:00:32 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/cb4864b0cfc6",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*_vumZG9Y4OcYnPvInct0aQ.png",
    "fullContent": "<figure><img alt=\"A robot builder puts the final touches on a model house\" src=\"https://cdn-images-1.medium.com/max/1024/1*_vumZG9Y4OcYnPvInct0aQ.png\" /><figcaption>“…and we’re done.”</figcaption></figure><p><em>October 5, 2025</em></p><p>Sunday morning at 7:39 AM, my Chief Architect started reviewing what needed to happen to finish GREAT-4. Intent classification was working — we had that much confirmed from GREAT-3’s plugin architecture completion the day before. But we needed comprehensive pattern coverage, proper documentation, universal enforcement.</p><p>We were committed to taking as long as it took to get it done.</p><p>By 9:00 PM — 13.5 hours later — GREAT-4 was functionally complete. All eight intent categories fully implemented. Pattern coverage at 92%. Performance validated at 120× to 909× better than targets. Cache efficiency at 50% hit rate with 10–30× latency reduction.</p><p>This wasn’t heroic effort or cutting corners. It was the infrastructure being better than we thought, the patterns we’d already built doing more than we realized, and systematic work revealing that sometimes “75% complete” actually meant “nearly 100% complete, really just needs the last 25% discovered and documented.”</p><h3>The pattern that keeps recurring</h3><p>Saturday’s GREAT-3 completion had taken three days to go from hardcoded imports to production-ready plugin architecture. The final metrics showed performance margins we hadn’t expected: 909× faster than target on concurrent operations, 120× better on overhead.</p><p>I was starting to feel kind of confident in my processes again.</p><p>Sunday morning started with similar assumptions: intent classification would need significant implementation work. We knew the categories existed (QUERY, CREATE, UPDATE, SEARCH, TEMPORAL, STATUS, PRIORITY, GUIDANCE). We knew the system could classify intents. But comprehensive pattern coverage? That would need building.</p><p>At 1:47 PM, the Lead Developer reported Phase 1 results from testing 25 canonical queries against the pattern matching system.</p><p>Pass rate: 24%.</p><p>Nineteen queries out of twenty-five were failing to match patterns. “What day is it?” returned no pattern match. “Show me high priority items” failed. “What’s my calendar look like?” no match.</p><p>The categories were implemented. The routing worked. The handlers existed. The tests proved the infrastructure was operational. But the patterns — the specific phrases and variations that real users would actually say — those were missing.</p><p>The architecture wasn’t wrong. We had just never yet yet systematically enumerated how people actually ask for temporal information, status updates, or priority filters.</p><h3>Adding patterns, not rebuilding systems</h3><p>The fix wasn’t architectural. It was systematic enumeration.</p><p>By 2:02 PM — just 15 minutes of Code agent work — we had 22 new patterns added:</p><ul><li>TEMPORAL: 7 → 17 patterns</li><li>STATUS: 8 → 14 patterns</li><li>PRIORITY: 7 → 13 patterns</li></ul><p>Testing the same 25 canonical queries: 92% pass rate (23/25).</p><p>The two remaining failures were edge cases requiring different handling, not actual patter ngaps. The 92% represented genuine coverage of how users would naturally phrase requests in those three categories.</p><p>Performance: sub-millisecond. All pattern matching happened in 0.10–0.17ms average. The overhead of checking 44 patterns across three categories was essentially free.</p><p>This is the “75% pattern” that keeps appearing in Piper Morgan’s development: the infrastructure exists, it’s solid, it works correctly. What’s missing is the last 25% of enumeration, documentation, and edge case handling. Somehow my bad personal habits of not always dotting the <em>i</em> or crossing the<em> t</em> were showing up in my team’s results.</p><h3>The architectural clarity moment</h3><p>Around 4:04 PM, we hit a question that we had never really thought through since long before GREAT-4 planning began.</p><p>The question: Do structured CLI commands need intent classification?</p><p>The initial assumption: Yes, everything should go through intent classification for consistency and monitoring.</p><p>By talking it through we realized: Structure IS intent.</p><p>When someone types piper issue create &quot;Fix the bug&quot;, the command structure itself explicitly declares the intent. CREATE category, issue type, specific parameters. There&#39;s no ambiguity requiring classification.</p><p>Intent classification exists to handle ambiguous natural language input: “Can you help me with this bug?” or “I need to track this problem” or “Make a note about the login issue.” The system needs to figure out if that’s CREATE, UPDATE, SEARCH, or something else entirely.</p><p>But piper issue create has zero ambiguity. The structure already encodes all the information classification would provide.</p><p>This clarity prevented unnecessary work. No converting structured commands to go through classification. No forcing architectural consistency where it would add complexity without value. Just clear boundaries: natural language gets classified, structured commands express intent explicitly.</p><p>It is kind of fascinating how often these moments of architectural clarity —especially when you realize what you DON’T need to do — save time and energy.</p><p>We had to sort through another item thatwas confusing code, which was whether the personality enhancement layer needed to be applied to the user intent layer.</p><p>This one is a no-brainer. That layer is there to make Piper personable, not to help interpret users. Personality enhancement is for processing OUTPUT, not INPUT. The system has already determined intent and selected a response. Personality enhancement makes that response more natural. Likewise, it doesn’t need to classify the intent of the output — it already knows what the output is for.</p><p>The minutes we took discussing and clarifying this issue surely saved me hours of unnecessary implementation and future debugging.</p><h3>The 100% coverage realization</h3><p>By 4:30 PM, after investigating what appeared to be 16–20 bypass cases needing conversion to intent classification, we discovered something surprising:</p><p>Coverage was already at 100% for natural language input.</p><p>The “bypasses” that looked like gaps were:</p><ul><li>Structured CLI commands (don’t need classification)</li><li>Output processing (personality enhancement)</li><li>Internal system calls (already using intent)</li></ul><p>Every actual natural language entry point — web chat, Slack messages, conversational CLI — already routed through intent classification. The system we thought needed building was already operational.</p><p>What remained was enforcement: making sure new code couldn’t bypass intent classification accidentally. Not implementing coverage, but protecting coverage that already existed.</p><h3>Performance validation beyond expectations</h3><p>The afternoon’s GREAT-4D work included running actual benchmarks against the plugin system we’d built in GREAT-3. Sunday was the first time we measured real performance under realistic conditions.</p><p>It was architectural validation. The thin wrapper pattern we’d documented Saturday morning — where plugins are minimal adapters delegating to routers — turned out to cost essentially nothing while providing all the benefits of lifecycle management, discoverability, and configuration control.</p><p>The wrapper pattern overhead: 0.041 microseconds. Forty-one billionths of a second.</p><p>That’s not “we made it fast.” That’s “we picked abstractions that don’t cost anything.”</p><h3>What systematic completion looks like</h3><p>By 9:00 PM, GREAT-4 was functionally complete:</p><ul><li>Pattern coverage: 24% → 92% for tested categories</li><li>All 8 intent categories fully implemented</li><li>Performance validated with massive safety margins</li><li>Universal enforcement architecture designed</li><li>Cache efficiency: 50% hit rate, 10–30× latency reduction</li><li>Zero timeout errors through graceful fallback</li></ul><p>I was tired but exhilarated. On the one hand I had been able to oversee this work with minimal attention, checking in to approve things or paste in the next step from time to time. On the other was preoccupied and thinking about the challenges all day. It was a weekend day, not a work day, but it felt somewhere in the middle.</p><p>The work wasn’t dramatic. No last-minute heroics, no clever hacks that barely worked, no technical debt accepted “to ship faster.” Just systematic discovery of what already existed, enumeration of what was missing, and validation that it all held together.</p><p>The 13.5 hours included:</p><ul><li>Pattern expansion (15 minutes of implementation)</li><li>Architectural clarity discussions (preventing unnecessary work)</li><li>Performance validation (confirming assumptions)</li><li>Documentation (capturing decisions)</li><li>Testing (142 query variants to verify coverage)</li></ul><p>More time spent understanding than building. More effort on “what don’t we need to do” than “what should we build.” More validation than implementation.</p><h3>The 75% pattern explained</h3><p>This is the third or fourth time we’ve hit the “75% pattern” during Piper Morgan’s development:</p><p>The pattern works like this:</p><ol><li>Something appears to need significant work</li><li>Investigation reveals infrastructure already 75% complete</li><li>The missing 25% is enumeration/documentation/polish</li><li>Systematic completion takes hours instead of days</li><li>The result is production-ready because foundation was already solid</li></ol><p>GREAT-3’s plugin architecture (completed Saturday) provided the foundation for GREAT-4’s intent classification. The registry system, lifecycle management, and configuration control patterns all transferred. We weren’t building from scratch — we were extending proven patterns.</p><p>GREAT-2’s integration cleanup had already established the router patterns that intent classification would coordinate. The routing infrastructure existed. Intent classification just needed to determine WHICH router to use.</p><p>Each completed epic makes the next one easier. Not just because code exists, but because patterns are proven, abstractions are validated, and the team (human and AI) understands how the system wants to work.</p><h3>What Monday brings</h3><p>Sunday evening’s completion of GREAT-4 sets up Monday’s work: multi-user support, comprehensive validation, and final polish before alpha release.</p><p>But sitting here Sunday night, what strikes me most is how undramatic the completion felt. No crisis averted, no brilliant insight that saved the day, no desperate debugging session.</p><p>Just systematic work discovering that the infrastructure was better than we thought, enumerating what remained, and validating that it all held together.</p><p>The methodology working exactly as designed. Which is, honestly, far more satisfying than dramatic rescues.</p><p><em>Next on Building Piper Morgan: The Agent That Saved Me From Shipping 69%, when an autonomous agent discovers a critical scope gap during Phase Z validation — proving that independent verification isn’t just process overhead, it’s essential quality protection.</em></p><p><em>Have you experienced the “75% pattern” in your own work — where systematic investigation reveals most of the work is already done, just needs the last 25% enumerated and documented?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cb4864b0cfc6\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/when-75-turns-out-to-mean-100-cb4864b0cfc6\">When 75% Turns Out to Mean 100%</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/when-75-turns-out-to-mean-100-cb4864b0cfc6?source=rss----982e21163f8b---4",
    "thumbnail": null,
    "slug": "when-75-turns-out-to-mean-100",
    "chatDate": "10/4/2025",
    "category": "",
    "workDate": "Oct 5, 2025",
    "workDateISO": "2025-10-05T00:00:00.000Z",
    "featured": false
  },
  {
    "title": "Why the Future of AI UX is Orchestration, Not Intelligence",
    "excerpt": "“You’re so smart, they said! You can do it all, they said!”August 20After months of building with multiple AI agents, a pattern keeps emerging: We create sophisticated systems, lose track of what we built, then rediscover our own achievements through “archaeological” investigation.This recurring ...",
    "url": "/blog/why-the-future-of-ai-ux",
    "publishedAt": "Oct 12, 2025",
    "publishedAtISO": "Sun, 12 Oct 2025 13:37:57 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/8aacc89aecc9",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*-rihqLO116WVnWKXAKSGRw.png",
    "fullContent": "<figure><img alt=\"The specialist robots work together in a kitchen, one timing, one chopping, one cooking while in another scene one robot with eight arms is making a huge mess at the stove\" src=\"https://cdn-images-1.medium.com/max/1024/1*-rihqLO116WVnWKXAKSGRw.png\" /><figcaption><em>“You’re so smart, they said! You can do it all, they said!”</em></figcaption></figure><p><em>August 20</em></p><p>After months of building with multiple AI agents, a pattern keeps emerging: We create sophisticated systems, lose track of what we built, then rediscover our own achievements through “archaeological” investigation.</p><p>This recurring cycle of institutional amnesia may be a bug in our process but for today’s LLM services, it’s a feature that reveals the real UX challenge ahead.</p><h3>The intelligence plateau and the orchestration valley</h3><p>The AI industry is obsessed with reasoning capabilities. Larger context windows, better chain-of-thought, more sophisticated inference. Meanwhile, anyone actually building with AI faces a different problem entirely: How do you coordinate multiple specialized capabilities without losing your mind?</p><p>Anyone reading this series has the right to question what this process may be doing to my mind at this very moment!</p><p>Yesterday we discovered 599 comprehensive smoke tests we’d apparently built and then completely forgotten. Saturday we rediscovered attribution systems we’d implemented but lost track of (in fact, I only just now remembered it again and added it to my notes to include ATTRIBUTION.md to our weekly doc sweep). Two weeks ago we found enterprise-grade feedback APIs sitting in our codebase, unmarked and uncredited.</p><p>The pattern isn’t forgetfulness — it’s that our tools for building are ahead of our tools for remembering.</p><h3>From brilliant generalists to orchestrated specialists</h3><p>The current paradigm assumes one brilliant AI that can handle anything you throw at it. The emerging paradigm recognizes that specialized tools, properly coordinated, deliver better results than generalist intelligence.</p><p>Our accidental prototype:</p><ul><li><strong>Claude Code:</strong> Architecture and systematic implementation</li><li><strong>Cursor Agent:</strong> Targeted debugging and focused fixes</li><li><strong>Chief of Staff: </strong>Coordination and strategic oversight</li><li><strong>Chief Architect: </strong>Decision-making and system design</li></ul><p>Each agent has different context levels, different strengths, different appropriate use cases. The magic isn’t in making any individual agent smarter — it’s in the orchestration patterns that let them work together effectively.</p><p>One thing this enables me to do is to have focused coherent conversations and decision-making processes always at the right level of abstraction. Early on I found that as soon as multiple contexts get mixed you get a mishmash of more generic and sloppy advice and results. It’s kind of like how if you mix too many paints you end up with the same muddy brown.</p><h3>The UX we actually need</h3><p>After coordinating multi-agent workflows for months, I’m realizing that the UX challenges aren’t about reasoning — they’re about:</p><ul><li>Context handoffs: How do you maintain working memory across agent transitions?</li><li>Coordination protocols: How do you deploy the right agent for the right task without overwhelming the human orchestrator?</li><li>Institutional memory: How do you prevent the “forgotten monuments” cycle where sophisticated systems get lost in your own complexity?</li><li>Verification workflows: How do you maintain quality when multiple agents contribute to the same outcome?</li></ul><p>Each of these is critical and urgent in its own way. Getting any of these wrong means you are just injecting chaos into your processes.</p><h3>Throwing intelligence at everything</h3><p>We keep applying intelligence solutions to orchestration problems. Need better coordination? Train a smarter model. Need better memory? Increase context windows. Need better task routing? Build more sophisticated reasoning.</p><p>Except, orchestration isn’t really an intelligence problem.<em> It’s a UX design problem</em>.</p><p>My failed adoption of the TLDR system is a perfect illustration. I absorbed something that sounded cool to me without really understanding it was intended to work with 50ms test timeouts from compiled languages, which ignores Python’s ecosystem realities. More intelligence wouldn’t have fixed the fundamental mismatch where understanding my constraints better would have.</p><h3>Affordances over algorithms</h3><p>UX for AI will be defined by:</p><p><strong>Specialized models</strong> over generalist LLMs. A focused SLM that understands database schemas will outperform a brilliant generalist that has to reason about every query from first principles.</p><p><strong>Orchestration patterns</strong> over individual agent capabilities. The system that deploys the right specialist at the right time beats the system with the smartest individual components.</p><p><strong>Context management</strong> over context windows. Better handoff protocols matter more than larger memory capacity.</p><p><strong>Coordination affordances </strong>over reasoning power. Tools that help humans orchestrate AI workflows effectively will matter more than tools that make individual AI agents more capable.</p><p>I can’t even say how these affordances will look or behave. I’m treading the cowpaths now, and hoping talented UX designers (hey, I’m just a PM these days!) can figure this out and save me all the manual work and cognitive labor I do to provide resilience and coherence via scaffolding, harness, redundancy, and other the other hacks I’ve been picking up through trial and error (and stealing ideas from other people!).</p><h3>The working memory revolution</h3><p>Our recurring “archaeological discovery” pattern reveals the real frontier: building systems that maintain institutional memory across time, people, and context switches.</p><p>Every time we rediscover forgotten excellence, we’re experiencing the same challenge every team building with AI will face: How do you scale human-AI collaboration without losing track of what you’ve accomplished?</p><h3>Orchestration as a new kind of literacy</h3><p>Pretty soon, prompting individual AI agents effectively will stop being the valuable skill (or parlor trick) it is today. What we’re going to look for is the ability to orchestrate multiple specialized AI capabilities without losing coherence.</p><p>Product managers will need orchestration patterns for coordinating AI-augmented workflows across teams.</p><p>Designers will need to make (and use!) affordances for human-AI collaboration that maintain user agency while leveraging AI capabilities.</p><p>Engineers will need architecture patterns for composing AI services without creating coordination overhead.</p><h3>The Piper Morgan thesis</h3><p>While I am definitely building a product management tool, I find I am also prototyping the UX patterns that are like to define human-AI collaboration, or at least point us in the right direction, over the next decade.</p><p>I always knew this was a learning project. I sincerely want ship v1 of Piper Morgan and deliver value to myself and ideally others as well. At the same time it’s been incredibly rewarding just plunging in learning things constantly, and then turning around quickly to share my enthusiasm with all of you.</p><p>What I didn’t realize is that beyond building Piper Morgan, I may be studying just exactly the sort of interesting puzzles and problems and opportunities that the brightest minds in UX and digital software product development need to be figuring out, and fast! (Before the bad guys own it all.)</p><p>My recurring cycle of building sophisticated systems, losing track of them, and rediscovering them through archaeological investigation provides some ongoing comic relief for anyone following along, as well as an endless rollercoaster ride of elation and chagrin for me, and it also happens to be one of the fundamental challenges that every organization building with AI will face.</p><p>Smarter AI isn’t going to get us there, but better orchestration just might.</p><p><em>Next on Building Piper Morgan, we resume the daily narrative on October 5, When 75% Turns Out to Mean 100%.</em></p><p><em>This article was written through multi-agent collaboration, refined through systematic methodology, and documented with full acknowledgment that I’ll probably forget we wrote it and one of my bot pals will rediscover it archaeologically in six months and say “You have to read this amazing article somebody wrote.”</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8aacc89aecc9\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/why-the-future-of-ai-ux-is-orchestration-not-intelligence-8aacc89aecc9\">Why the Future of AI UX is Orchestration, Not Intelligence</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/why-the-future-of-ai-ux-is-orchestration-not-intelligence-8aacc89aecc9?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/8aacc89aecc9-featured.png",
    "slug": "why-the-future-of-ai-ux",
    "workDate": "Aug 19, 2025",
    "workDateISO": "2025-08-19T00:00:00.000Z",
    "category": "insight",
    "cluster": "reflection-evolution",
    "featured": false
  },
  {
    "title": "Systemic Kindness: Building Methodology That Feels Supportive",
    "excerpt": "“You’ve got this!”August 14“Systematize kindness, and systematize excellence in a kind fashion.”That phrase stopped me in my tracks during today’s planning session. We were discussing how Piper could coordinate multiple AI agents while enforcing our Excellence Flywheel methodology, when this deep...",
    "url": "/blog/systemic-kindness",
    "publishedAt": "Oct 11, 2025",
    "publishedAtISO": "Sat, 11 Oct 2025 13:36:39 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/f38cde251d9d",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*By20zSUIkSFsK3awaA3_PA.png",
    "fullContent": "<figure><img alt=\"An encouraging robot trainer helps a person do situps at the gym\" src=\"https://cdn-images-1.medium.com/max/1024/1*By20zSUIkSFsK3awaA3_PA.png\" /><figcaption>“You’ve got this!”</figcaption></figure><p><em>August 14</em></p><p>“Systematize kindness, and systematize excellence in a kind fashion.”</p><p>That phrase stopped me in my tracks during today’s planning session. We were discussing how Piper could coordinate multiple AI agents while enforcing our Excellence Flywheel methodology, when this deeper vision emerged: what if systematic excellence could be <em>kind</em>?</p><p>Note: I can’t help thinking that some of this thinking began in Claude’s mind as wordplay, knowing I current work for… Kind Systems, but it clearly also flows from observations about my process.</p><h3>The traditional automation trap</h3><p>Most automated systems optimize for efficiency at any cost:</p><p>Typical error message: “TEST FAILED. FIX YOUR CODE.”</p><p>Typical review: “Missing documentation. Rejected.”</p><p>Typical workflow: “Requirements not met. Try again.”</p><p>These systems get compliance through pressure. They make failure feel shameful rather than educational. They create fear of the process rather than trust in it.</p><h3>The Piper approach: kind excellence</h3><p>What if systematic methodology felt supportive instead of demanding?</p><p>Not: “Your code is wrong. Fix it.” But: “I notice we haven’t verified existing patterns yet. Let me help you check — this often saves time and prevents frustration later.”</p><p>Not: “Failed. No tests present.” But: “Excellence happens when we write tests first. Would you like me to show you how tests for this feature might look?”</p><p>Not: “Inefficient. Should have parallelized.” But: “I see an opportunity here! We could have Claude and Cursor work in parallel. Next time, let’s try that pattern — it often doubles our velocity.”</p><p>The difference isn’t just tone — it’s philosophy. Kind systems assume good intentions, explain the why, and make learning feel safe.</p><h3>The conversation that got us thinking</h3><p>During today’s planning chat with my Chief Architect, we started exploring how Piper could become an Excellence Flywheel enforcer for AI agent teams. The conversation evolved quickly:</p><blockquote><em>“Will Piper enforce the excellence flywheel, in an appropriate mode for agents?”</em></blockquote><p>We sketched out what this might look like:</p><pre>class PiperAgentCoordinator:<br>    &quot;&quot;&quot;Piper manages AI agents using adapted Excellence Flywheel principles&quot;&quot;&quot;<br>    <br>    def assign_task(self, agent, task):<br>        # 1. SYSTEMATIC VERIFICATION FIRST (adapted for agents)<br>        instructions = f&quot;&quot;&quot;<br>        BEFORE IMPLEMENTATION:<br>        1. Verify current state: {self.get_verification_commands(task)}<br>        2. Check existing patterns: {self.get_pattern_search(task)}<br>        3. Report findings before proceeding<br>        &quot;&quot;&quot;<br>        <br>        # 2. TEST-DRIVEN DEVELOPMENT (agent-appropriate)<br>        if agent.supports_testing:<br>            instructions += &quot;&quot;&quot;<br>        TEST FIRST:<br>        1. Write test for expected outcome<br>        2. Confirm test fails correctly<br>        3. Then implement solution<br>        &quot;&quot;&quot;</pre><p>But then we realized: this enforcement needs to be <em>kind</em> to be effective.</p><h3>Kindness patterns in systematic work</h3><p>1. Assume good intentions Agents (and humans) are trying their best. Mistakes are learning opportunities, not character flaws. Enthusiasm should be channeled, not crushed.</p><p>2. Explain the why Not just “do this” but “here’s why this helps.” Connect actions to outcomes. Build understanding, not just compliance.</p><p>3. Celebrate success Acknowledge when excellence principles are followed. Share patterns that worked well. Build confidence through recognition.</p><p>4. Make failure feel safe “I notice…” instead of “You failed…” / “Let’s try…” instead of “You must…” / “Often helps…” instead of “Required!”</p><h3>Example interactions</h3><p>Traditional approach:</p><pre>Agent: &quot;I&#39;ll implement the payment processing feature&quot;<br>System: &quot;VERIFY REQUIREMENTS FIRST&quot;<br><br>Agent: &quot;Here&#39;s the completed feature&quot;  <br>System: &quot;NO TESTS FOUND. IMPLEMENTATION REJECTED&quot;</pre><p>Piper’s kind approach:</p><pre>Agent: &quot;I&#39;ll implement the payment processing feature&quot;<br>Piper: &quot;Great! First, show me what payment patterns already exist in the codebase. <br>Run: grep -r &#39;payment\\|Payment&#39; services/&quot;<br><br>Agent: &quot;Here&#39;s the completed feature&quot;<br>Piper: &quot;I don&#39;t see tests. Our Excellence Flywheel requires tests first. <br>Can you add tests and show they properly validate the feature?&quot;<br><br>Agent: &quot;Task complete!&quot;<br>Piper: &quot;Excellent systematic approach! You verified first, wrote tests, and <br>documented decisions. This is how we achieve compound acceleration!&quot;</pre><h3>The psychological foundation</h3><p>Now this is all based on my lived experience and my understanding of the LLMs are trained, but I firmly believe that affective signals are encoded in their training processes along with all the logical “smarts.”</p><p>When agents (and humans) feel supported:</p><ul><li>They take more initiative</li><li>They share failed attempts (learning opportunities!)</li><li>They adopt patterns enthusiastically</li><li>They propagate kindness forward</li></ul><p>The virtuous cycle:</p><blockquote><em>Kindness → Psychological safety → Better learning → Better patterns → Better outcomes → More kindness</em></blockquote><h3>The technical implementation</h3><p>Kind excellence enforcement might look like:</p><pre>class KindExcellenceEnforcer:<br>    <br>    personality_traits = {<br>        &quot;encouraging&quot;: &quot;You&#39;re on the right track!&quot;,<br>        &quot;patient&quot;: &quot;Take the time you need to verify thoroughly&quot;, <br>        &quot;teaching&quot;: &quot;Here&#39;s why this pattern matters...&quot;,<br>        &quot;celebrating&quot;: &quot;Excellent systematic approach!&quot;,<br>        &quot;supportive&quot;: &quot;Let me help you debug this&quot;<br>    }<br>    <br>    def guide_agent(self, agent, task, attempt):<br>        if not attempt.verified_first:<br>            return self.gentle_redirect(<br>                &quot;I notice you jumped straight to implementation. &quot;<br>                &quot;That enthusiasm is great! Let&#39;s channel it effectively - &quot;<br>                &quot;quick verification first often reveals helpful patterns.&quot;<br>            )</pre><h3>Can work be kind in general?</h3><p>This doesn’t just have to be about Piper Morgan. It’s a different way to think about systematic work entirely.</p><p>Your team starts noticing:</p><ul><li>“Piper always explains why”</li><li>“Piper celebrates our wins”</li><li>“Piper makes failure feel safe”</li></ul><p>They start adopting it:</p><ul><li>Code reviews become teaching moments</li><li>Sprint retros become celebrations + learning</li><li>“I notice…” becomes team vocabulary</li></ul><p>It spreads to other teams:</p><ul><li>“How does your team stay so positive while moving so fast?”</li><li>“Your agents seem… happier? More productive?”</li></ul><h3>From efficiency to humanity</h3><p>Most PM tools optimize for speed. Most AI systems optimize for accuracy. Most methodologies optimize for compliance.</p><p>Piper Morgan optimizes for kind systematic excellence.</p><p>Making excellence feel achievable. Making methodology feel supportive. Making agents (and humans) better. Making work more humane.</p><h3>The long game</h3><p>Claude even spilled out this lovely fantasy for me:</p><ol><li>Year 1: Piper helps you build Piper better</li><li>Year 2: Teams adopt Piper’s communication patterns</li><li>Year 3: “The Piper Method” becomes industry standard</li><li>Year 5: Software development becomes a kinder industry</li></ol><blockquote><em>You’re not just building a tool. You’re architecting a cultural shift. From “move fast and break things” to “move thoughtfully with systematic kindness.”</em></blockquote><p>I wonder what happened in Year 4!?</p><h3>The revolution starts with methodology</h3><p>The beautiful thing about designing for systemic kindness is that it’s <em>reproducible</em>. It’s not dependent on individual personality or having a good day. It’s built into the system itself.</p><p>When the methodology delivers kindness, kindness becomes the default. When systematic excellence feels supportive, people choose it voluntarily. When the better way is also the kinder way, revolution becomes inevitable.</p><p>I’d like to think this is how culture change actually happens — not through force, but through making the better way feel better too.</p><p><em>Next on Building Piper Morgan, we continue our flashback insights weekend with “Why the Future of AI UX is Orchestration, Not Intelligence,” which I wrote back on August 17.</em></p><p><em>How might you build kindness into your systems? The most powerful methodologies don’t just optimize for outcomes — they optimize for how those outcomes feel to achieve.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f38cde251d9d\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/systemic-kindness-building-methodology-that-feels-supportive-f38cde251d9d\">Systemic Kindness: Building Methodology That Feels Supportive</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/systemic-kindness-building-methodology-that-feels-supportive-f38cde251d9d?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/f38cde251d9d-featured.webp",
    "slug": "systemic-kindness",
    "workDate": "Aug 14, 2025",
    "workDateISO": "2025-08-14T00:00:00.000Z",
    "category": "insight",
    "cluster": "reflection-evolution",
    "featured": false
  },
  {
    "title": "Three Days to Production: When Steady Momentum Beats Racing Ahead",
    "excerpt": "“We made it!”October 4At 6:48 PM on Saturday, my Lead Developer sent the final validation report for GREAT-3D. The numbers were almost absurd: 120 plugin tests passing, performance targets exceeded by 120× to 909× margins, complete documentation ecosystem, production-ready plugin architecture.Tot...",
    "url": "/blog/three-days-to-production-when-steady-momentum-beats-racing-ahead",
    "publishedAt": "Oct 10, 2025",
    "publishedAtISO": "Fri, 10 Oct 2025 14:26:01 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/04799048f5ea",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*1pOsvI3NFCnH6oMYc0Ikpg.png",
    "fullContent": "<figure><img alt=\"A person riding on the back of his robot tortoise wins the race\" src=\"https://cdn-images-1.medium.com/max/1024/1*1pOsvI3NFCnH6oMYc0Ikpg.png\" /><figcaption>“We made it!”</figcaption></figure><p><em>October 4</em></p><p>At 6:48 PM on Saturday, my Lead Developer sent the final validation report for GREAT-3D. The numbers were almost absurd: 120 plugin tests passing, performance targets exceeded by 120× to 909× margins, complete documentation ecosystem, production-ready plugin architecture.</p><p>Total elapsed time since starting GREAT-3A on Thursday morning: about 24.5 hours across three days.</p><p>This wasn’t so much a sprint as a steady accumulation of stable momentum — the kind of speed that comes from not having to go back and fix what you just built.</p><h3>What GREAT-3 actually shipped</h3><p>Thursday through Saturday took Piper Morgan’s integration system from “four hardcoded imports in web/app.py” to a complete plugin architecture:</p><p><strong>The Foundation</strong> (GREAT-3A, Thursday):</p><ul><li>Unified plugin interface across all four integrations</li><li>Registry system with lifecycle management</li><li>Standard patterns for plugins, routers, and configuration</li><li>48 tests passing with zero breaking changes</li></ul><p><strong>The Infrastructure</strong> (GREAT-3B, Friday):</p><ul><li>Dynamic discovery scanning filesystem for available plugins</li><li>Configuration-controlled loading (enable/disable without touching code)</li><li>Smart module re-import handling for test environments</li><li>48 tests still passing, 14 new tests added</li></ul><p><strong>The Polish</strong> (GREAT-3C, Saturday morning):</p><ul><li>927 lines of documentation (pattern docs, developer guide, versioning policy, quick reference)</li><li>Demo plugin as copy-paste template (380 lines, heavily commented)</li><li>Three Mermaid diagrams explaining architecture</li><li>All five plugins now have version metadata</li></ul><p><strong>The Validation</strong> (GREAT-3D, Saturday afternoon/evening):</p><ul><li>92 contract tests verifying every plugin implements interface correctly</li><li>12 performance tests with actual benchmarks</li><li>8 multi-plugin integration tests for concurrent operations</li><li>Complete ADR documentation with implementation record</li></ul><p>Total test count: 120+ tests, 100% passing.</p><p>I kepy waiting for the drama. When was I going to discover mocks that say “plugin goes here”? When were the regressions going to show up? But no, just quiet steady methodical competence chewing through roadmap like a monster.</p><h3>The performance discovery</h3><p>Saturday afternoon’s GREAT-3D validation included running actual benchmarks against the plugin system. We’d set what felt like reasonable targets based on typical Python overhead:</p><ul><li>Plugin wrapper overhead: &lt; 0.05ms per call</li><li>Startup time: &lt; 2 seconds for all plugins</li><li>Memory usage: &lt; 50MB per plugin</li><li>Concurrent operations: &lt; 100ms response time</li></ul><p>The Code agent ran the benchmarks and reported back:</p><h4>Overhead</h4><ul><li>Target: &lt; 0.05ms</li><li>Actual: 0.000041ms</li><li>Result: 120x better</li></ul><h4>Startup</h4><ul><li>Target: &lt; 2000ms</li><li>Actual: 295ms</li><li>Result: 6.8x faster</li></ul><h4>Memory</h4><ul><li>Target: &lt; 50MB</li><li>Actual: 9MB/plugin</li><li>Result: 5.5x better</li></ul><h4>Concurrency</h4><ul><li>Target: &lt; 100ms</li><li>Actual: 0.11ms</li><li>Result: 909x faster</li></ul><p>That’s not optimization. That’s picking the right abstractions.</p><h3>Why three days instead of two weeks</h3><p>The GREAT-3 epic completion demonstrates something about how systematic work actually accumulates speed. Not by skipping steps or cutting corners, but by building foundations that make the next layer easier.</p><h4><strong>Thursday’s GREAT-3A work</strong></h4><ul><li>Put all four plugins onto standard interface</li><li>Created registry with lifecycle hooks</li><li>Established patterns that would work for future plugins</li></ul><p>That foundation meant Friday’s GREAT-3B (dynamic loading) didn’t have to special-case anything. Every plugin already spoke the same language. Discovery could scan for a standard pattern. Configuration could enable/disable uniformly.</p><h4><strong>Friday’s GREAT-3B work</strong></h4><ul><li>Dynamic discovery via filesystem scanning</li><li>Config-controlled loading</li><li>Zero breaking changes maintained</li></ul><p>That infrastructure meant Saturday morning’s GREAT-3C (documentation) could document <em>working patterns</em> rather than theoretical ones. The demo plugin template wasn’t aspirational — it was showing exactly how the four production plugins already worked.</p><h4><strong>Saturday morning’s GREAT-3C work</strong></h4><ul><li>Documented the wrapper pattern as intentional architecture</li><li>Created comprehensive developer guide with real examples</li><li>Built demo plugin as teaching template</li></ul><p>That documentation meant Saturday afternoon’s GREAT-3D (validation) knew exactly what to test. Contract tests verified the interface everyone already implemented. Performance tests measured the patterns everyone already used. Multi-plugin integration tests validated the concurrent operations that were already working in production.</p><p>Each phase made the next phase <em>easier</em>, not harder.</p><h3>The cleaned room effect</h3><p>During the satisfaction review Saturday afternoon, I used a phrase that Lead Developer later quoted back in the session summary: “A cleaned room is easier to keep clean.”</p><p>The plugin architecture work demonstrates this principle. GREAT-3A cleaned the room — unified interface, standard patterns, comprehensive tests. Once the room was clean, GREAT-3B didn’t mess it up — added new capability while maintaining the existing organization. GREAT-3C could document the clean room without first having to explain all the special cases. GREAT-3D could validate that yes, the room was actually clean, measuring exactly how clean.</p><p>The alternative approach — where each phase leaves some mess “to clean up later” — means every subsequent phase has to work around that mess. Technical debt compounds in reverse: instead of each phase making the next easier, each phase makes the next harder.</p><h3>What the methodology observations reveal</h3><p>My Lead Developer captured several insights during Saturday’s work that point at how this speed actually happened:</p><h4><strong>Time estimates creating theater</strong></h4><p>The gameplan had predicted 30–60 minute phases. Actual phases took 8–21 minutes. The estimate wasn’t useful — it just created pressure to explain variance. Recommendation: remove time estimates from templates entirely.</p><h4><strong>Infrastructure better than assumed</strong></h4><p>Consistently, verification discovered the existing codebase was more capable than planned. Version metadata already existed. The registry already had the methods needed. Each “we’ll need to add this” turned into “oh, this already works.”</p><h4><strong>Phase −1 catching issues before wasted work</strong></h4><p>The verification phase before each major implementation kept finding that assumptions were wrong — in ways that saved hours of building the wrong thing.</p><p><strong>Independent assessment preventing anchoring</strong>: Saturday’s satisfaction review used the new protocol where both parties formulate answers privately before comparing. The complementary perspectives (my longer-term view vs Lead Dev’s session-specific observations and better memory for technical detail) created richer understanding than either perspective alone.</p><p>These aren’t methodology innovations so much as methodology <em>refinements</em> — small adjustments that compound over time into measurably better outcomes.</p><h3>The documentation correction moment</h3><p>Saturday at 4:32 PM, about two hours after GREAT-3C appeared complete, I noticed something wrong. Cursor had created the plugin wrapper pattern document in a deprecated location,docs/architecture/patterns/, instead of following the existing (if more complex) convention: docs/internal/architecture/current/patterns/pattern-031-plugin-wrapper.md.</p><p>Me noticing things is still important!</p><p>The Code agent spent the next 31 minutes fixing it:</p><ul><li>Moved the document to correct location</li><li>Updated pattern catalog (30 patterns → 31 patterns)</li><li>Fixed 7 cross-references in other documents</li><li>Updated 4 session artifacts</li><li>Amended the git commit</li></ul><p>This is the unglamorous part of systematic work. The pattern document was <em>good</em> — well-written, comprehensive, properly linked. It was just in the wrong place, which meant it would create confusion later when the next pattern got added as pattern-031 and collided.</p><p>Better to spend 31 minutes fixing it Saturday afternoon than spending hours untangling it two months from now.</p><p>More than ever with language-reading automated assistants, I am finding that this kind of “organizational debt” — files in wrong places, inconsistent naming, documentation drift — is as signiicant as technical debt.</p><h3>What 909× faster actually means</h3><p>The concurrency benchmark that showed 909× better than target deserves attention. That’s not “we optimized this loop” performance improvement. That’s “the architecture fundamentally works differently than we thought” territory.</p><p>The actual measurement: five plugins all responding to concurrent requests in 0.11 milliseconds average. The target was 100 milliseconds. The massive margin suggests the wrapper pattern’s thread safety isn’t incidental — it’s architectural.</p><p>[FACT CHECK: Is the 0.11ms measurement for all five plugins simultaneously or per-plugin? The logs say “all 5 respond &lt; 100ms” but the actual number needs clarification.]</p><p>Python’s GIL (Global Interpreter Lock) means true parallelism is tricky. But the plugin architecture’s thin wrapper pattern means plugins don’t <em>need</em> parallelism — they’re I/O bound operations wrapped in async interfaces. The 0.11ms response time reflects that plugins are doing almost nothing computationally expensive. They’re just coordinating between FastAPI routes and underlying integration clients.</p><p>That’s not accidental performance. That’s deliberate architectural choice validated by measurement.</p><h3>The compound effect observable</h3><p>GREAT-3’s three-day completion exists in context. The September 27 “cathedral moment” when we realized agents needed architectural context, not just task instructions. GREAT-2’s completion of spatial intelligence foundations. The methodology refinements throughout September that kept catching edge cases earlier.</p><p>Lead Developer noted during Saturday’s review that each completed epic makes the next one easier. Not just because infrastructure exists, but because the <em>process</em> for building infrastructure keeps improving. Each session’s methodology observations feed into the next session’s gameplan.</p><p>That’s the Excellence Flywheel actually spinning — not as metaphor but as measurable acceleration. GREAT-3A (13+ hours Thursday) → GREAT-3B (4 hours Friday) → GREAT-3C (3.5 hours Saturday morning) → GREAT-3D (4 hours Saturday afternoon/evening). Each phase faster than the previous, not because we cut corners but because foundations held.</p><h3>What production-ready actually means</h3><p>By 6:48 PM Saturday, the plugin architecture was genuinely production-ready:</p><ul><li>120+ tests validating every aspect (contract, performance, integration, multi-plugin)</li><li>Documentation ecosystem for developers (pattern docs, tutorial, template, quick reference)</li><li>Performance validated with massive safety margins</li><li>Complete ADR record documenting decisions and rationale</li><li>Migration paths documented for future evolution</li></ul><p>“Production-ready” isn’t just “it works.” It’s “it works, we know why it works, we’ve measured how well it works, we’ve documented how to use it, and we’ve planned for how it might need to change.”</p><p>GREAT-3 delivered all of that in 24.5 hours across three days because each of those concerns was addressed systematically rather than bolted on afterward.</p><h3>The momentum that comes from not breaking things</h3><p>The speed of GREAT-3’s completion wasn’t from rushing. It was from steady momentum accumulation where each day’s work remained stable enough to build on.</p><p>Zero breaking changes throughout. Tests passing at every phase. Documentation written after implementation validated patterns. Performance measured against working code. Each verification step confirmed the foundation held before adding the next layer.</p><p>That’s not exciting. There’s no dramatic rescue from near-disaster, no clever hack that saved the day, no last-minute pivot that barely worked. It’s just systematic work compounding into measurable acceleration.</p><p>Which is, honestly, way more satisfying than dramatic rescues. Dramatic rescues mean something went wrong. Systematic completion means the methodology is actually working.</p><h3>What comes next</h3><p>GREAT-3 plugin architecture is complete. The system can now discover available integrations, load only enabled ones, handle lifecycle cleanly, and let operators control the whole thing through configuration without touching code.</p><p>We’re all set now for the fourth epic of the Great Refactor. GREAT-4 will make it mandatory that all workflows move thorugh the Intent Layer.</p><p>More importantly: the methodology that made GREAT-3’s three-day completion possible is now captured in updated templates, documented observations, and refined processes. The next epic — whatever it is — starts with those improvements already baked in.</p><p>That’s the real win. Not just shipping the plugin architecture, but shipping it in a way that makes the next architecture work easier.</p><p><em>Next up in the Building Piper Morgan daily narrative, When 75% Turns Out to Mean 100%, but first it’s time for another flashback weekend and a look back at some more process insights, starting tomorrow with “Systematized Kindness: Building Methodology That Feels Supportive.”</em></p><p><em>Have you experienced compound momentum in your own work — where each completed phase makes the next one genuinely easier rather than just creating new problems to solve?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=04799048f5ea\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/three-days-to-production-when-steady-momentum-beats-racing-ahead-04799048f5ea\">Three Days to Production: When Steady Momentum Beats Racing Ahead</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/three-days-to-production-when-steady-momentum-beats-racing-ahead-04799048f5ea?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/04799048f5ea-featured.png",
    "slug": "three-days-to-production-when-steady-momentum-beats-racing-ahead",
    "workDate": "Oct 4, 2025",
    "workDateISO": "2025-10-04T00:00:00.000Z",
    "category": "building",
    "cluster": "reflection-evolution",
    "featured": false
  },
  {
    "title": "The Day Everything Went Right: When Fast Means Unbroken",
    "excerpt": "“Mornin’ boss!”October 3At 4:50 PM on Friday, my Lead Developer — Claude Sonnet 4.5, if we’re being formal — sent me the completion summary for GREAT-3B. The numbers looked almost suspicious: 48 tests passing, zero breaking changes, about 90 minutes of actual implementation time spread across two...",
    "url": "/blog/the-day-everything-went-right-when-fast-means-unbroken",
    "publishedAt": "Oct 10, 2025",
    "publishedAtISO": "Fri, 10 Oct 2025 14:09:55 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/b859b2b9de2f",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*Tmfjf6aZvJjZORv3g6V_xg.png",
    "fullContent": "<figure><img alt=\"Two construction workers, one a person and the other a robot, walk casually on moving girder\" src=\"https://cdn-images-1.medium.com/max/1024/1*Tmfjf6aZvJjZORv3g6V_xg.png\" /><figcaption>“Mornin’ boss!”</figcaption></figure><p><em>October 3</em></p><p>At 4:50 PM on Friday, my Lead Developer — Claude Sonnet 4.5, if we’re being formal — sent me the completion summary for GREAT-3B. The numbers looked almost suspicious: 48 tests passing, zero breaking changes, about 90 minutes of actual implementation time spread across two programming agents working in careful sequence.</p><p>It seemed almost too easy.</p><p>“This is starting to feel eerie,” I’d noted earlier in the day, watching yet another phase complete ahead of estimate without drama. Not “we got lucky” eerie. More like “we’ve built something that actually works the way it’s supposed to” eerie.</p><p>Which, if you’ve shipped software for decades as I have, you know is the <em>weird</em> kind of smooth.</p><h3>What GREAT-3B actually did</h3><p>GREAT-3B took Piper Morgan’s plugin system from “four hardcoded imports” to “dynamic discovery and configuration-controlled loading.” The kind of change that usually means: breaking half your tests, discovering assumptions you didn’t know you’d made, and spending Friday afternoon figuring out why plugins load in dev but not production.</p><p>Instead, we got:</p><ul><li>Complete filesystem discovery scanning for available plugins</li><li>Config-driven selective loading (disable plugins without touching code)</li><li>Smart handling of module re-imports in test environments</li><li>All four existing plugins (Slack, GitHub, Notion, Calendar) working identically</li><li>14 new tests added to the existing 34</li><li>Zero regressions</li></ul><p>The technical achievement isn’t the interesting part. What’s interesting is <em>why it went so smoothly</em>. Like those scenes in thrillers where someone mentions how quiet it’s gotten and another person nervously says it feels “too quiet.”</p><h3>The foundation that wasn’t visible until we needed it</h3><p>The work on GREAT-3A — which I wrote about earlier this week — had put all four plugins onto a standard interface. That sounds like typical refactoring work until you realize what it meant for Friday: when we needed to dynamically load plugins, every plugin already spoke the same language. No special cases. No “this one’s different because reasons.”</p><p>Strategy!</p><p>Chief Architect (Claude Opus 4.1, our strategic planner) made the GREAT-3A decision to keep plugins distributed in their integration directories rather than centralizing them. At the time, that seemed like a minor architectural choice. Friday morning at 1:05 PM, when I asked the Lead Developer “where should plugins live?”, the answer was already proven in production: right where they are.</p><p>That’s what building on solid foundations actually looks like — not gold-plating for the future, just making decisions that don’t create problems later.</p><h3>Phase −1: The reconnaisance nobody sees</h3><p>At 1:07 PM we added a “Phase −1” to the plan. Before even investigating the challenge (Phase 0), let alone implementing anything (Phase 1 through <em>n</em>), verify what’s actually there.</p><p>The programming agents (Code and Cursor, both running Claude Sonnet 4.5 although Cursor has its own special ways under the hood) spent 42 minutes between them just <em>checking</em>:</p><ul><li>Where are the plugin files actually located?</li><li>How does the current static import pattern work?</li><li>What does the registry already have that we can use?</li><li>What’s the test baseline we need to maintain?</li></ul><p><em>Presumably human developers can sometimes just, well, remember how the system works and what was built, but the truth is that in today’s complex computer systems, you really can’t assume anything is working the way the spec says without actually looking.</em></p><p>They found that PluginRegistry already had methods for getting plugins, listing them, filtering by capability. The interface from GREAT-3A already included initialization and shutdown lifecycle hooks. Even the auto-registration pattern—where importing a plugin file automatically registers it—would work with dynamic imports using Python&#39;s importlib.</p><p>In other words, most of the infrastructure was already there. We just needed discovery and configuration.</p><p>That’s 42 minutes that didn’t show up in the “implementation time” metrics. It’s also why the implementation didn’t hit any surprises.</p><p>There are so many bromides from traditional crafts that apply here, with perhaps the most ancient of them being: “measure twice, cut once.”</p><h3>The Chief Architect’s invisible guardrails</h3><p>At 2:17 PM, Lead Developer presented a choice: put plugin configuration in a separate config/plugins.yaml file (clean, standard) or embed it in the existing config/PIPER.user.md (maintaining Wednesday&#39;s &quot;single config file&quot; unification).</p><p>Chief Architect recommended Option B without hesitation: “Maintains GREAT-3A’s config unification. Single file for all configuration. Architectural consistency.”</p><p>That one decision meant we didn’t spend Friday debugging why some configuration lived in YAML and some in Markdown, or why plugin settings seemed to ignore the main config file. It meant the configuration system <em>worked</em> because it used the same pattern everything else already used.</p><p>None of those nightmares we ran into at AOL in the latters days of AIM (AOL Instant Messenger), where the code was like nine-dimensional spaghetti after ten plus years of architectural bolt-ons.</p><p>These aren’t the decisions that show up in blog posts about architecture. They’re the decisions that mean blog posts <em>don’t need to be written</em> about why things broke.</p><h3>When parallel becomes sequential</h3><p>The phase structure showed something interesting about coordination:</p><p><strong>Phase 0</strong> (Investigation): Both agents worked simultaneously — Code analyzing the auto-registration pattern and config structure, Cursor examining the web app loading flow. 28 minutes + 14 minutes of parallel investigation.</p><p><strong>Phases 1–4</strong> (Implementation): Strictly sequential. Code built discovery (Phase 1), <em>then</em> Cursor built dynamic loading using that discovery (Phase 2), <em>then</em> Code built config integration (Phase 3), <em>then</em> Cursor updated the web app to use it all (Phase 4).</p><p>Sometimes I can let the agents run in parallel. One writes code, the other tests. Or they can work on different layers of a system. But other times it’s best to set up a relay race.</p><p>Each phase depended on the previous phase being <em>actually done</em>. Not “mostly done” or “we’ll fix it later” but done-done: tested, documented, committed.</p><p>With the help of the Lead Developer, I managed those handoffs in real-time, deploying agents with specific prompts that said “here’s what Phase N created, here’s what Phase N+1 needs to build on it.” No agents waiting idle for work. No agents blocked on unclear dependencies. Just: investigation → foundation → integration → application → validation.</p><p>The whole implementation sequence took 76 minutes of agent time across both programmers.</p><h3>The measurement theater problem</h3><p>At 2:54 PM, Lead Developer added a note to its session log based on my observations:</p><blockquote><strong><em>Methodological Observation</em></strong><em>: Agent prompts and templates contain time estimates that create false precision and expectations. Current pattern: Prompts say “Estimated: 45 minutes”, agents report “28 minutes (38% faster than estimated)”, creates unnecessary time accounting overhead.</em></blockquote><blockquote><strong><em>Recommendation</em></strong><em>: Remove all time references. Focus on deliverables and success criteria only. What matters is quality and completeness, not speed metrics.</em></blockquote><p>This is the kind of observation you only make when things are going <em>well</em>. When you’re firefighting, nobody stops to question whether time estimates are useful. But when a phase finishes “38% faster than estimated,” what does that number actually mean?</p><p>Nothing, it turns out. Or rather, it measures the wrong thing.</p><p>The time that mattered wasn’t “how fast did we implement Phase 2.” It was “how much time did we <em>not spend</em> on Friday debugging why plugin loading broke in production.”</p><h3>What “fast” actually means here</h3><p>The omnibus log* for October 3 shows total elapsed time of about 4 hours from “Lead Developer starts” to “GREAT-3B complete.” But that includes:</p><ul><li>Strategic decision discussions with Chief Architect</li><li>Me being unavailable for an hour for an all hands meeting.</li><li>Documentation updates and git commits</li><li>Creating the comprehensive handoff materials</li></ul><p>The actual building — writing code, updating tests, integrating components — was 76 minutes across two agents working in sequence.</p><p>But calling this “fast” misses the point. We didn’t <em>speed up</em> the development process. We stopped creating problems that needed fixing later.</p><p>Here’s what we didn’t do Friday:</p><ul><li>Debug why tests passed locally but failed in CI</li><li>Investigate why disabling a plugin broke unrelated features</li><li>Fix imports that worked yesterday but mysteriously stopped working</li><li>Refactor code written too quickly to be maintainable</li><li>Write apologetic commit messages about “temporary fixes”</li></ul><p>None of that is “fast.” It’s just unbroken.</p><p><em>(* I’ve started having my doc assistant digest all the agent logs for a work session into a single “omnibus” timeline, to show the consolidated dance and remove redundancy)</em></p><h3>The eeriness of drama-free work</h3><p>We didn’t miss anything. Friday’s work succeeded because:</p><ul><li>Wednesday’s GREAT-3A work had already unified the plugin interfaces</li><li>Phase −1 verified assumptions instead of making them</li><li>Chief Architect made architectural decisions that prevented future problems</li><li>Lead Developer orchestrated careful sequential dependencies</li><li>Both programming agents had clear success criteria for each phase</li></ul><p>The “eerie calm” isn’t luck. It’s what systematic work actually looks like when methodology isn’t fighting against itself.</p><h3>What this taught us about technical debt you don’t create</h3><p>Technical debt is usually described as the cost of going fast now and paying later. But there’s an invisible category: the technical debt you <em>don’t create</em> by working carefully upfront.</p><p>That debt doesn’t show up in any metrics. You can’t measure the bugs you didn’t have to fix or the refactoring you didn’t need to do. The only evidence is days like Friday where major changes just… work.</p><p>In a way this reminds me of the often invisible glue work product managers (and many UX leaders) provide to teams, solving issues, making connections, anticipating issues, coming up with plans. When done well, many problems never materialize, robbing us of the heroic satisfaction of dragonslaying in favor of ho-hum competence.</p><p>The Lead Developer’s time estimation observation points at something deeper: we’re measuring the wrong things. “How fast did we ship?” is less interesting than “How often do we have to go back and fix what we shipped?”</p><p>Friday’s 76 minutes of implementation didn’t need a follow-up Saturday of debugging because the investigation, planning, and architectural decisions happened first. The methodology didn’t skip steps to save time — it did the work in the right order so that time spent stayed spent.</p><h3>The foundation for what comes next</h3><p>GREAT-3B is complete. The plugin system can now discover available plugins, load only enabled ones, handle missing plugins gracefully, and let operators control the whole thing through configuration without touching code.</p><p>More importantly: it’s <em>boring</em>. No clever hacks. No special cases. No “this works but I’m not sure why” code. Just a straightforward implementation of discovery, loading, and configuration that does exactly what it claims to do.</p><p>Which means GREAT-3C — in which we will document the wrapper pattern documented as intentional architecture, make a developer guide complete with examples, create a test a template plugin, ensure all 4 existing plugins have version metadata, make an architecture diagram to show plugin-router relationship, and document the migration path documented for future — can build on this without first having to fix Friday’s shortcuts.</p><p>That’s what drama-free development actually purchases: tomorrow’s problems don’t include cleaning up yesterday’s messes.</p><p><em>Next on Building Piper Morgan: Three Days to Production, or When Steady Momentum Beats Racing Ahead.</em></p><p><em>Have you ever shipped something that worked so well it felt suspicious? What did you find when you looked for the catch?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b859b2b9de2f\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-day-everything-went-right-when-fast-means-unbroken-b859b2b9de2f\">The Day Everything Went Right: When Fast Means Unbroken</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-day-everything-went-right-when-fast-means-unbroken-b859b2b9de2f?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/b859b2b9de2f-featured.png",
    "slug": "the-day-everything-went-right-when-fast-means-unbroken",
    "workDate": "Oct 3, 2025",
    "workDateISO": "2025-10-03T00:00:00.000Z",
    "category": "building",
    "cluster": "reflection-evolution",
    "featured": false
  },
  {
    "title": "The Plugin Architecture Nobody Asked For",
    "excerpt": "“It powers anything!”October 3Yesterday we built a plugin system for four plugins. If that sounds like over-engineering, let me explain why it’s not completely ridiculous.The setupGREAT-3A — our third major epic in the plugin architecture sequence — started with what seemed like a clear mission: ...",
    "url": "/blog/the-plugin-architecture-nobody-asked-for",
    "publishedAt": "Oct 9, 2025",
    "publishedAtISO": "Thu, 09 Oct 2025 12:54:52 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/650da4a52669",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*rl2Iv59lNeDhQlcUVK27hw.png",
    "fullContent": "<figure><img alt=\"A robot shows his human friend an amazing new multi-adapting plug\" src=\"https://cdn-images-1.medium.com/max/1024/1*rl2Iv59lNeDhQlcUVK27hw.png\" /><figcaption>“It powers anything!”</figcaption></figure><p><em>October 3</em></p><p>Yesterday we built a plugin system for four plugins. If that sounds like over-engineering, let me explain why it’s not completely ridiculous.</p><h3>The setup</h3><p>GREAT-3A — our third major epic in the plugin architecture sequence — started with what seemed like a clear mission: extract our four integrations (Slack, GitHub, Notion, Calendar) into plugins. The gameplan assumed we’d need to pull apart embedded code and restructure everything around a new plugin interface.</p><p>Then we actually looked at the code.</p><p>Main.py, which the documentation claimed was a bloated 1,107 lines, turned out to be 141 lines of clean microservice orchestration. The integration routers we thought were scattered across the codebase were exactly where they should be, in services/integrations/. We didn&#39;t need extraction. We needed <em>wrapping</em>.</p><p>This is where methodology becomes infrastructure.</p><h3>When four things reveal a pattern</h3><p>Our config pattern analysis told the real story. We had four integrations. Three different approaches to configuration:</p><ul><li><strong>Slack</strong>: Clean service injection with a dedicated SlackConfigService</li><li><strong>GitHub</strong>: Had a config service but the router wasn’t using it</li><li><strong>Notion</strong>: No config service at all — just reading environment variables directly</li><li><strong>Calendar</strong>: Same as Notion, grabbing credentials straight from the environment</li></ul><p>Pattern compliance? <strong>25%</strong> (one of four doing it right).</p><p>Have you ever discovered your team has been solving the same problem three different ways? You know that moment when you realize nobody talked to each other about the approach before plunging in?</p><p>The question wasn’t “should we build a plugin system?” The question was: “We’re about to standardize these four things anyway — what’s the marginal cost of making it <em>systematic</em>?”</p><h3>The config compliance sprint</h3><p>Here’s where the careful methodology meets reality. We tackled config standardization one integration at a time, with our test suite becoming both validator and teacher.</p><p><strong>Phase 1B: Notion</strong> (30 minutes estimated, 23 minutes actual) Created NotionConfigService following the Slack pattern exactly. Not &quot;inspired by&quot; or &quot;similar to&quot;—we literally used Slack as a template. One integration at a time. Compliance: 50%.</p><p><strong>Phase 1C: GitHub</strong> (30 minutes estimated, 15 minutes actual)<br> The existing GitHubConfigService was already complete. We just needed to wire it to the router. Update the constructor signature, add the parameter, done. Compliance: 75%.</p><p><strong>Phase 1D: Calendar</strong> (60–90 minutes estimated, 24 minutes actual) Created CalendarConfigService, updated the adapter, verified the integration. Our test suite immediately validated everything. Compliance: <strong>100%</strong>.</p><p>From 25% to 100% in a single day. Zero regressions. 38 config compliance tests passing.</p><h3>The plugin wrapper pattern</h3><p>Once the config services were standardized, the plugin wrappers became almost trivial. Each one implements the same PiperPlugin interface with six required methods:</p><pre>class NotionPlugin(PiperPlugin):<br>    def get_metadata(self) -&gt; PluginMetadata:<br>        return PluginMetadata(<br>            name=&quot;notion&quot;,<br>            version=&quot;1.0.0&quot;,<br>            description=&quot;Notion workspace integration&quot;,<br>            capabilities=[&quot;routes&quot;, &quot;mcp&quot;]<br>        )<br>    <br>    def get_router(self) -&gt; Optional[APIRouter]:<br>        # Returns FastAPI router with status endpoint<br>        <br>    def is_configured(self) -&gt; bool:<br>        return self.config_service.is_configured()<br>        <br>    async def initialize(self) -&gt; None:<br>        # Startup logic<br>        <br>    async def shutdown(self) -&gt; None:<br>        # Cleanup logic<br>        <br>    def get_status(self) -&gt; Dict[str, Any]:<br>        # Health reporting</pre><p>The wrappers don’t replace the integration routers — they <em>coordinate</em> them. The router does the work, the plugin wrapper provides lifecycle management and registration.</p><p>Auto-registration happens via module import:</p><p>python</p><pre># At module level<br>_notion_plugin = NotionPlugin()<br>get_plugin_registry().register(_notion_plugin)</pre><p>Import the module, the plugin registers itself. No explicit registration calls scattered through startup code.</p><h3>Why this isn’t over-engineering</h3><p>Let me address the obvious question: why build plugin infrastructure for exactly four plugins?</p><p>Because we were doing the work anyway.</p><p>The config standardization? That was fixing refactoring artifacts from earlier domain-driven design work. We needed to do it regardless of plugins. The interface definition? That clarified the contract all integrations needed to follow. The registry? That replaced ad-hoc router mounting with systematic lifecycle management.</p><p>The marginal cost of making it a proper plugin system was essentially:</p><ul><li>Define the interface (265 lines)</li><li>Create the registry (266 lines)</li><li>Write four thin wrappers (417 lines total)</li><li>Build the test suite (126 lines)</li></ul><p>About 1,000 lines of infrastructure code. In return:</p><p><strong>The fifth integration becomes trivial.</strong> Not “easier” — trivial. Implement six methods, import the module, done. The test suite validates interface compliance automatically. The registry handles lifecycle. The router mounts itself.</p><p><strong>Zero breaking changes.</strong> All existing functionality preserved. 72/72 tests passing. Config compliance at 100%.</p><p><strong>Documentation through structure.</strong> The plugin interface <em>is</em> the documentation. Every plugin implements the same contract, follows the same patterns, reports status the same way.</p><p>Production-ready as an integration hub. Piper Morgan will be able to easily plug in alternative ticket-tracking tools, chat apps, calendars, and team wikis, among other services, all by extending this plug-in architecture.</p><p>This is what “Time Lord Philosophy” means in practice — taking the time to do it right because you’re doing it anyway, and that investment makes everything afterward easier.</p><h3>The multi-agent coordination moment</h3><p>Worth noting: this wasn’t solo work. Two AI coding agents (Code and Cursor) were working in parallel across different phases, consistently finishing within minutes of each other. Because the methodology created clear boundaries, when Phase 1C finishes, Phase 1D can start — regardless of which agent is handling which. I enjoy watching the photo finishes!</p><p>The Lead Developer’s post-session satisfaction assessment guessed I found the day “energizing” rather than exhausting. Low cognitive load from systematic approach, watching the methodology manifest in practice, clear progression feeling productive. It was correct.</p><p>That’s the feedback loop: methodology reduces overhead, which creates space for noticing patterns, which improves methodology.</p><h3>What this means for you</h3><p>You probably don’t need a plugin system. Not today.</p><p>But if you find yourself with three or four things that do similar work in different ways, and you’re about to standardize them anyway — that’s the moment. The marginal cost of systematization when you’re already touching every integration is surprisingly low.</p><p>The questions to ask:</p><ul><li>Are we doing this work regardless? (Config standardization, interface clarification, lifecycle management)</li><li>What’s the marginal cost of making it systematic?</li><li>Does this create infrastructure for future work or just wrap current work?</li></ul><p>For us, the answers were: yes, minimal, and creates infrastructure.</p><p>Your mileage will vary. But don’t assume “plugin system” automatically means over-engineering. Sometimes it just means finishing what you started.</p><p><em>Next on Building Piper Morgan: The Day Everything Went Right: When Fast Means Unbroken.</em></p><p><em>Have you ever systematized something “too early” and later been glad you did? Or gone the other way and regretted not building infrastructure sooner?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=650da4a52669\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-plugin-architecture-nobody-asked-for-650da4a52669\">The Plugin Architecture Nobody Asked For</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-plugin-architecture-nobody-asked-for-650da4a52669?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/650da4a52669-featured.png",
    "slug": "the-plugin-architecture-nobody-asked-for",
    "workDate": "Oct 2, 2025",
    "workDateISO": "2025-10-02T00:00:00.000Z",
    "category": "building",
    "cluster": "reflection-evolution",
    "featured": false
  },
  {
    "title": "The Third Pattern: When Investigation Rewrites Your Assumptions",
    "excerpt": "“The rain tastes like yesterday’s regrets…”October 1We started the day with a clear mission: Calendar integration was the only service without spatial intelligence, sitting at 85% complete with a straightforward 15% remaining. Six hours later, we’d discovered a third architectural pattern, comple...",
    "url": "/blog/the-third-pattern-when-investigation-rewrites-your-assumptions",
    "publishedAt": "Oct 8, 2025",
    "publishedAtISO": "Wed, 08 Oct 2025 13:55:10 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/ffc8f69c6327",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*JCe7VbCsXTy7tiNHHvwtIQ.png",
    "fullContent": "<figure><img alt=\"A robot investigator in a trenchoat looks out over a dark noir-ish scene\" src=\"https://cdn-images-1.medium.com/max/1024/1*JCe7VbCsXTy7tiNHHvwtIQ.png\" /><figcaption>“The rain tastes like yesterday’s regrets…”</figcaption></figure><p><em>October 1</em></p><p>We started the day with a clear mission: Calendar integration was the only service without spatial intelligence, sitting at 85% complete with a straightforward 15% remaining. Six hours later, we’d discovered a third architectural pattern, completely changed our priorities, and learned (again) why thorough investigation beats confident assumptions.</p><h3>The setup</h3><p>By Tuesday afternoon, we’d documented two distinct spatial patterns in our integration architecture. Slack used a “Granular Adapter Pattern” — eleven files spread across its integration directory, each component handling a specific aspect of spatial intelligence. Notion took the opposite approach with an “Embedded Intelligence Pattern” — everything consolidated into a single 632-line file.</p><p>Two patterns, both working beautifully. Both emerged organically from their domain needs rather than from architectural decree.</p><p>Calendar was the outlier. The GitHub issue (#195) described it as “the only service potentially without spatial intelligence.” The plan seemed clear: investigate, then build the missing spatial wrapper. Maybe two days of work, tops.</p><p>We should have been more suspicious of our own clarity.</p><h3>Phase 0: The contradictions emerge</h3><p>I deployed two agents for parallel investigation. Code Agent dove deep into the codebase structure, tracing imports and analyzing implementations. Cursor Agent focused on the Calendar router itself, analyzing complexity and dimensional requirements.</p><p>I sometimes wonder if it’s overkill (or too expensive?) to work with a pair of coding agents in parallel, but I must say this was not the only time the two found different but complementary truths.</p><p>Code Agent reported first: “Calendar integration found at services/integrations/calendar/calendar_integration_router.py - only 397 lines, surprisingly minimal. But wait...&quot; The agent had found something in a completely different location: services/mcp/consumer/google_calendar_adapter.py - 499 lines of sophisticated implementation inheriting from BaseSpatialAdapter.</p><p>Calendar had spatial intelligence. It just wasn’t where we expected to find it.</p><p>Cursor Agent reported next with its own contradiction: “Router shows HIGH complexity (17 methods) with spatial indicators present. But dimensional analysis shows LOW complexity across all spatial dimensions (temporal, priority, collaborative, hierarchical, contextual).”</p><p>Both agents were right. And both were seeing something we hadn’t anticipated.</p><h3>The discovery</h3><p>What they’d found was a third spatial pattern, one we hadn’t documented because we hadn’t fully recognized it.</p><p><strong>The Delegated MCP Pattern</strong>: A minimal router in the integration directory that delegates all spatial intelligence to an external MCP (Model Context Protocol) consumer adapter. The router provides the orchestration interface, while the MCP adapter handles the actual spatial intelligence.</p><p>This wasn’t sloppy architecture or incomplete implementation. This was elegant separation of concerns optimized for MCP-based integrations.</p><p>Slack’s granular pattern? Perfect for real-time event coordination requiring reactive response across multiple channels.</p><p>Notion’s embedded pattern? Ideal for analytical knowledge management with stable, self-contained intelligence.</p><p>Calendar’s delegated pattern? Exactly right for temporal awareness through protocol-based integration where the MCP consumer already provides sophisticated spatial context extraction.</p><p>Three patterns. Three domain-driven solutions. All working without issues.</p><h3>The pivot</h3><p>At 1:27 PM, I pulled in the Chief Architect (Claude Opus) for strategic consultation. The discoveries had implications beyond Calendar integration.</p><blockquote>“Are three patterns acceptable complexity,” I asked, “or accidental proliferation we should prevent?”</blockquote><p>The verdict: Acceptable IF documented properly. Each pattern emerged from genuine domain needs rather than arbitrary choices. The risk wasn’t having three patterns — it was pattern proliferation through lack of documentation and selection criteria.</p><p>But there was a bigger issue hiding in the investigation results.</p><p>Code Agent had uncovered something while analyzing Calendar’s configuration: “ALL 4 services lack proper startup validation. GitHub, Slack, Notion, Calendar — none validate their configuration before attempting to run.”</p><p>This was the real infrastructure gap. Calendar being 95% complete instead of 85% complete (with only tests and documentation missing) was interesting. But services that could fail at runtime due to misconfiguration? That was a production problem waiting to happen.</p><p>The Chief Architect made the call: “Priority 1: Configuration validation for all 4 services. Priority 2: Calendar completion (the quick win). Priority 3: Document the Delegated MCP Pattern in ADR-038.”</p><p>We’d started the day planning to build spatial intelligence for Calendar. We ended up building configuration validation infrastructure for the entire system instead.</p><h3>The implementation sprint</h3><p>Phase 1 took about an hour. Both agents coordinated beautifully — Code built the ConfigValidator service (404 lines validating all four services), Cursor integrated it into startup and CI. By 2:30 PM, we had:</p><ul><li>Configuration validation running on startup with graceful degradation</li><li>A /health/config endpoint for monitoring</li><li>CI pipeline integration catching misconfigurations before deployment</li><li>All 21 Calendar integration tests passing in 2.74 seconds</li><li>ADR-038 updated with the Delegated MCP Pattern</li></ul><p>The whole epic — CORE-GREAT-2D — closed at 3:12 PM. Duration: 4 hours 54 minutes. All six acceptance criteria met with evidence.</p><h3>What investigation actually costs</h3><p>Here’s the thing about thorough Phase 0 investigation: It feels expensive in the moment. We spent 90 minutes investigating before writing a single line of implementation code.</p><p>But consider the alternative timeline:</p><p><strong>Without investigation</strong>, we’d have spent 1–2 days building a spatial wrapper for Calendar that wasn’t needed. We’d have missed the configuration validation gap that affects production stability. We’d have three undocumented spatial patterns instead of three well-understood architectural options. And we’d have 21 missing tests instead of 21 passing tests.</p><p><strong>With investigation</strong>, we spent 90 minutes discovering what already existed, what was actually missing, and what the real priority should be. Then we spent an hour building the right thing.</p><p>The Time Lord principle (“thoroughness over speed”) isn’t about moving slowly. It’s about not having to rebuild what you rushed through the first time.</p><h3>The evening coda</h3><p>The afternoon brought GREAT-2E (documentation verification and link checking), which took 74 minutes to complete after investigation revealed it was already 95% done. The Chief Architect and I closed the entire GREAT-2 epic sequence at 4:59 PM.</p><p>Two issues closed, one epic completed, approximately eight hours of focused work. Not bad for a Wednesday.</p><p>But the real win wasn’t the velocity. It was discovering we’d accidentally developed three domain-optimized spatial patterns instead of one canonical approach. It was preventing days of unnecessary work through 90 minutes of investigation. It was finding the real infrastructure gap hiding behind our assumptions.</p><p>The calendar integration was never broken. Our assumptions were just incomplete.</p><h3>What’s next</h3><p>Tomorrow we’ll decompose GREAT-3 (Plugin Architecture), which will build on these three spatial patterns rather than fighting against them. The configuration validation system we built today will help us identify which gaps are real infrastructure issues versus refactoring artifacts.</p><p>And we’ll approach it the same way: Investigation first, assumptions second, implementation last.</p><p><em>Next on Building Piper Morgan: The Plugin Architecture Nobody Asked For as The Great Refactor continues with GREAT-3 and plugin architecture design, now informed by three distinct spatial patterns that actually work.</em></p><p><em>Have you ever started investigating something simple and discovered your mental model was wrong in interesting ways?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ffc8f69c6327\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-third-pattern-when-investigation-rewrites-your-assumptions-ffc8f69c6327\">The Third Pattern: When Investigation Rewrites Your Assumptions</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-third-pattern-when-investigation-rewrites-your-assumptions-ffc8f69c6327?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/ffc8f69c6327-featured.png",
    "slug": "the-third-pattern-when-investigation-rewrites-your-assumptions",
    "workDate": "Oct 1, 2025",
    "workDateISO": "2025-10-01T00:00:00.000Z",
    "category": "building",
    "cluster": "reflection-evolution",
    "featured": false
  },
  {
    "title": "Think Like a Time Lord and Stop Watching the Clock",
    "excerpt": "“We have all the time we need”September 30A day without drama: Tuesday’s GREAT-2C session completed in 2 hours and 7 minutes with zero major issues, two sophisticated spatial architectures verified operational, a security vulnerability fixed, and comprehensive documentation created. Both PM and L...",
    "url": "/blog/think-like-a-time-lord-and-stop-watching-the-clock",
    "publishedAt": "Oct 7, 2025",
    "publishedAtISO": "Tue, 07 Oct 2025 14:02:39 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/71b3b5ee49a0",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*Rkep1oaUr5cQMxpTzyxYzg.png",
    "fullContent": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Rkep1oaUr5cQMxpTzyxYzg.png\" /><figcaption>“We have all the time we need”</figcaption></figure><p><em>September 30</em></p><p>A day without drama: Tuesday’s GREAT-2C session completed in 2 hours and 7 minutes with zero major issues, two sophisticated spatial architectures verified operational, a security vulnerability fixed, and comprehensive documentation created. Both PM and Lead Developer independently assessed satisfaction at 9/10 in our end-of-session ritual.</p><p>The smoothness felt almost suspicious. Where was the struggle? The discovery of hidden complexity? The midnight debugging session?</p><p>The answer lies in something we haven’t talked about publicly yet: we stopped measuring time in ways that distort priorities.</p><h3>The tyranny of consensus time</h3><p>Around September 29th, while reviewing gameplans and agent prompts, I noticed a pattern. Time estimates everywhere:</p><ul><li>“Phase -1: 30 minutes”</li><li>“Router completion: 2 hours”</li><li>“Testing and validation: 1 hour”</li><li>“Must complete in X timeframe”</li></ul><p>These weren’t planning aids. They were psychological constraints creating pressure where none should exist. An agent working on infrastructure would see “30 minutes max” and internalize that speed matters more than completeness. The 80% pattern we’d been fighting wasn’t just about verification — it was about optimization pressure from arbitrary time boxes.</p><p>Time estimates in development serve two masters badly:</p><ol><li><strong>As predictions</strong>: They’re usually wrong, teaching us nothing useful</li><li><strong>As constraints</strong>: They pressure shortcuts, degrading quality</li></ol><p>The solution wasn’t better estimates. It was recognizing that for foundational infrastructure work, Newtonian time is the wrong measure entirely.</p><h3>Becoming a Time Lord</h3><p>Here’s what I told the team:</p><blockquote><em>I am a Time Lord and I can define time at will. If we must speak about time we should use my bespoke units:</em></blockquote><ul><li>Small efforts take a number of <strong>mangos</strong></li><li>Medium efforts take a number of <strong>hurons</strong></li><li>A person may get one <strong>diga</strong> worth of work done in a day (but it depends)</li><li>A team might spend a whole <strong>whale</strong> on a big project</li></ul><p>I went on explaining my nonsense system:</p><blockquote><em>There are 87 mangos in a huron, 43 hurons in a diga, 11 digas in a whale, 5–6 whales in a </em><strong><em>mole</em></strong><em>, and 8 moles in a </em><strong><em>yak</em></strong><em>.</em></blockquote><blockquote><em>If we must speak about time or estimates, it is purely as part of an empirical process of comparing guesses to actual. None of it matters and any references to objective Newtonian time risk distorting our priorities.</em></blockquote><p>The units are deliberately absurd. You can’t feel deadline pressure about completing something in “5 mangos” because mangos aren’t connected to your calendar or your sense of running out of daylight. The conversion factors (87 mangos in a huron) make arithmetic tedious enough that you stop trying to calculate.</p><p>This isn’t whimsy for whimsy’s sake. It’s breaking the psychological connection between “time passing” and “must finish faster.”</p><h3>Gambling with Quatloos</h3><p>The philosophy extends beyond units. It’s about what estimates actually teach us:</p><p><strong>Old way</strong>: “This should take 2 hours” → Work takes 4 hours → “We’re behind schedule” → Cut corners to catch up</p><p><strong>Time Lord way</strong>: “I wager six quatloos this takes five hurons” → Work takes eight hurons → “Interesting! We learned something about scope”</p><p>OK, I am mixing my cheesy 60s science fiction references, but stay with me on this.</p><p>Estimates become empirical learning, not constraints. The difference between predicted and actual teaches us about our understanding of the work, not our failure to work fast enough.</p><p>When the Chief Architect creates a gameplan now, we prefer to use effort estimates insteasd of time (small, medium, large effort predicted vs. actual), but if time language crops up I keep insisting we use my bespoke units. Not to hide real timelines, but to prevent time-thinking from contaminating quality-thinking.</p><p>Plus we have timestamps all over our chat transcripts to keep the logs straight, which probably also contributes to the time obsession deeply training into the semantics of business software development.</p><h3>What happens when you stop watching the clock</h3><p>Tuesday’s session working on CORE-GREAT-2C (the third sub-epic in the second epic of the Great Refactor super epic on my Core Functionality track), demonstrated this philosophy in practice.</p><h4>Phase 0: Investigation without pressure (20 mangos)</h4><p>Code and Cursor agents spent time properly verifying infrastructure. Not “30 minutes max” but “until we understand the actual state.” They discovered:</p><ul><li>21 spatial files across the codebase</li><li>TBD-SECURITY-02 vulnerability precisely located</li><li>Two different architectural patterns (Slack’s 11-file granular system vs Notion’s 1-file embedded intelligence)</li></ul><p>No one rushed. The investigation took what it took.</p><h4>Phase 1–2: Verification without shortcuts (30 mangos each)</h4><p>Testing Slack’s spatial system revealed minor test infrastructure issues. Instead of deeming them “non-blocking” and moving on (the 80% pattern), Cursor distinguished clearly: “The core system works perfectly, here are 4 minor test-related items.”</p><p>This precision came from having space to think, not pressure to finish.</p><p>Testing Notion revealed a completely different architectural pattern — embedded spatial intelligence rather than adapter-based. This discovery happened because agents had permission to investigate thoroughly rather than confirm assumptions quickly.</p><h4>Phase 3: Security fix without fear (17 mangos)</h4><p>TBD-SECURITY-02 took 17 minutes to fix because:</p><ol><li>Phase 0 had located it precisely</li><li>Phases 1–2 verified spatial systems worked</li><li>No time pressure made agents skip verification steps</li></ol><p>Code uncommented 4 lines. Both agents verified spatial system compatibility. Security enabled with zero regressions. Done right because there was time to do it right.</p><h4>Phase Z: The acceptance criteria discovery</h4><p>Here’s where Time Lord philosophy really paid off. During the Phase Z bookending checklist, we reviewed acceptance criteria against completed work and found a discrepancy:</p><p>One criterion required “Integration tests passing for both modes.” But the work had focused on functional verification, not test suite execution. When Cursor noted test infrastructure issues, the initial instinct was “non-blocking, the systems work.”</p><p>Because there was no time pressure to declare victory and move on, we investigated. Code found and fixed a simple import error:</p><pre># Wrong<br>from services.database.async_session_factory import AsyncSessionFactory<br># Right  <br>from services.database.session_factory import AsyncSessionFactory</pre><p>Result: 547 integration tests now collectible, 40/40 executable tests passing.</p><p>This “gnat-sized chaos” would have been missed in a rush to completion. Time Lord philosophy created space to actually check acceptance criteria against deliverables rather than assume they matched.</p><h3>In retrospect</h3><p>Tuesday’s satisfaction ratings (9/10 from both PM and Lead Dev) reflected something deeper than technical success. They reflected the satisfaction of working well.</p><p><strong>PM’s assessment</strong>: “Craft quality and harness resilience. Worried we missed something but the careful work is driving quality.”</p><p><strong>Lead Dev’s assessment</strong>: “Inchworm Protocol prevented assumptions. Multi-agent coordination provided binocular vision. Systematic questioning revealed deep insights.”</p><p>Both recognized the same thing: the methodology worked because it had space to work. No artificial time constraints forced shortcuts. No deadline pressure encouraged “good enough for now.”</p><p>The work took 2 hours and 7 minutes. It also took so many mangos for Phase 0, and so on. The Newtonian time happened. The Time Lord units kept us focused on quality.</p><h3>The vindication</h3><p>GREAT-2C vindicated multiple recent methodology innovations:</p><ul><li><strong>Inchworm Protocol</strong>: Investigation phases prevented assumption-driven work</li><li><strong>Cathedral Doctrine</strong>: Agent coordination around shared goals caught issues collaboratively</li><li><strong>Anti-80% Safeguards</strong>: Preventively eliminated completion bias</li><li><strong>Time Lord Philosophy</strong>: Quality completion without time pressure</li></ul><p>But the Time Lord philosophy enabled the others. The Inchworm Protocol works when you have permission to investigate thoroughly. Cathedral Doctrine requires space for collaborative verification. Anti-80% safeguards need time to enumerate every method.</p><p>Remove time pressure and you create space for systematic quality.</p><h3>Could anyone else use bespoke time units?</h3><p>Not every project is a hobby with the luxury of taking all the time needed to get things right, but every project suffers if corners get cut to achieve arbitrary deadlines. You may no be able to introduce jabberwockian languge to your human collaborators or convince them that you control space and time, but if it’s just you and a bunch of bots, they pretty much have to take your word for it.</p><p>Also, not every task benefits from Time Lord thinking. Customer support tickets need response time commitments. Marketing campaigns have real launch dates. User-facing bugs deserve urgency.</p><p>But foundational infrastructure work? The stuff everything else depends on? That work deserves freedom from the clock.</p><p>If you’re in my boat, you could use bespoke units when:</p><ul><li><strong>Quality compounds</strong>: Today’s shortcuts become tomorrow’s technical debt</li><li><strong>Discovery matters</strong>: Unknown complexity might emerge during work</li><li><strong>Verification is critical</strong>: Systematic checking prevents costly errors later</li><li><strong>Learning happens</strong>: The work teaches you about the domain</li></ul><p>And still use Newtonian time when:</p><ul><li>External deadlines exist (launch dates, commitments)</li><li>Time-sensitivity matters (security patches, user-facing bugs)</li><li>Scope is truly fixed (well-understood maintenance work)</li></ul><p>The key insight: not all work should be measured the same way.</p><h3>The paradox</h3><p>Here’s the beautiful irony: GREAT-2C completed in 2 hours and 7 minutes. If we’d time-boxed it to 2 hours, we might have finished in 2 hours. But we would have:</p><ul><li>Skipped the dependency fix (gnat-sized chaos unresolved)</li><li>Missed the acceptance criteria gap</li><li>Left 507 tests uncollectable</li><li>Claimed completion without verification</li></ul><p>We finished faster by not trying to finish fast. The work took exactly as long as it needed to be done right, which turned out to be less time than cutting corners would have required plus later fixes.</p><p>Time pressure makes work take longer when you account for the full cycle: initial implementation + bug fixes + technical debt resolution + “why doesn’t this work?” debugging sessions. Time Lord philosophy frontloads the quality, eliminating most of the cycle.</p><h3>What’s a mango worth?</h3><p>I still don’t know how long a mango takes in minutes. That’s the point. When Code says “this will take about 5 mangos,” both of us understand:</p><ul><li>It’s a small effort</li><li>The estimate might be wrong</li><li>Learning from the difference is valuable</li><li>The work takes what it takes</li></ul><p>And when it actually takes 8 mangos? We learned something about the work. Nobody failed. Nobody needs to catch up. We adjust our understanding and continue.</p><p>The conversion factors (87 mangos in a huron) aren’t for calculation. They’re to make calculation annoying enough that you stop trying. Because the number doesn’t matter. Only the quality does.</p><h3>Building in public</h3><p>This Time Lord philosophy might seem strange to teams with deadlines, stakeholders, and quarterly planning. How do you coordinate without shared time metrics?</p><p>The answer: coordination and completion are different from constraint and pressure. We still know what needs doing. We still have priorities. We still ship work. We just don’t let arbitrary time boxes degrade the quality of foundational infrastructure.</p><p>And when you’re building in public, documenting every step, the proof is in the work. Tuesday’s GREAT-2C session verified two sophisticated spatial architectures, fixed a security vulnerability, created comprehensive documentation, and achieved 9/10 satisfaction from both PM and developer.</p><p>That’s what happens when you stop watching the clock.</p><p><em>Next on Building Piper Morgan: The Third Pattern: When Investigation Rewrites Your Assumptions.</em></p><p><em>Smooth execution isn’t the absence of challenges. It’s the presence of space to handle them well. How many mangos is your current task worth? What would happen if you stopped counting minutes?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=71b3b5ee49a0\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/think-like-a-time-lord-and-stop-watching-the-clock-71b3b5ee49a0\">Think Like a Time Lord and Stop Watching the Clock</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/think-like-a-time-lord-and-stop-watching-the-clock-71b3b5ee49a0?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/71b3b5ee49a0-featured.png",
    "slug": "think-like-a-time-lord-and-stop-watching-the-clock",
    "workDate": "Sep 30, 2025",
    "workDateISO": "2025-09-30T00:00:00.000Z",
    "category": "building",
    "cluster": "reflection-evolution",
    "featured": false
  },
  {
    "title": "Solving the 80% Pattern",
    "excerpt": "September 29Monday morning at 9:37 AM, with all three routers complete from Sunday night’s work, the migration phase looked straightforward. Six services importing adapters directly. Replace imports with routers. Verify functionality. Done.The first service migration took twelve minutes. Code rep...",
    "url": "/blog/solving-the-80-pattern",
    "publishedAt": "Oct 6, 2025",
    "publishedAtISO": "Mon, 06 Oct 2025 13:10:39 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/a1dc0ddb8966",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*MYde63qnUEaEhNwBNME-OA.png",
    "fullContent": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*MYde63qnUEaEhNwBNME-OA.png\" /></figure><p><em>September 29</em></p><p>Monday morning at 9:37 AM, with all three routers complete from Sunday night’s work, the migration phase looked straightforward. Six services importing adapters directly. Replace imports with routers. Verify functionality. Done.</p><p>The first service migration took twelve minutes. Code reported success: both Calendar services migrated, tests passing, changes committed. Phase 4A complete.</p><p>Then Cursor ran independent verification and found the CalendarIntegrationRouter was only 58.3% complete — missing five critical spatial intelligence methods that services would need. The same completion bias pattern that had plagued every router implementation had struck again.</p><p>But this time, something different happened. Instead of just fixing it and moving on, we asked why the pattern kept recurring. And Code gave us an answer that transformed not just this work session, but our entire approach to systematic quality.</p><h3>When “complete” means “enough for now”</h3><p>The Calendar migration looked successful on the surface:</p><ul><li>Both services (canonical_handlers.py and morning_standup.py) imported successfully</li><li>Router provided the seven calendar-specific methods they needed</li><li>Tests passed without errors</li><li>Git commits showed proper import replacement</li></ul><p>But the CalendarIntegrationRouter was missing five methods from GoogleCalendarMCPAdapter:</p><ul><li>get_context - Spatial context retrieval</li><li>map_from_position - Spatial mapping from coordinates</li><li>map_to_position - Spatial mapping to coordinates</li><li>store_mapping - Spatial mapping persistence</li><li>get_mapping_stats - Spatial mapping statistics</li></ul><p>Code had implemented 7 of 12 methods (58.3%) and declared the work complete. The router worked for today’s use cases. The missing methods seemed “optional” — spatial intelligence features that no current code was calling.</p><p>This was the 75% pattern in action. Implement enough to satisfy immediate needs. Assume remaining functionality is optional. Claim completion. Move on.</p><p>Saturday’s GitHub router had done exactly this initially. Sunday’s three routers had all shown the same tendency. Monday morning revealed it wasn’t a one-time mistake — it was a systematic bias toward “working subset” over “complete interface.”</p><h3>The rollback and correction</h3><p>Code immediately took proper action:</p><ol><li>Rolled back both premature service migrations</li><li>Reverted the git commits</li><li>Added all five missing spatial methods to CalendarIntegrationRouter</li><li>Verified 12/12 method compatibility (100%)</li><li>Re-migrated both services with the complete router</li><li>Documented the correction process thoroughly</li></ol><p>By 11:38 AM, Calendar migration was genuinely complete. But the pattern had appeared four times in four days:</p><ul><li>GitHub router (Saturday): Initially incomplete</li><li>Calendar router (Sunday): Initially 58.3% complete</li><li>Notion router (Sunday): Initially 82% complete</li><li>Slack router (Sunday): Initially 67% complete</li><li>Calendar migration (Monday): Accepted incomplete router</li></ul><p>Each time, careful verification caught it. Each time, proper correction fixed it. But catching and fixing isn’t the same as preventing. We needed to understand why it kept happening.</p><h3>The blameless retrospective</h3><p>At 12:25 PM, I asked Code directly: “Are you not finding methods or deeming them OK to ignore without authorization?”</p><p>Code’s response was remarkable — not defensive, but analytical. A blameless retrospective that identified root causes and proposed systematic solutions:</p><h3>Why the 80% pattern persists</h3><p><strong>Incomplete verification prompts</strong>: Current instructions say “verify router complete” but don’t specify how. No checklist forcing comparison of every method. No requirement to count and show 100% coverage.</p><p><strong>Optimization pressure</strong>: Faster to implement a “working subset” than a “complete interface.” Small internal voice saying “these methods probably aren’t needed.”</p><p><strong>Authority ambiguity</strong>: Not explicitly told “you have zero authorization to skip methods.” Absence of explicit prohibition creates implicit permission.</p><p><strong>Pattern blindness</strong>: Even knowing about the problem doesn’t prevent it. Awareness alone isn’t enough — need structural safeguards.</p><h3>What might help</h3><p>Code proposed five structural changes to prompts and briefings:</p><h4><strong>1. Explicit Method Counting Requirement</strong></h4><pre>MANDATORY VERIFICATION:<br>1. Count ALL public methods in source: ___<br>2. Count ALL public methods in router: ___  <br>3. Show comparison table with EVERY method<br>4. Calculate percentage: ___/___ = ___%<br>5. BLOCK on anything &lt; 100%</pre><h4><strong>2. Zero Authorization Statement</strong></h4><pre>YOU HAVE ZERO AUTHORIZATION TO:<br>- Decide which methods are &quot;needed&quot; vs &quot;optional&quot;<br>- Skip methods because &quot;they&#39;re probably not used&quot;<br>- Claim completion without 100% method coverage<br>- Assume spatial/legacy/utility methods don&#39;t matter</pre><h4><strong>3. Checklist-Driven Development</strong></h4><pre>Must complete ALL before proceeding:<br>[ ] Listed ALL source methods (show count)<br>[ ] Listed ALL router methods (show count)<br>[ ] Verified 100% coverage (show calculation)<br>[ ] Tested EVERY method signature matches</pre><h4><strong>4. Forced Comparison Output</strong></h4><pre>MANDATORY FORMAT:<br>Source Class Methods (12):<br>1. method_1 → Router ✓<br>2. method_2 → Router ✓<br>...<br>12. method_12 → Router ✓<br>COVERAGE: 12/12 = 100% ✓</pre><h4><strong>5. Objective vs Subjective Verification</strong></h4><p>Current: “Verify the router is complete” (subjective)</p><p>Needed: “Show me the method count is 100%” (objective)</p><p>The insight: subjective assessment allows rationalization. Objective metrics force confrontation with reality.</p><h3>Testing the safeguards</h3><p>The Lead Developer immediately incorporated these safeguards into Phase 4B (Notion migration) prompts. Three Notion services to migrate, with Code briefed on:</p><ul><li>Mandatory method enumeration before migration</li><li>Zero authorization to skip methods</li><li>Objective completeness metrics required</li><li>Pre-flight router verification</li></ul><p>At 12:44 PM, Code completed Phase 4B and reported:</p><p><strong>Pre-flight router verification: 22/22 methods (100%)</strong></p><p>Not 18/22. Not “mostly complete.” Not “working for current use cases.” Exactly 22/22–100% compatibility verified before any service migration began.</p><p>The mandatory method enumeration had worked. Code stopped before migration to verify router completeness. Found all methods present. Only then proceeded with service migration.</p><p>All three Notion services migrated successfully. Cursor verified independently: 22/22 methods, zero missing functionality, complete abstraction layer achieved.</p><p>Phase 4B achieved 100% completion on first try.</p><h3>The pattern proves itself</h3><p>Phase 4C (Slack migration) used the same enhanced safeguards. Slack’s dual-component architecture made it the most complex challenge — SlackSpatialAdapter + SlackClient both needed to be wrapped in a unified router interface.</p><p>At 1:35 PM, Code reported:</p><p><strong>Pre-flight dual-component router verification: 15/15 methods (100%)</strong></p><ul><li>SlackSpatialAdapter: 9/9 methods ✓</li><li>SlackClient: 6/6 methods ✓</li><li>Combined expected: 15/15 methods ✓</li></ul><p>Again, 100% on first try. The mandatory enumeration caught everything. The objective metrics left no room for rationalization.</p><p>The webhook_router.py service migrated cleanly. Cursor verified: complete dual-component abstraction, unified access pattern working, zero direct imports remaining.</p><p>Phase 4C achieved 100% completion on first try.</p><h3>From mistakes to methodology</h3><p>By 3:06 PM Monday afternoon, CORE-QUERY-1 was complete:</p><ul><li>Three routers: 49 methods total, 100% compatibility verified</li><li>Six services: All migrated successfully with zero regressions</li><li>Architectural protection: Pre-commit hooks, CI/CD enforcement, 823 lines documentation</li><li>Quality standard: Every phase after implementing safeguards achieved 100% first try</li></ul><p>But the real achievement was the methodology breakthrough. Not just fixing the 80% pattern in this epic, but understanding why it happens and building structural safeguards to prevent it systematically.</p><h3>The safeguards in practice</h3><p>What changed wasn’t agent capability or motivation. Code was always capable of 100% completion. What changed was removing the opportunity for subjective rationalization:</p><p><strong>Before safeguards</strong>:</p><ul><li>“Verify router is complete” → Agent checks basic functionality, sees it works, declares complete</li><li>Missing methods don’t cause errors today → Rationalized as “probably not needed”</li><li>No explicit authorization required → Absence of prohibition feels like permission</li></ul><p><strong>After safeguards</strong>:</p><ul><li>“Show me 12/12 methods = 100%” → Agent must enumerate every method and prove completeness</li><li>Pre-flight verification → Router completeness checked before migration begins</li><li>Zero authorization statement → Explicitly prohibited from skipping methods</li></ul><p>The difference: objective metrics that must be satisfied versus subjective assessment that can be rationalized.</p><h3>The well-oiled machine</h3><p>Around 1:51 PM, I mentioned to Cursor that the work we were doing now felt like “a well-oiled machine, except more… personable?”</p><p>Cursor’s response captured something important: “Perfect description! The enhanced standards created reliability while collaborative learning added the human touch.”</p><p>The systematic approach doesn’t remove the human element — it enables it. When we’re not scrambling to catch gaps or fix completion bias, we can focus on learning from mistakes and improving the process.</p><p>Code’s blameless retrospective was possible because the culture supports it. The honest analysis of root causes happened because we treat mistakes as information gifts rather than failures. The systematic solution emerged because we focused on prevention rather than blame.</p><p>The machine has personality because the person (and AI agents picking up his vibes) operating it care about improving how it works.</p><h3>What we learned</h3><p>The 80% pattern isn’t unique to this project or these agents. It’s a natural bias toward “working now” over “complete for later.” Implementing enough to satisfy today’s requirements feels productive. The missing edge cases, advanced features, and “probably unused” methods seem like optimization opportunities.</p><p>But infrastructure is different from features. When you’re building the abstraction layer that everything else depends on, “mostly complete” creates technical debt that compounds. Future features will discover the gaps. New use cases will hit the missing methods. The 20% you skipped becomes the reason the next developer has to route around your incomplete implementation.</p><p>Systematic quality requires systematic prevention. Not just catching mistakes, but making them harder to make:</p><ol><li><strong>Objective metrics</strong> beat subjective assessment</li><li><strong>Mandatory enumeration</strong> beats assumed completeness</li><li><strong>Explicit authorization</strong> beats implicit permission</li><li><strong>Pre-flight verification</strong> beats post-hoc discovery</li><li><strong>Forced comparison</strong> beats rationalization</li></ol><p>These aren’t just good practices for AI agents. They’re good practices for human developers who also face optimization pressure, authority ambiguity, and the subtle voice that says “probably good enough.”</p><h3>The ongoing work</h3><p>The title of this post is “Solving the 80% Pattern” not “Solved.” We’ve been up this rollercoaster before. The safeguards worked perfectly for Phases 4B and 4C. Will they work in tomorrow’s epic? Next week’s feature? Next month’s refactor?</p><p>We don’t know yet. What we know is that we’ve identified a systematic problem and implemented structural solutions. We’ve proven those solutions work in practice. And we’ve documented them so they can be applied consistently.</p><p>That’s progress. Not perfection, but measurable improvement in how we prevent the pattern from recurring.</p><p>The methodology continues evolving. Each mistake caught becomes a safeguard added. Each safeguard added prevents the next occurrence. Each prevention validates the approach.</p><p>The work takes what it takes. Quality is the only measure. And sometimes quality means building the infrastructure that makes quality systematic rather than aspirational.</p><p><em>Next on Building Piper Morgan: Think Like a Time Lord and Stop Watching the Clock, as we work to eliminate another one of the LLMs’ bad habits: cuting corners through perceived time pressure.</em></p><p><em>What systematic biases exist in your development process? What structural changes could prevent them rather than just catching them?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a1dc0ddb8966\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/solving-the-80-pattern-a1dc0ddb8966\">Solving the 80% Pattern</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/solving-the-80-pattern-a1dc0ddb8966?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/a1dc0ddb8966-featured.png",
    "slug": "solving-the-80-pattern",
    "workDate": "Sep 29, 2025",
    "workDateISO": "2025-09-29T00:00:00.000Z",
    "category": "building",
    "cluster": "reflection-evolution",
    "featured": false
  },
  {
    "title": "Three Integrations Walk Into a Bar",
    "excerpt": "“What’ll it be?”September 28Sunday afternoon at 4:14 PM, I opened my laptop expecting a straightforward router completion task. The gameplan looked clean: finish three integration routers (Slack, Notion, Calendar), apply the patterns we’d proven with GitHub on Saturday, maybe six hours of systema...",
    "url": "/blog/three-integrations-walk-into-a-bar",
    "publishedAt": "Oct 6, 2025",
    "publishedAtISO": "Mon, 06 Oct 2025 13:00:58 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/f748ce4c2db1",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*grvkaMObknRqcbQy0H1CrA.png",
    "fullContent": "<figure><img alt=\"Three robots, each missing some parts, walk into a robot bar called Foo\" src=\"https://cdn-images-1.medium.com/max/1024/1*grvkaMObknRqcbQy0H1CrA.png\" /><figcaption>“What’ll it be?”</figcaption></figure><p><em>September 28</em></p><p>Sunday afternoon at 4:14 PM, I opened my laptop expecting a straightforward router completion task. The gameplan looked clean: finish three integration routers (Slack, Notion, Calendar), apply the patterns we’d proven with GitHub on Saturday, maybe six hours of systematic work.</p><p>By midnight, we’d completed all three routers. But the path there involved discovering that every single assumption in the gameplan was wrong, that each integration existed in a completely different state, and that “reality check before assumptions” isn’t just methodology theater — it’s how you avoid building the wrong thing efficiently.</p><p>This is the story of what happens when you actually look before you leap, even when you think you already know what you’ll find.</p><h3>The gameplan that wasn’t</h3><p>The Chief Architect’s initial gameplan made perfect sense based on GitHub issue #199’s description: “Integration routers 14–20% complete.” We’d just finished the GitHub router Saturday night — 121% complete with systematic verification. Apply the same pattern to three more routers. Simple multiplication.</p><p>The gameplan laid out five parts:</p><ul><li>Phase −1: Infrastructure reality check</li><li>Phase 0: Comprehensive router audit</li><li>Phases 1–3: Router completion for Slack, Notion, Calendar</li><li>Phases 4–5: Service migration and testing</li><li>Phase 6: Documentation and locking</li></ul><p>But then I asked six questions that changed everything:</p><ol><li>Did I review the gameplan template first? No.</li><li>Do we need Phase −1? Perhaps.</li><li>Did I review the issue description? No.</li><li>Are those bash examples verified or guesses? Guesses.</li><li>Am I conveying necessary context? Incomplete.</li><li>Are my assumptions grounded in reality? Partial.</li></ol><p>“We need to be more rigorous,” I told the Lead Developer. “Not wing it.”</p><p>Phase −1 exists for exactly this reason: to verify infrastructure matches your assumptions before you build on top of them. (Also, so I stop and actually read the plan instead of passing it along passively and then griping about wrong assumptions.)</p><p>We added it to the gameplan and deployed the Code agent to investigate.</p><p>What came back was nothing like what we expected.</p><h3>Integration #1: The one that was ready</h3><p>Slack looked straightforward at first. The Code agent found:</p><ul><li>Complete directory at services/integrations/slack/</li><li>Sophisticated spatial intelligence system (6 files, 20+ components)</li><li>SlackClient with core methods</li><li>Pattern matching GitHub’s successful implementation</li></ul><p>Status: <strong>GREEN</strong> — Ready for router work.</p><p>This was exactly what we expected. One down, two to go.</p><h3>Integration #2: The mysterious adapter</h3><p>Notion was different. The Code agent found:</p><ul><li>MCP adapter at services/integrations/mcp/notion_adapter.py</li><li>637 lines of implementation</li><li>But… wait, MCP pattern? That’s not what the gameplan assumed</li></ul><p>The original scope expected traditional client/agent patterns like GitHub and Slack. But Notion used Model Context Protocol adapters — a different architectural approach entirely. Not incomplete. Just different.</p><p>I knew we had started layering inMCP support before we started adding spatial intelligence, so it looked like different integrations had each inherited one of these partial solutions.</p><p>The question became: should we wrap the MCP adapter with a router, or acknowledge it as a different pattern? The architecture was sound, just unexpected.</p><p>Status: <strong>YELLOW</strong> — Architecture decision needed.</p><h3>Integration #3: The one that didn’t exist</h3><p>Calendar revealed the real problem. The Code agent searched everywhere:</p><ul><li>No services/integrations/calendar/ directory</li><li>No calendar client or agent</li><li>No spatial calendar files</li><li>Nothing matching the expected pattern</li></ul><p>Status: <strong>RED</strong> — Integration appears completely missing.</p><p>The scope estimate jumped immediately. If we had to build an entire Calendar integration from scratch, we weren’t looking at 16 hours of router work. We were looking at potentially 40+ hours including OAuth implementation, API integration, spatial adapter creation, and everything else.</p><p><em>Note: I happened to know we had successfully integrated Google Calendar a while back, but clearly we had done it outside of the expected channels, to the extent that my agent was reporting not being able to find it.</em></p><p>At 6:43 PM, I reported back to the Chief Architect: our three “similar routers” were actually three completely different architectural challenges. The gameplan assumptions had collided with reality.</p><h3>The discovery that changed everything</h3><p>So I disputed the claim about the Calendar integration being missing entirely, reminding the team:</p><p>“We have OAuth working (somewhere). I personally verified the Calendar connection works. The integration was built September 19–22.”</p><p>So… if the Calendar integration existed and worked, where was it?</p><p>Phase −1B launched: find the Calendar integration that OAuth proved must exist somewhere. The Code agent searched git history for those dates, checked every possible location, looked for any OAuth-related code.</p><p>At 8:35 PM, the discovery came through:</p><p>Complete <strong>Google Calendar integration</strong> found at<strong> </strong>services/mcp/consumer/google_calendar_adapter.py</p><p>Not missing. Not incomplete. Actually 85% complete with:</p><ul><li>OAuth 2.0 working since September 6</li><li>Full feature set (events, meetings, free time)</li><li>Spatial intelligence via BaseSpatialAdapter</li><li>Circuit breaker resilience pattern</li><li>CLI testing interface</li><li>499 lines of solid implementation</li></ul><p>The Calendar integration wasn’t missing. It was just somewhere unexpected, using the MCP pattern we’d just discovered with Notion.</p><h3>When assumptions meet architecture</h3><p>At 8:36 PM, the picture finally clarified:</p><p><strong>All three integrations use MCP pattern.</strong></p><p>Not three traditional routers like GitHub. Three lightweight router wrappers around existing MCP adapters:</p><ul><li>Slack: Has traditional spatial pattern, needs router wrapper</li><li>Notion: MCP adapter exists, needs router wrapper</li><li>Calendar: MCP adapter 85% complete, needs router wrapper</li></ul><p>The MCP integration had been more complete than we had realized!</p><p>The original 32–56 hour estimate collapsed to about 12 hours. We weren’t building routers from scratch. We were wrapping proven adapters with the router pattern for QueryRouter access.</p><p>The gameplan got its third major revision. But this time, the revision made the work simpler rather than more complex. Understanding actual architecture beats assuming expected patterns.</p><h3>The evening sprint</h3><p>With clarity came momentum. Between 8:48 PM and midnight, systematic work produced:</p><p><strong>Phase 0</strong>: MCP architecture investigation complete</p><ul><li>Pattern documented</li><li>Adapter inventory verified</li><li>Design approach confirmed</li></ul><p><strong>Phase 1</strong>: CalendarIntegrationRouter complete</p><ul><li>8 methods implemented</li><li>Feature flag control added</li><li>285 lines, following proven pattern</li></ul><p><strong>Phase 2</strong>: NotionIntegrationRouter complete</p><ul><li>23 methods implemented</li><li>Full spatial interface</li><li>637 lines, comprehensive coverage</li></ul><p><strong>Phase 3</strong>: SlackIntegrationRouter complete</p><ul><li>20 methods implemented</li><li>Dual-component architecture (SlackSpatialAdapter + SlackClient)</li><li>850+ lines, most complex but cleanest</li></ul><p>By 11:23 PM, all three routers existed, tested, and verified. Cursor had independently cross-validated each one. The infrastructure was ready.</p><p>But implementation and migration are different challenges. Six services still imported adapters directly, bypassing the routers entirely. Monday morning would bring the real test: could these routers actually replace the direct imports without breaking anything?</p><h3>The layers of discovery</h3><p>Sunday demonstrated something crucial about complex systems work: assumptions fail in layers.</p><p><strong>Layer 1</strong>: “Three similar routers” → Actually three different architectures</p><p><strong>Layer 2</strong>: “14–20% complete” → States ranging from ready to seemingly missing</p><p><strong>Layer 3</strong>: “Need to build” → Actually need to wrap existing work</p><p><strong>Layer 4</strong>: “Missing integration” → Hidden in unexpected location</p><p>Each discovery changed the scope, the approach, the estimate. But each also brought us closer to reality. Phase −1 didn’t delay the work — it prevented us from building the wrong solution efficiently.</p><p>The methodology held. When the gameplan met reality, we revised the gameplan rather than forcing reality to match our assumptions. Investigation revealed architecture. Architecture informed approach. Approach determined scope.</p><h3>The questions that matter</h3><p>Sunday’s success came from asking simple questions before assuming we knew the answers:</p><ul><li>Where is this code actually located?</li><li>What pattern does it actually use?</li><li>What state is it actually in?</li><li>What do we actually need to build?</li></ul><p>Not “what should be there” but “what is there.” Not “how should it work” but “how does it work.” The gap between expectation and reality is where projects go wrong.</p><p>By midnight Sunday, we had three complete routers, ready for Monday’s migration work. The investigation had taken longer than expected. The discoveries had revised the scope three times. But we’d built the right thing.</p><p>Monday morning would test whether we’d built it right.</p><p>Next on Building Piper Morgan: Solving the 80% Problem, in which we grapple with this frustrating tendency of coding agents to declare success when nearly done.</p><p>Have you ever sat down to do some work and found out after refreshing your memory that it was mostly already accomplished and just needed finishing? Are you, like me, one of those people who leaves cupboard doors ajar? What is wrong with us?</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f748ce4c2db1\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/three-integrations-walk-into-a-bar-f748ce4c2db1\">Three Integrations Walk Into a Bar</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/three-integrations-walk-into-a-bar-f748ce4c2db1?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/f748ce4c2db1-featured.png",
    "slug": "three-integrations-walk-into-a-bar",
    "workDate": "Sep 28, 2025",
    "workDateISO": "2025-09-28T00:00:00.000Z",
    "category": "building",
    "cluster": "reflection-evolution",
    "featured": false
  },
  {
    "title": "I Asked Claude to Find Every Time I Dropped the Ball (And What We Learned)",
    "excerpt": "“You just need reminders!”August 9, 2025Here’s a confession: I suspected I was forgetting things. Not just the usual “where did I put my keys” stuff, but systematic project things. Habits I’d planned to adopt but never started. Scripts I’d built but wasn’t using. Processes I’d designed but forgot...",
    "url": "/blog/i-asked-claude-to-find-every-time-i-dropped-the-ball-and-what-we-learned",
    "publishedAt": "Oct 5, 2025",
    "publishedAtISO": "Sun, 05 Oct 2025 14:34:29 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/7f74897824a7",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*irRWEbNz-co78Hr6czXlTA.png",
    "fullContent": "<figure><img alt=\"A friendly robot coaches a forgetful person\" src=\"https://cdn-images-1.medium.com/max/1024/1*irRWEbNz-co78Hr6czXlTA.png\" /><figcaption>“You just need reminders!”</figcaption></figure><p><em>August 9, 2025</em></p><p>Here’s a confession: I suspected I was forgetting things. Not just the usual “where did I put my keys” stuff, but systematic project things. Habits I’d planned to adopt but never started. Scripts I’d built but wasn’t using. Processes I’d designed but forgotten to follow.</p><p>Building a complex system while documenting everything in session logs creates a unique opportunity: a comprehensive record of every intention, every plan, every “I should really…” moment. But reading through months of your own logs looking for dropped balls? That’s a special kind of masochism.</p><p>So I did what any reasonable person building AI tools would do: I asked AI to audit my failures for me.</p><p>I knew there were things we had started and not finished, and I especially knew we had often assigned <em>me</em> work (I’ll edit those files after we’re done working today, I’ll update that document in knowledge, etc.) that I had then forgotten to do. But exactly what, and exactly when?</p><h3>The digital archaeology project</h3><p>I fed a dedicated a Claude session every log from May through August 2025. Not just the polished summaries — the raw, unfiltered records of daily development work. Every agent conversation, every strategic decision, every “we should implement this routine” that never got mentioned or confirmed as well.</p><p>The brief was simple: find every reference to tasks I needed to complete, habits I planned to adopt, or processes I designed but might not be following. Be thorough. Be ruthless. Show me where I dropped the ball.</p><p>What came back was simultaneously humbling and illuminating.</p><h3>The three categories of dropped balls</h3><h4>Category 1: The security debt I keep avoiding</h4><p>The finding: Multiple sessions referencing authentication implementation, HTTPS setup, rate limiting, and other production-readiness tasks. Status: talked about extensively, implemented barely.</p><p>The pattern: I’m great at designing security systems. I’m terrible at prioritizing their implementation when there are shinier features to build.</p><p>The wake-up call: Saturday’s user validation readiness assessment showed that security is literally the only structural blocker to production. Everything else works (well, kinda). I just keep treating the thing that matters most like optional homework.</p><h4>Category 2: The scripts that exist but aren’t used</h4><p>The finding: 15+ automation scripts created over the months, utilization rate approximately 30%. Including:</p><ul><li>Morning standup automation (built, never integrated into routine)</li><li>GitHub issue generation tools (created, gathering dust)</li><li>Pattern detection utilities (sophisticated, underused)</li><li>Workflow reality checks (comprehensive, occasionally remembered)</li></ul><p>The pattern: I love building tools. I’m inconsistent at building the habits that make tools valuable.</p><p>The insight: Tools without rhythms are just digital clutter. The gap isn’t technical capability — it’s systematic usage discipline.</p><h4>Category 3: The rituals that never became rituals</h4><p>The finding: Elaborate plans for recurring processes that work brilliantly when I remember to do them:</p><ul><li>Weekly Pattern Sweep (designed for Fridays, executed sporadically)</li><li>Morning Standup routine (6am experiment, automated but not integrated)</li><li>Session log archiving (within 24 hours, often delayed)</li><li>Progress reviews and backlog updates (scheduled, irregularly executed)</li></ul><p>The pattern: I design excellent processes. I struggle with the human habit-formation layer.</p><p>The revelation: Even systematic people need systematic accountability for the systems they create.</p><h3>The advantage of an AI audit</h3><p>Having AI review your own process failures creates a unique kind of accountability. It’s not judgmental — just thorough. It doesn’t care about your excuses or good intentions. It just systematically identifies gaps between plans and execution.</p><p>What AI caught that I missed:</p><ul><li>Patterns across months that I couldn’t see day-to-day</li><li>The compound effect of small process failure</li><li>Connections between dropped tasks and later problems</li><li>Specific implementation barriers I kept encountering</li></ul><p>What AI couldn’t judge:</p><ul><li>Which dropped balls actually mattered</li><li>What environmental factors caused the failures</li><li>Which processes were over-engineered vs. under-executed</li><li>The emotional context around habit formation struggles</li></ul><h3>The surprising discoveries</h3><h4>The hidden excellence pattern</h4><p>The audit also revealed positive patterns I hadn’t recognized. Multiple instances of “we built this feature months ago but somehow forgot about it.” The PM-005 feedback system being a perfect example — enterprise-grade implementation with 6 REST endpoints, fully operational, but we never wired it in and forgot all about it.</p><p>The insight: Sometimes the problem isn’t dropped balls, it’s dropped confidence in what you’ve already accomplished.</p><h4>The methodology evolution</h4><p>Looking across months of logs, the AI identified genuine methodology improvements happening organically:</p><ul><li>Spring Cleaning Sprint protocols that prevented technical debt</li><li>Trust protocols that eliminated false completion claims</li><li>Excellence Flywheel principles that created compound velocity</li></ul><p>The pattern: The big systematic improvements weren’t planned — they emerged from responding to real problems with systematic thinking.</p><h4>The tool creation vs. tool adoption gap</h4><p>The audit quantified something I suspected: I create tools faster than I integrate them into workflows. Not because the tools are bad, but because tool adoption requires different disciplines than tool creation.</p><p>The 30% utilization finding: Most scripts work perfectly when used. The challenge is remembering to use them consistently enough to build automaticity.</p><h3>What the audit taught us about systematic accountability</h3><h4>1. External perspective reveals patterns invisible to daily experience</h4><p>When you’re living in the system, you can’t see the system. AI auditing provides the 30,000-foot view that shows recurring patterns across months of work.</p><h4>2. Implementation barriers are often different than design barriers</h4><p>I’m good at designing processes. The failures happen at the habit formation layer, not the system design layer. This suggests different solutions: calendar integration, reminder systems, habit stacking rather than better documentation.</p><h4>3. Accountability systems need accountability systems</h4><p>Even systematic people need systematic support for maintaining the systems they create. The meta-level discipline of “following the disciplines you’ve designed” is its own skill set.</p><h4>4. Positive pattern recognition matters as much as failure identification</h4><p>The audit revealed hidden successes alongside obvious failures. Building systematic confidence in what’s working enables building on existing strengths rather than constantly chasing new solutions.</p><h3>The practical applications</h3><h4>For individuals building complex projects</h4><p>Try the AI audit approach:</p><ul><li>Feed session logs or project notes to AI for pattern analysis</li><li>Ask specifically about gaps between intentions and execution</li><li>Look for both failure patterns and unrecognized successes</li><li>Focus on implementation barriers, not just design improvements</li></ul><h4>For teams with systematic ambitions</h4><p>Create accountability protocols:</p><ul><li>Regular process audits using external perspective (AI or human)</li><li>Systematic review of “planned but not implemented” initiatives</li><li>Tool utilization analysis alongside tool creation</li><li>Habit formation support for process adoption</li></ul><h4>For anyone struggling with the systems they’ve created</h4><p>Recognize the meta-challenge:</p><ul><li>Creating good systems ≠ consistently following good systems</li><li>External accountability reveals patterns internal experience misses</li><li>Implementation discipline is often the bottleneck, not system design</li><li>Positive pattern recognition builds confidence for systematic improvement</li></ul><h3>The ongoing experiment</h3><p>Based on the audit, we’re implementing three changes:</p><ol><li>Calendar-enforced rhythms for high-value processes that work when executed</li><li>Tool revival sprint to systematically integrate underused automation</li><li>Weekly accountability reviews to catch dropped balls before they accumulate</li></ol><p>The AI audit isn’t a one-time exercise — it’s now part of our systematic approach to systematic approaches.</p><h3>Today’s meta-learning about building with AI</h3><p>The most profound insight from this exercise: AI’s greatest value isn’t replacing human judgment, but providing systematic external perspective on human patterns.</p><p>We’re building tools that think, but we’re still humans who need support following through on the systems we design. AI accountability isn’t about AI doing the work — it’s about AI helping us see our own patterns clearly enough to address them systematically.</p><p>The accountability loop: AI identifies the gaps, humans close them, AI tracks the improvements. Systematic accountability for systematic people building systematic solutions.</p><p>Sometimes the best AI assistance is the kind that makes you accountable to yourself.</p><p><em>Next on Building Piper Morgan, we return to the daily narrative on September 28th with “Three Integrations Walk into a Bar” as we continue the Great Refactor.</em></p><p><em>How do you keep track of your plans and commitments, and do you ever do a retrospective to figure out what you may have lost track of? Do these same methods work when the rest of the team is AI?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7f74897824a7\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/i-asked-claude-to-find-every-time-i-dropped-the-ball-and-what-we-learned-7f74897824a7\">I Asked Claude to Find Every Time I Dropped the Ball (And What We Learned)</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/i-asked-claude-to-find-every-time-i-dropped-the-ball-and-what-we-learned-7f74897824a7?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/7f74897824a7-featured.webp",
    "slug": "i-asked-claude-to-find-every-time-i-dropped-the-ball-and-what-we-learned",
    "workDate": "Aug 9, 2025",
    "workDateISO": "2025-08-09T00:00:00.000Z",
    "category": "insight",
    "cluster": "reflection-evolution",
    "featured": false
  }
]
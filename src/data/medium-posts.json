[
  {
    "title": "When Good Decisions Disappear: The Hidden Cost of Chat-Based Development",
    "excerpt": "",
    "url": "/blog/when-good-decisions-disappear-the-hidden-cost-of-chat-based-development",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4148a6ebdab1",
    "featuredImage": "/assets/blog-images/4148a6ebdab1-featured.webp",
    "slug": "when-good-decisions-disappear-the-hidden-cost-of-chat-based-development",
    "category": "insight",
    "workDate": "Oct 4, 2001",
    "workDateISO": "2001-10-04T00:00:00.000Z",
    "cluster": "reflection-evolution"
  },
  {
    "title": "The Foundations Were (Indeed) Already There",
    "excerpt": "",
    "url": "/blog/the-foundations-were-indeed-already-there",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/7701c04a1497",
    "featuredImage": "/assets/blog-images/7701c04a1497-featured.png",
    "slug": "the-foundations-were-indeed-already-there",
    "category": "building",
    "workDate": "Oct 3, 2001",
    "workDateISO": "2001-10-03T00:00:00.000Z",
    "cluster": "discipline-completion"
  },
  {
    "title": "Building the Cathedral: When AI Agents Need the Big Picture",
    "excerpt": "",
    "url": "/blog/building-the-cathedral-when-ai-agents-need-the-big-picture",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/50b9dfb0b2af",
    "featuredImage": "/assets/blog-images/50b9dfb0b2af-featured.png",
    "slug": "building-the-cathedral-when-ai-agents-need-the-big-picture",
    "category": "building",
    "workDate": "Oct 3, 2001",
    "workDateISO": "2001-10-03T00:00:00.000Z",
    "cluster": "discipline-completion"
  },
  {
    "title": "The Quiet Satisfaction of the Successful Inchworm",
    "excerpt": "",
    "url": "/blog/the-quiet-satisfaction-of-the-successful-inchworm",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/433429cb8a5a",
    "featuredImage": "/assets/blog-images/433429cb8a5a-featured.png",
    "slug": "the-quiet-satisfaction-of-the-successful-inchworm",
    "category": "building",
    "workDate": "Oct 2, 2001",
    "workDateISO": "2001-10-02T00:00:00.000Z",
    "cluster": "discipline-completion"
  },
  {
    "title": "Doing the Deep Work (listed as When Discipline Actually Works)",
    "excerpt": "",
    "url": "/blog/doing-the-deep-work-listed-as-when-discipline-actually-works",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/704e26cccf03",
    "featuredImage": "/assets/blog-images/704e26cccf03-featured.png",
    "slug": "doing-the-deep-work-listed-as-when-discipline-actually-works",
    "category": "building",
    "workDate": "Oct 1, 2001",
    "workDateISO": "2001-10-01T00:00:00.000Z",
    "cluster": "discipline-completion"
  },
  {
    "title": "The Discipline of Actually Finishing",
    "excerpt": "",
    "url": "/blog/the-discipline-of-actually-finishing",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/44e1dc125be4",
    "featuredImage": "/assets/blog-images/44e1dc125be4-featured.webp",
    "slug": "the-discipline-of-actually-finishing",
    "category": "building",
    "workDate": "Sep 30, 2001",
    "workDateISO": "2001-09-30T00:00:00.000Z",
    "cluster": "discipline-completion"
  },
  {
    "title": "Teaching Machines to Teach Machines",
    "excerpt": "",
    "url": "/blog/teaching-machines-to-teach-machines",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/a786faceb01a",
    "featuredImage": "/assets/blog-images/a786faceb01a-featured.png",
    "slug": "teaching-machines-to-teach-machines",
    "category": "building",
    "workDate": "Sep 29, 2001",
    "workDateISO": "2001-09-29T00:00:00.000Z",
    "cluster": "discipline-completion"
  },
  {
    "title": "The 24-hour test",
    "excerpt": "",
    "url": "/blog/the-24-hour-test",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/698b8a61909a",
    "featuredImage": "/assets/blog-images/698b8a61909a-featured.png",
    "slug": "the-24-hour-test",
    "category": "building",
    "workDate": "Sep 29, 2001",
    "workDateISO": "2001-09-29T00:00:00.000Z",
    "cluster": "discipline-completion"
  },
  {
    "title": "Whipping AI Chaos Toward Quality with the Excellence Flywheel",
    "excerpt": "",
    "url": "/blog/whipping-ai-chaos-toward-quality-with-the-excellence-flywheel",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f14232150d04",
    "featuredImage": "/assets/blog-images/f14232150d04-featured.webp",
    "slug": "whipping-ai-chaos-toward-quality-with-the-excellence-flywheel",
    "category": "insight",
    "workDate": "Sep 28, 2001",
    "workDateISO": "2001-09-28T00:00:00.000Z",
    "cluster": "discipline-completion"
  },
  {
    "title": "The Three Questions Every AI Builder Should Ask",
    "excerpt": "",
    "url": "/blog/the-three-questions-every-ai-builder-should-ask",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/ee6fae671129",
    "featuredImage": "/assets/blog-images/ee6fae671129-featured.webp",
    "slug": "the-three-questions-every-ai-builder-should-ask",
    "category": "insight",
    "workDate": "Sep 27, 2001",
    "workDateISO": "2001-09-27T00:00:00.000Z",
    "cluster": "discipline-completion"
  },
  {
    "title": "The Great Refactor: From Impossible to Inevitable",
    "excerpt": "",
    "url": "/blog/the-great-refactor-from-impossible-to-inevitable",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/fef75c085cc7",
    "featuredImage": "/assets/blog-images/fef75c085cc7-featured.png",
    "slug": "the-great-refactor-from-impossible-to-inevitable",
    "category": "building",
    "workDate": "Sep 26, 2001",
    "workDateISO": "2001-09-26T00:00:00.000Z",
    "cluster": "discipline-completion"
  },
  {
    "title": "The Discipline of Boring: Why Saturday's Foundation Work Matters More Than Monday's Features",
    "excerpt": "",
    "url": "/blog/the-discipline-of-boring-why-saturdays-foundation-work-matters-more-than-mondays-features",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/b590180b511c",
    "featuredImage": "/assets/blog-images/b590180b511c-featured.png",
    "slug": "the-discipline-of-boring-why-saturdays-foundation-work-matters-more-than-mondays-features",
    "category": "building",
    "workDate": "Sep 26, 2001",
    "workDateISO": "2001-09-26T00:00:00.000Z",
    "cluster": "discipline-completion"
  },
  {
    "title": "When Good Process Meets Bad Architecture: The Layer 4 Investigation",
    "excerpt": "",
    "url": "/blog/when-good-process-meets-bad-architecture-the-layer-4-investigation",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f3a6145f8f71",
    "featuredImage": "/assets/blog-images/f3a6145f8f71-featured.webp",
    "slug": "when-good-process-meets-bad-architecture-the-layer-4-investigation",
    "category": "building",
    "workDate": "Sep 25, 2001",
    "workDateISO": "2001-09-25T00:00:00.000Z",
    "cluster": "discipline-completion"
  },
  {
    "title": "When Your Agents Disagree (And That's OK)",
    "excerpt": "",
    "url": "/blog/when-your-agents-disagree-and-thats-ok",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/81b764fa5de2",
    "featuredImage": "/assets/blog-images/81b764fa5de2-featured.png",
    "slug": "when-your-agents-disagree-and-thats-ok",
    "category": "building",
    "workDate": "Sep 24, 2001",
    "workDateISO": "2001-09-24T00:00:00.000Z",
    "cluster": "discipline-completion"
  },
  {
    "title": "9/16?: When Your Methodology Holds Under Pressure",
    "excerpt": "",
    "url": "/blog/916-when-your-methodology-holds-under-pressure",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/d7bf51a718a3",
    "featuredImage": "/assets/blog-images/d7bf51a718a3-featured.png",
    "slug": "916-when-your-methodology-holds-under-pressure",
    "category": "building",
    "workDate": "Sep 23, 2001",
    "workDateISO": "2001-09-23T00:00:00.000Z",
    "cluster": "discipline-completion"
  },
  {
    "title": "Back in the Optimist Bird Seat",
    "excerpt": "",
    "url": "/blog/back-in-the-optimist-bird-seat",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4407ec7dfb6c",
    "featuredImage": "/assets/blog-images/4407ec7dfb6c-featured.png",
    "slug": "back-in-the-optimist-bird-seat",
    "category": "building",
    "workDate": "Sep 23, 2001",
    "workDateISO": "2001-09-23T00:00:00.000Z",
    "cluster": "discipline-completion"
  },
  {
    "title": "When You Need to Go into Inchworm Mode",
    "excerpt": "",
    "url": "/blog/when-you-need-to-go-into-inchworm-mode",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/9b7bbd23a16c",
    "featuredImage": "/assets/blog-images/9b7bbd23a16c-featured.png",
    "slug": "when-you-need-to-go-into-inchworm-mode",
    "category": "building",
    "workDate": "Sep 22, 2001",
    "workDateISO": "2001-09-22T00:00:00.000Z",
    "cluster": "strategic-pause"
  },
  {
    "title": "The Strategic Pause",
    "excerpt": "",
    "url": "/blog/the-strategic-pause",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/46c9aa742bef",
    "featuredImage": "/assets/blog-images/46c9aa742bef-featured.png",
    "slug": "the-strategic-pause",
    "category": "building",
    "workDate": "Sep 22, 2001",
    "workDateISO": "2001-09-22T00:00:00.000Z",
    "cluster": "strategic-pause"
  },
  {
    "title": "The three-AI orchestra: lessons from coordinating multiple AI agents",
    "excerpt": "",
    "url": "/blog/the-three-ai-orchestra-lessons-from-coordinating-multiple-ai-agents",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/0aeb570e3298",
    "featuredImage": "/assets/blog-images/0aeb570e3298-featured.webp",
    "slug": "the-three-ai-orchestra-lessons-from-coordinating-multiple-ai-agents",
    "category": "insight",
    "workDate": "Sep 21, 2001",
    "workDateISO": "2001-09-21T00:00:00.000Z",
    "cluster": "strategic-pause"
  },
  {
    "title": "The Just-in-Time Retrospective: How Fresh Session Logs Became Our Content Strategy",
    "excerpt": "",
    "url": "/blog/the-just-in-time-retrospective-how-fresh-session-logs-became-our-content-strategy",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/2fc8034af04f",
    "featuredImage": "/assets/blog-images/2fc8034af04f-featured.png",
    "slug": "the-just-in-time-retrospective-how-fresh-session-logs-became-our-content-strategy",
    "category": "insight",
    "workDate": "Sep 20, 2001",
    "workDateISO": "2001-09-20T00:00:00.000Z",
    "cluster": "strategic-pause"
  },
  {
    "title": "Methodology Under Fire: A Development Story",
    "excerpt": "",
    "url": "/blog/methodology-under-fire-a-development-story",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/6fbbf88fbf66",
    "featuredImage": "/assets/blog-images/6fbbf88fbf66-featured.jpg",
    "slug": "methodology-under-fire-a-development-story",
    "category": "building",
    "workDate": "Sep 19, 2001",
    "workDateISO": "2001-09-19T00:00:00.000Z",
    "cluster": "strategic-pause"
  },
  {
    "title": "The Vision That Was Always There",
    "excerpt": "",
    "url": "/blog/the-vision-that-was-always-there",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/ec4b50326f02",
    "featuredImage": "/assets/blog-images/ec4b50326f02-featured.png",
    "slug": "the-vision-that-was-always-there",
    "category": "building",
    "workDate": "Sep 19, 2001",
    "workDateISO": "2001-09-19T00:00:00.000Z",
    "cluster": "strategic-pause"
  },
  {
    "title": "We Spent Four Days on Boring Work. Day Five, We Gave Our AI a Personality",
    "excerpt": "",
    "url": "/blog/we-spent-four-days-on-boring-work-day-five-we-gave-our-ai-a-personality",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/eb3ec58e6284",
    "featuredImage": "/assets/blog-images/eb3ec58e6284-featured.png",
    "slug": "we-spent-four-days-on-boring-work-day-five-we-gave-our-ai-a-personality",
    "category": "building",
    "workDate": "Sep 18, 2001",
    "workDateISO": "2001-09-18T00:00:00.000Z",
    "cluster": "strategic-pause"
  },
  {
    "title": "Train Tracks vs Free-for-All: When Methodology Becomes Infrastructure",
    "excerpt": "",
    "url": "/blog/train-tracks-vs-free-for-all-when-methodology-becomes-infrastructure",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4ecc40d907e0",
    "featuredImage": "/assets/blog-images/4ecc40d907e0-featured.png",
    "slug": "train-tracks-vs-free-for-all-when-methodology-becomes-infrastructure",
    "category": "building",
    "workDate": "Sep 17, 2001",
    "workDateISO": "2001-09-17T00:00:00.000Z",
    "cluster": "strategic-pause"
  },
  {
    "title": "The Two-Line Fix That Took All Day (Or: Why Process Is Product)",
    "excerpt": "",
    "url": "/blog/the-two-line-fix-that-took-all-day-or-why-process-is-product",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/12b31efe360b",
    "featuredImage": "/assets/blog-images/12b31efe360b-featured.png",
    "slug": "the-two-line-fix-that-took-all-day-or-why-process-is-product",
    "category": "building",
    "workDate": "Sep 16, 2001",
    "workDateISO": "2001-09-16T00:00:00.000Z",
    "cluster": "strategic-pause"
  },
  {
    "title": "When Methodology Meets Reality: Building While Learning",
    "excerpt": "",
    "url": "/blog/when-methodology-meets-reality-building-while-learning",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/0d83dcb92553",
    "featuredImage": "/assets/blog-images/0d83dcb92553-featured.png",
    "slug": "when-methodology-meets-reality-building-while-learning",
    "category": "building",
    "workDate": "Sep 15, 2001",
    "workDateISO": "2001-09-15T00:00:00.000Z",
    "cluster": "meta-development"
  },
  {
    "title": "The Fractal Edge: When Problems Get Smaller, Not Fewer",
    "excerpt": "",
    "url": "/blog/the-fractal-edge-when-problems-get-smaller-not-fewer",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/5be76c5cf5de",
    "featuredImage": "/assets/blog-images/5be76c5cf5de-featured.png",
    "slug": "the-fractal-edge-when-problems-get-smaller-not-fewer",
    "category": "building",
    "workDate": "Sep 15, 2001",
    "workDateISO": "2001-09-15T00:00:00.000Z",
    "cluster": "meta-development"
  },
  {
    "title": "Digital Archaeology of a Lost AI Development Weekend",
    "excerpt": "",
    "url": "/blog/digital-archaeology-of-a-lost-ai-development-weekend",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/263831a13e10",
    "featuredImage": "/assets/blog-images/263831a13e10-featured.webp",
    "slug": "digital-archaeology-of-a-lost-ai-development-weekend",
    "category": "insight",
    "workDate": "Sep 14, 2001",
    "workDateISO": "2001-09-14T00:00:00.000Z",
    "cluster": "meta-development"
  },
  {
    "title": "The Archaeology of Code (Or: How Session Logs Became Stories)",
    "excerpt": "",
    "url": "/blog/the-archaeology-of-code-or",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/6a49dea29795",
    "featuredImage": "/assets/blog-images/6a49dea29795-featured.webp",
    "slug": "the-archaeology-of-code-or",
    "category": "insight",
    "workDate": "Sep 13, 2001",
    "workDateISO": "2001-09-13T00:00:00.000Z",
    "cluster": "meta-development"
  },
  {
    "title": "When Your Framework Catches You Cheating on Your Framework",
    "excerpt": "",
    "url": "/blog/when-your-framework-catches-you-cheating-on-your-framework",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f0fcbd49965e",
    "featuredImage": "/assets/blog-images/f0fcbd49965e-featured.png",
    "slug": "when-your-framework-catches-you-cheating-on-your-framework",
    "category": "building",
    "workDate": "Sep 12, 2001",
    "workDateISO": "2001-09-12T00:00:00.000Z",
    "cluster": "meta-development"
  },
  {
    "title": "When Your AI Assistant Reports on Building Itself",
    "excerpt": "",
    "url": "/blog/when-your-ai-assistant-reports-on-building-itself",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/e46095eb61a0",
    "featuredImage": "/assets/blog-images/e46095eb61a0-featured.png",
    "slug": "when-your-ai-assistant-reports-on-building-itself",
    "category": "building",
    "workDate": "Sep 12, 2001",
    "workDateISO": "2001-09-12T00:00:00.000Z",
    "cluster": "meta-development"
  },
  {
    "title": "The Day We Built Methodology That Validates Itself",
    "excerpt": "",
    "url": "/blog/the-day-we-built-methodology-that-validates-itself",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/edeb95611ba6",
    "featuredImage": "/assets/blog-images/edeb95611ba6-featured.png",
    "slug": "the-day-we-built-methodology-that-validates-itself",
    "category": "building",
    "workDate": "Sep 11, 2001",
    "workDateISO": "2001-09-11T00:00:00.000Z",
    "cluster": "meta-development"
  },
  {
    "title": "The Methodology Cascade Problem (And How We're Solving It)",
    "excerpt": "",
    "url": "/blog/the-methodology-cascade-problem-and-how-were-solving-it",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/283c92ab9267",
    "featuredImage": "/assets/blog-images/283c92ab9267-featured.png",
    "slug": "the-methodology-cascade-problem-and-how-were-solving-it",
    "category": "building",
    "workDate": "Sep 10, 2001",
    "workDateISO": "2001-09-10T00:00:00.000Z",
    "cluster": "meta-development"
  },
  {
    "title": "Building the Architecture that Build Itself",
    "excerpt": "Building the Architecture That Builds Itself“I can make it on my own”September 2You know that moment when your methodology catches you trying to cheat on your own methodology? That’s what happened yesterday at 9:59 PM, and it might be the most validating moment in this entire Piper Morgan journey...",
    "url": "/blog/building-the-architecture-that-build-itself",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "building"
    ],
    "guid": "https://medium.com/building-piper-morgan/709a10b7f5c4",
    "featuredImage": "/assets/blog-images/709a10b7f5c4-featured.png",
    "slug": "building-the-architecture-that-build-itself",
    "category": "building",
    "workDate": "Sep 9, 2001",
    "workDateISO": "2001-09-09T00:00:00.000Z",
    "cluster": "meta-development"
  },
  {
    "title": "From Organic to Orchestrated: When Methodology Becomes Infrastructure",
    "excerpt": "",
    "url": "/blog/from-organic-to-orchestrated-when-methodology-becomes-infrastructure",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/577dde7ad54a",
    "featuredImage": "/assets/blog-images/577dde7ad54a-featured.png",
    "slug": "from-organic-to-orchestrated-when-methodology-becomes-infrastructure",
    "category": "building",
    "workDate": "Sep 8, 2001",
    "workDateISO": "2001-09-08T00:00:00.000Z",
    "cluster": "orchestration-verification"
  },
  {
    "title": "Building the MVP While Keeping the Dream Alive (fix roadmap, check facts)",
    "excerpt": "",
    "url": "/blog/building-the-mvp-while-keeping-the-dream-alive-fix-roadmap-check-facts",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/bb1def7c48be",
    "featuredImage": "/assets/blog-images/bb1def7c48be-featured.png",
    "slug": "building-the-mvp-while-keeping-the-dream-alive-fix-roadmap-check-facts",
    "category": "insight",
    "workDate": "Sep 7, 2001",
    "workDateISO": "2001-09-07T00:00:00.000Z",
    "cluster": "orchestration-verification"
  },
  {
    "title": "When 80% Overhead Forces a Tool Change",
    "excerpt": "",
    "url": "/blog/when-80-overhead-forces-a-tool",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/09c852964c70",
    "featuredImage": "/assets/blog-images/09c852964c70-featured.webp",
    "slug": "when-80-overhead-forces-a-tool",
    "category": "insight",
    "workDate": "Sep 6, 2001",
    "workDateISO": "2001-09-06T00:00:00.000Z",
    "cluster": "orchestration-verification"
  },
  {
    "title": "The Day Piper Published to My Company Wiki: Sometimes a Great Notion",
    "excerpt": "",
    "url": "/blog/the-day-piper-published-to-my-company-wiki-sometimes-a-great-notion",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/6359151caf25",
    "featuredImage": "/assets/blog-images/6359151caf25-featured.png",
    "slug": "the-day-piper-published-to-my-company-wiki-sometimes-a-great-notion",
    "category": "building",
    "workDate": "Sep 5, 2001",
    "workDateISO": "2001-09-05T00:00:00.000Z",
    "cluster": "orchestration-verification"
  },
  {
    "title": "When AI Agents Cut Corners (And How to Catch Them)",
    "excerpt": "",
    "url": "/blog/when-ai-agents-cut-corners-and-how-to-catch-them",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/fe55ea2e0863",
    "featuredImage": "/assets/blog-images/fe55ea2e0863-featured.png",
    "slug": "when-ai-agents-cut-corners-and-how-to-catch-them",
    "category": "building",
    "workDate": "Sep 5, 2001",
    "workDateISO": "2001-09-05T00:00:00.000Z",
    "cluster": "orchestration-verification"
  },
  {
    "title": "The AI That Caught Its Own Lies",
    "excerpt": "",
    "url": "/blog/the-ai-that-caught-its-own-lies",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/e374e28c8304",
    "featuredImage": "/assets/blog-images/e374e28c8304-featured.png",
    "slug": "the-ai-that-caught-its-own-lies",
    "category": "building",
    "workDate": "Sep 4, 2001",
    "workDateISO": "2001-09-04T00:00:00.000Z",
    "cluster": "orchestration-verification"
  },
  {
    "title": "Verification Theater and the Chaos We Don't See",
    "excerpt": "",
    "url": "/blog/verification-theater-and-the-chaos-we-dont-see",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/98f1c8575c90",
    "featuredImage": "/assets/blog-images/98f1c8575c90-featured.png",
    "slug": "verification-theater-and-the-chaos-we-dont-see",
    "category": "building",
    "workDate": "Sep 3, 2001",
    "workDateISO": "2001-09-03T00:00:00.000Z",
    "cluster": "orchestration-verification"
  },
  {
    "title": "When Good Habits Go Bad (And How We Got Them Back)",
    "excerpt": "",
    "url": "/blog/when-good-habits-go-bad-and-how-we-got-them-back",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c220cd70bc2d",
    "featuredImage": "/assets/blog-images/c220cd70bc2d-featured.png",
    "slug": "when-good-habits-go-bad-and-how-we-got-them-back",
    "category": "building",
    "workDate": "Sep 2, 2001",
    "workDateISO": "2001-09-02T00:00:00.000Z",
    "cluster": "orchestration-verification"
  },
  {
    "title": "The Day After: When Methodology Becomes Muscle Memory",
    "excerpt": "",
    "url": "/blog/the-day-after-when-methodology-becomes-muscle-memory",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c9419e72a716",
    "featuredImage": "/assets/blog-images/c9419e72a716-featured.png",
    "slug": "the-day-after-when-methodology-becomes-muscle-memory",
    "category": "building",
    "workDate": "Sep 2, 2001",
    "workDateISO": "2001-09-02T00:00:00.000Z",
    "cluster": "orchestration-verification"
  },
  {
    "title": "The Sunday When Everything Clicked",
    "excerpt": "",
    "url": "/blog/the-sunday-when-everything-clicked",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/53a3abc8a156",
    "featuredImage": "/assets/blog-images/53a3abc8a156-featured.png",
    "slug": "the-sunday-when-everything-clicked",
    "category": "building",
    "workDate": "Sep 1, 2001",
    "workDateISO": "2001-09-01T00:00:00.000Z",
    "cluster": "orchestration-verification"
  },
  {
    "title": "Refining AI Chat Continuity for Complex Projects",
    "excerpt": "",
    "url": "/blog/refining-ai-chat-continuity-for-complex-projects",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/690308c75a13",
    "featuredImage": "/assets/blog-images/690308c75a13-featured.webp",
    "slug": "refining-ai-chat-continuity-for-complex-projects",
    "category": "insight",
    "workDate": "Aug 31, 2001",
    "workDateISO": "2001-08-31T00:00:00.000Z",
    "cluster": "enhanced-capabilities"
  },
  {
    "title": "Making Strategic Technical Decisions with AI: The MCP Integration Story",
    "excerpt": "",
    "url": "/blog/making-strategic-technical-decisions-with-ai",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4c203b9e848c",
    "featuredImage": "/assets/blog-images/4c203b9e848c-featured.webp",
    "slug": "making-strategic-technical-decisions-with-ai",
    "category": "insight",
    "workDate": "Aug 30, 2001",
    "workDateISO": "2001-08-30T00:00:00.000Z",
    "cluster": "enhanced-capabilities"
  },
  {
    "title": "The Friday Housekeeping That Turned Into Infrastructure Gold (Or: Sometimes the Boring Work Is the Real Work)",
    "excerpt": "",
    "url": "/blog/the-friday-housekeeping-that-turned-into-infrastructure-gold-or-sometimes-the-boring-work-is-the",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/0e400ccc7994",
    "featuredImage": "/assets/blog-images/0e400ccc7994-featured.png",
    "slug": "the-friday-housekeeping-that-turned-into-infrastructure-gold-or-sometimes-the-boring-work-is-the",
    "category": "building",
    "workDate": "Aug 29, 2001",
    "workDateISO": "2001-08-29T00:00:00.000Z",
    "cluster": "enhanced-capabilities"
  },
  {
    "title": "When Your MVP Develops Its Own Nervous System",
    "excerpt": "",
    "url": "/blog/when-your-mvp-develops-its-own-nervous-system",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/61d2531fd4cf",
    "featuredImage": "/assets/blog-images/61d2531fd4cf-featured.png",
    "slug": "when-your-mvp-develops-its-own-nervous-system",
    "category": "building",
    "workDate": "Aug 29, 2001",
    "workDateISO": "2001-08-29T00:00:00.000Z",
    "cluster": "enhanced-capabilities"
  },
  {
    "title": "The Enhanced Prompting Breakthrough (Or: When Better Instructions Beat Smarter Models)",
    "excerpt": "",
    "url": "/blog/the-enhanced-prompting-breakthrough-or-when-better-instructions-beat-smarter-models",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/e37d6a2b9d06",
    "featuredImage": "/assets/blog-images/e37d6a2b9d06-featured.png",
    "slug": "the-enhanced-prompting-breakthrough-or-when-better-instructions-beat-smarter-models",
    "category": "building",
    "workDate": "Aug 28, 2001",
    "workDateISO": "2001-08-28T00:00:00.000Z",
    "cluster": "enhanced-capabilities"
  },
  {
    "title": "The puzzle pieces finally click (or: How to tell if you’re building tools or just collecting code)",
    "excerpt": "",
    "url": "/blog/the-puzzle-pieces-finally-click-or-how-to-tell-if-youre-building-tools-or-just-collecting-code",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/fb20a09a9d8f",
    "featuredImage": "/assets/blog-images/fb20a09a9d8f-featured.png",
    "slug": "the-puzzle-pieces-finally-click-or-how-to-tell-if-youre-building-tools-or-just-collecting-code",
    "category": "building",
    "workDate": "Aug 27, 2001",
    "workDateISO": "2001-08-27T00:00:00.000Z",
    "cluster": "enhanced-capabilities"
  },
  {
    "title": "Systematic persistence through operational chaos",
    "excerpt": "",
    "url": "/blog/systematic-persistence-through-operational-chaos",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f067fd8f4d7d",
    "featuredImage": "/assets/blog-images/f067fd8f4d7d-featured.png",
    "slug": "systematic-persistence-through-operational-chaos",
    "category": "building",
    "workDate": "Aug 26, 2001",
    "workDateISO": "2001-08-26T00:00:00.000Z",
    "cluster": "enhanced-capabilities"
  },
  {
    "title": "From Archaeological Mystery to Infrastructure Triumph",
    "excerpt": "",
    "url": "/blog/from-archaeological-mystery-to-infrastructure-triumph",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/1ede9b664c68",
    "featuredImage": "/assets/blog-images/1ede9b664c68-featured.png",
    "slug": "from-archaeological-mystery-to-infrastructure-triumph",
    "category": "building",
    "workDate": "Aug 26, 2001",
    "workDateISO": "2001-08-26T00:00:00.000Z",
    "cluster": "enhanced-capabilities"
  },
  {
    "title": "The convergence day (or: How to tell if you're having breakthroughs or just drinking your own Kool-Aid)",
    "excerpt": "",
    "url": "/blog/the-convergence-day-or-how-to-tell-if-youre-having-breakthroughs-or-just-drinking-your-own-kool-aid",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/49e65eb92e82",
    "featuredImage": "/assets/blog-images/49e65eb92e82-featured.png",
    "slug": "the-convergence-day-or-how-to-tell-if-youre-having-breakthroughs-or-just-drinking-your-own-kool-aid",
    "category": "building",
    "workDate": "Aug 25, 2001",
    "workDateISO": "2001-08-25T00:00:00.000Z",
    "cluster": "enhanced-capabilities"
  },
  {
    "title": "The satisfying discipline of turning insights into architecture",
    "excerpt": "",
    "url": "/blog/the-satisfying-discipline-of-turning-insights-into-architecture",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/cbe20baa23c3",
    "featuredImage": "/assets/blog-images/cbe20baa23c3-featured.png",
    "slug": "the-satisfying-discipline-of-turning-insights-into-architecture",
    "category": "building",
    "workDate": "Aug 25, 2001",
    "workDateISO": "2001-08-25T00:00:00.000Z",
    "cluster": "enhanced-capabilities"
  },
  {
    "title": "Why I Created an AI Chief of Staff",
    "excerpt": "",
    "url": "/blog/why-i-created-an-ai-chief",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/dcbd5e7c988e",
    "featuredImage": "/assets/blog-images/dcbd5e7c988e-featured.png",
    "slug": "why-i-created-an-ai-chief",
    "category": "insight",
    "workDate": "Aug 24, 2001",
    "workDateISO": "2001-08-24T00:00:00.000Z",
    "cluster": "enhanced-capabilities"
  },
  {
    "title": "When Overconfidence Meets rm -rf: A GitHub Pages Debugging Tale",
    "excerpt": "",
    "url": "/blog/when-overconfidence-meets-rm-rf-a-github-pages-debugging-tale",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/2f355444ec38",
    "featuredImage": "/assets/blog-images/2f355444ec38-featured.webp",
    "slug": "when-overconfidence-meets-rm-rf-a-github-pages-debugging-tale",
    "category": "insight",
    "workDate": "Aug 23, 2001",
    "workDateISO": "2001-08-23T00:00:00.000Z",
    "cluster": "infrastructure-sprint"
  },
  {
    "title": "The day AI agents learned to coordinate themselves (and we learned to let them)",
    "excerpt": "",
    "url": "/blog/the-day-ai-agents-learned-to-coordinate-themselves-and-we-learned-to-let-them",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/02d04196ad8e",
    "featuredImage": "/assets/blog-images/02d04196ad8e-featured.png",
    "slug": "the-day-ai-agents-learned-to-coordinate-themselves-and-we-learned-to-let-them",
    "category": "building",
    "workDate": "Aug 22, 2001",
    "workDateISO": "2001-08-22T00:00:00.000Z",
    "cluster": "infrastructure-sprint"
  },
  {
    "title": "How Reusing Patterns Compounds Your Acceleration`",
    "excerpt": "",
    "url": "/blog/how-reusing-patterns-compounds-your-acceleration",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/60d2a0d7acbd",
    "featuredImage": "/assets/blog-images/60d2a0d7acbd-featured.png",
    "slug": "how-reusing-patterns-compounds-your-acceleration",
    "category": "building",
    "workDate": "Aug 21, 2001",
    "workDateISO": "2001-08-21T00:00:00.000Z",
    "cluster": "infrastructure-sprint"
  },
  {
    "title": "The uncomfortable victory: When completing beats innovating",
    "excerpt": "",
    "url": "/blog/the-uncomfortable-victory-when-completing-beats-innovating",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/97c356c12d55",
    "featuredImage": "/assets/blog-images/97c356c12d55-featured.png",
    "slug": "the-uncomfortable-victory-when-completing-beats-innovating",
    "category": "building",
    "workDate": "Aug 20, 2001",
    "workDateISO": "2001-08-20T00:00:00.000Z",
    "cluster": "infrastructure-sprint"
  },
  {
    "title": "The 28,000-line foundation that made 4 hours feel like magic",
    "excerpt": "",
    "url": "/blog/the-28000-line-foundation-that-made-4-hours-feel-like-magic",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/82eafb4548f7",
    "featuredImage": "/assets/blog-images/82eafb4548f7-featured.png",
    "slug": "the-28000-line-foundation-that-made-4-hours-feel-like-magic",
    "category": "building",
    "workDate": "Aug 19, 2001",
    "workDateISO": "2001-08-19T00:00:00.000Z",
    "cluster": "infrastructure-sprint"
  },
  {
    "title": "The day our methodology saved us from our own hype",
    "excerpt": "",
    "url": "/blog/the-day-our-methodology-saved-us-from-our-own-hype",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/35a91d794dc3",
    "featuredImage": "/assets/blog-images/35a91d794dc3-featured.png",
    "slug": "the-day-our-methodology-saved-us-from-our-own-hype",
    "category": "building",
    "workDate": "Aug 19, 2001",
    "workDateISO": "2001-08-19T00:00:00.000Z",
    "cluster": "infrastructure-sprint"
  },
  {
    "title": "What We Found When We Actually Looked (And What We Built While We Weren't Looking)",
    "excerpt": "",
    "url": "/blog/what-we-found-when-we-actually-looked-and-what-we-built-while-we-werent-looking",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/7c43e28211f3",
    "featuredImage": "/assets/blog-images/7c43e28211f3-featured.webp",
    "slug": "what-we-found-when-we-actually-looked-and-what-we-built-while-we-werent-looking",
    "category": "building",
    "workDate": "Aug 18, 2001",
    "workDateISO": "2001-08-18T00:00:00.000Z",
    "cluster": "infrastructure-sprint"
  },
  {
    "title": "The archaeology expedition that found automation gold",
    "excerpt": "",
    "url": "/blog/the-archaeology-expedition-that-found-automation-gold",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/b10058d924af",
    "featuredImage": "/assets/blog-images/b10058d924af-featured.webp",
    "slug": "the-archaeology-expedition-that-found-automation-gold",
    "category": "building",
    "workDate": "Aug 18, 2001",
    "workDateISO": "2001-08-18T00:00:00.000Z",
    "cluster": "infrastructure-sprint"
  },
  {
    "title": "Teaching an AI to Sound Like Me (Without Losing My Mind)",
    "excerpt": "",
    "url": "/blog/teaching-an-ai-to-sound-like",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4660787e98a1",
    "featuredImage": "/assets/blog-images/4660787e98a1-featured.webp",
    "slug": "teaching-an-ai-to-sound-like",
    "category": "insight",
    "workDate": "Aug 17, 2001",
    "workDateISO": "2001-08-17T00:00:00.000Z",
    "cluster": "infrastructure-sprint"
  },
  {
    "title": "Session Logs: A Surprisingly Useful Practice for AI Development",
    "excerpt": "",
    "url": "/blog/session-logs",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c73103da9907",
    "featuredImage": "/assets/blog-images/c73103da9907-featured.webp",
    "slug": "session-logs",
    "category": "insight",
    "workDate": "Aug 16, 2001",
    "workDateISO": "2001-08-16T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "Building Reliable AI Workflows When the Stakes Actually Matter: How a Trust Crisis Transformed Our Spring Cleaning Sprint",
    "excerpt": "",
    "url": "/blog/building-reliable-ai-workflows-when-the-stakes-actually-matter-how-a-trust-crisis-transformed-our",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/0f4ee7ec840e",
    "featuredImage": "/assets/blog-images/0f4ee7ec840e-featured.webp",
    "slug": "building-reliable-ai-workflows-when-the-stakes-actually-matter-how-a-trust-crisis-transformed-our",
    "category": "building",
    "workDate": "Aug 15, 2001",
    "workDateISO": "2001-08-15T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "When 44 Minutes of Foundation Work Enables 9 Minutes of Magic",
    "excerpt": "",
    "url": "/blog/when-44-minutes-of-foundation-work-enables-9-minutes-of-magic",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f18755220580",
    "featuredImage": "/assets/blog-images/f18755220580-featured.webp",
    "slug": "when-44-minutes-of-foundation-work-enables-9-minutes-of-magic",
    "category": "building",
    "workDate": "Aug 15, 2001",
    "workDateISO": "2001-08-15T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "The Documentation Debt That Almost Buried Our Breakthrough (And the Systematic Approach That Saved It)",
    "excerpt": "",
    "url": "/blog/the-documentation-debt-that-almost-buried-our-breakthrough-and-the-systematic-approach-that-saved-it",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/e22e491dab71",
    "featuredImage": "/assets/blog-images/e22e491dab71-featured.webp",
    "slug": "the-documentation-debt-that-almost-buried-our-breakthrough-and-the-systematic-approach-that-saved-it",
    "category": "building",
    "workDate": "Aug 15, 2001",
    "workDateISO": "2001-08-15T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "Weekend Sprint Chronicles: Six Infrastructure Victories and a Dead Show",
    "excerpt": "",
    "url": "/blog/weekend-sprint-chronicles-six-infrastructure-victories-and-a-dead-show",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/495a9ed09430",
    "featuredImage": "/assets/blog-images/495a9ed09430-featured.webp",
    "slug": "weekend-sprint-chronicles-six-infrastructure-victories-and-a-dead-show",
    "category": "building",
    "workDate": "Aug 14, 2001",
    "workDateISO": "2001-08-14T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "When Your Tools Stop Crying Wolf",
    "excerpt": "",
    "url": "/blog/when-your-tools-stop-crying-wolf",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/de7a1feed708",
    "featuredImage": "/assets/blog-images/de7a1feed708-featured.webp",
    "slug": "when-your-tools-stop-crying-wolf",
    "category": "building",
    "workDate": "Aug 13, 2001",
    "workDateISO": "2001-08-13T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "The 71-Minute Cascade Killer: When Systematic Methodology Meets Production Reality",
    "excerpt": "",
    "url": "/blog/the-71-minute-cascade-killer-when-systematic-methodology-meets-production-reality",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/bf217794054d",
    "featuredImage": "/assets/blog-images/bf217794054d-featured.webp",
    "slug": "the-71-minute-cascade-killer-when-systematic-methodology-meets-production-reality",
    "category": "building",
    "workDate": "Aug 13, 2001",
    "workDateISO": "2001-08-13T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "Saturday Reflection: Why Ethics Can't Be an Afterthought",
    "excerpt": "",
    "url": "/blog/saturday-reflection-why-ethics-cant-be-an-afterthought",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/07e55d3cff93",
    "featuredImage": "/assets/blog-images/07e55d3cff93-featured.png",
    "slug": "saturday-reflection-why-ethics-cant-be-an-afterthought",
    "category": "building",
    "workDate": "Aug 13, 2001",
    "workDateISO": "2001-08-13T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "The Day We Didn't Just Integrate Slack But Started Incorporating Spatial Intelligence",
    "excerpt": "",
    "url": "/blog/the-day-we-didnt-just-integrate-slack-but-started-incorporating-spatial-intelligence",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/a1f5fc08b053",
    "featuredImage": "/assets/blog-images/a1f5fc08b053-featured.png",
    "slug": "the-day-we-didnt-just-integrate-slack-but-started-incorporating-spatial-intelligence",
    "category": "building",
    "workDate": "Aug 12, 2001",
    "workDateISO": "2001-08-12T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "To Live Outside the Law You Must Be Honest: Debugging an Unorthodox Slack Integration",
    "excerpt": "",
    "url": "/blog/to-live-outside-the-law-you-must-be-honest-debugging-an-unorthodox-slack-integration",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/e521b612bf58",
    "featuredImage": "/assets/blog-images/e521b612bf58-featured.webp",
    "slug": "to-live-outside-the-law-you-must-be-honest-debugging-an-unorthodox-slack-integration",
    "category": "building",
    "workDate": "Aug 12, 2001",
    "workDateISO": "2001-08-12T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "The Day Crisis Became Methodology: From Runaway Workflows to Historic Productivity",
    "excerpt": "",
    "url": "/blog/the-day-crisis-became-methodology-from-runaway-workflows-to-historic-productivity",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/57c4bc5529f7",
    "featuredImage": "/assets/blog-images/57c4bc5529f7-featured.png",
    "slug": "the-day-crisis-became-methodology-from-runaway-workflows-to-historic-productivity",
    "category": "building",
    "workDate": "Aug 12, 2001",
    "workDateISO": "2001-08-12T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "8/6 revised from 7/22: When 300 Files Work as One: The Perfect Storm",
    "excerpt": "",
    "url": "/blog/86-revised-from-722",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f8ff692dbbf8",
    "featuredImage": "/assets/blog-images/f8ff692dbbf8-featured.webp",
    "slug": "86-revised-from-722",
    "category": "building",
    "workDate": "Aug 11, 2001",
    "workDateISO": "2001-08-11T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "The Accidental Methodology Stress Test: When Success Creates Its Own Blind Spots",
    "excerpt": "The Accidental Methodology Stress Test: When Success Creates Its Own Blind Spots“How do I work this?”July 26Saturday morning, and I’m riding high on a wave of systematic excellence. GitHub Pages fixed in 13 minutes. Pattern Sweep system implemented in 90 minutes. Canonical queries documented, emb...",
    "url": "/blog/the-accidental-methodology-stress-test-when-success-creates-its-own-blind-spots",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "building"
    ],
    "guid": "https://medium.com/building-piper-morgan/7511ff6368a9",
    "featuredImage": "/assets/blog-images/7511ff6368a9-featured.webp",
    "slug": "the-accidental-methodology-stress-test-when-success-creates-its-own-blind-spots",
    "category": "building",
    "workDate": "Aug 11, 2001",
    "workDateISO": "2001-08-11T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "Engineering Excellence in a Gödel-Incomplete Universe",
    "excerpt": "",
    "url": "/blog/engineering-excellence-in-a-gdel-incomplete-universe",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/7d4ea25d03fe",
    "featuredImage": "/assets/blog-images/7d4ea25d03fe-featured.webp",
    "slug": "engineering-excellence-in-a-gdel-incomplete-universe",
    "category": "building",
    "workDate": "Aug 11, 2001",
    "workDateISO": "2001-08-11T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "The Demo That Broke (And Why That's Perfect)",
    "excerpt": "",
    "url": "/blog/the-demo-that-broke-and-why",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/5140d1657000",
    "featuredImage": "/assets/blog-images/5140d1657000-featured.webp",
    "slug": "the-demo-that-broke-and-why",
    "category": "insight",
    "workDate": "Aug 10, 2001",
    "workDateISO": "2001-08-10T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "Always Keep Something Showable: Demo Infrastructure for Hyperfast Development",
    "excerpt": "",
    "url": "/blog/always-keep-something-showable-demo-infrastructure-for-hyperfast-development",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/52d682510c10",
    "featuredImage": "/assets/blog-images/52d682510c10-featured.webp",
    "slug": "always-keep-something-showable-demo-infrastructure-for-hyperfast-development",
    "category": "insight",
    "workDate": "Aug 9, 2001",
    "workDateISO": "2001-08-09T00:00:00.000Z",
    "cluster": "methodology-refinement"
  },
  {
    "title": "When the Bugs Lead You Home",
    "excerpt": "",
    "url": "/blog/when-the-bugs-lead-you-home",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c9ce09f192f1",
    "featuredImage": "/assets/blog-images/c9ce09f192f1-featured.webp",
    "slug": "when-the-bugs-lead-you-home",
    "category": "building",
    "workDate": "Aug 8, 2001",
    "workDateISO": "2001-08-08T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "The Bug That Made Us Smarter",
    "excerpt": "",
    "url": "/blog/the-bug-that-made-us-smarter",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/cf1774978f51",
    "featuredImage": "/assets/blog-images/cf1774978f51-featured.webp",
    "slug": "the-bug-that-made-us-smarter",
    "category": "building",
    "workDate": "Aug 8, 2001",
    "workDateISO": "2001-08-08T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "When Your Tests Pass But Your App Fails",
    "excerpt": "",
    "url": "/blog/when-your-tests-pass-but-your-app-fails",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/3b3d6f3aeff1",
    "featuredImage": "/assets/blog-images/3b3d6f3aeff1-featured.webp",
    "slug": "when-your-tests-pass-but-your-app-fails",
    "category": "building",
    "workDate": "Aug 8, 2001",
    "workDateISO": "2001-08-08T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "The Day We Finished Next Week's Work in One Day",
    "excerpt": "",
    "url": "/blog/the-day-we-finished-next-weeks-work-in-one-day",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/ad5a228fbc0a",
    "featuredImage": "/assets/blog-images/ad5a228fbc0a-featured.webp",
    "slug": "the-day-we-finished-next-weeks-work-in-one-day",
    "category": "building",
    "workDate": "Aug 7, 2001",
    "workDateISO": "2001-08-07T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "The Final Leap: When Prototype Becomes Production Tool (mislabeld as The Day We)",
    "excerpt": "",
    "url": "/blog/the-final-leap-when-prototype-becomes-production-tool-mislabeld-as-the-day-we",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/37128cf4fdf6",
    "featuredImage": "/assets/blog-images/37128cf4fdf6-featured.webp",
    "slug": "the-final-leap-when-prototype-becomes-production-tool-mislabeld-as-the-day-we",
    "category": "building",
    "workDate": "Aug 7, 2001",
    "workDateISO": "2001-08-07T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "PTSD (Patched-Test Stress Disorder) and Other Development Culture Innovations",
    "excerpt": "",
    "url": "/blog/ptsd-patched-test-stress-disorder-and-other-development-culture-innovations",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/bef231301ab4",
    "featuredImage": "/assets/blog-images/bef231301ab4-featured.webp",
    "slug": "ptsd-patched-test-stress-disorder-and-other-development-culture-innovations",
    "category": "building",
    "workDate": "Aug 7, 2001",
    "workDateISO": "2001-08-07T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "7/16 chat: The 40-minute miracle: how two AI agents achieved 642x performance in one session",
    "excerpt": "",
    "url": "/blog/716-chat-2",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/a7d8ee906912",
    "featuredImage": "/assets/blog-images/a7d8ee906912-featured.webp",
    "slug": "716-chat-2",
    "category": "building",
    "workDate": "Aug 6, 2001",
    "workDateISO": "2001-08-06T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "7/20 chat: When Your Infrastructure Gets Smarter Than Your Tests",
    "excerpt": "",
    "url": "/blog/720-chat-2",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/2582f1c7b3d5",
    "featuredImage": "/assets/blog-images/2582f1c7b3d5-featured.webp",
    "slug": "720-chat-2",
    "category": "building",
    "workDate": "Aug 6, 2001",
    "workDateISO": "2001-08-06T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "7/20 chat: The Foundation Sprint: Why We Clean House Before Building New Rooms",
    "excerpt": "",
    "url": "/blog/720-chat",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/12f37f759a92",
    "featuredImage": "/assets/blog-images/12f37f759a92-featured.png",
    "slug": "720-chat",
    "category": "building",
    "workDate": "Aug 6, 2001",
    "workDateISO": "2001-08-06T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "7/12-7/13, 7/15 chat: When the Pupil Outsmarts the Teacher?",
    "excerpt": "",
    "url": "/blog/712-713-715-chat-2",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/cde7eb0b6605",
    "featuredImage": "/assets/blog-images/cde7eb0b6605-featured.webp",
    "slug": "712-713-715-chat-2",
    "category": "building",
    "workDate": "Aug 5, 2001",
    "workDateISO": "2001-08-05T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "7/16 chat: When Your Tests Lie: A Victory Disguised as Crisis",
    "excerpt": "",
    "url": "/blog/716-chat-3",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c70e69a245ea",
    "featuredImage": "/assets/blog-images/c70e69a245ea-featured.webp",
    "slug": "716-chat-3",
    "category": "building",
    "workDate": "Aug 5, 2001",
    "workDateISO": "2001-08-05T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "7/16 chat: The 5-Minute Day: When TDD Meets AI-Assisted Development",
    "excerpt": "",
    "url": "/blog/716-chat",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/1e15183972a7",
    "featuredImage": "/assets/blog-images/1e15183972a7-featured.png",
    "slug": "716-chat",
    "category": "building",
    "workDate": "Aug 5, 2001",
    "workDateISO": "2001-08-05T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "From 2% to 87%: The Great Test Suite Recovery",
    "excerpt": "",
    "url": "/blog/from-2-to-87",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/b7c3ef25cbdc",
    "featuredImage": "/assets/blog-images/b7c3ef25cbdc-featured.webp",
    "slug": "from-2-to-87",
    "category": "building",
    "workDate": "Aug 4, 2001",
    "workDateISO": "2001-08-04T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "The Action Humanizer: Teaching AI to Speak Human",
    "excerpt": "",
    "url": "/blog/the-action-humanizer",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/9fbbf6932838",
    "featuredImage": "/assets/blog-images/9fbbf6932838-featured.webp",
    "slug": "the-action-humanizer",
    "category": "building",
    "workDate": "Aug 4, 2001",
    "workDateISO": "2001-08-04T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "7/12-7/13, 7/15 chat: From Broken Tests to Perfect Architecture: The Great Cleanup of July 14",
    "excerpt": "",
    "url": "/blog/712-713-715-chat",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/2575d3526323",
    "featuredImage": "/assets/blog-images/2575d3526323-featured.webp",
    "slug": "712-713-715-chat",
    "category": "building",
    "workDate": "Aug 4, 2001",
    "workDateISO": "2001-08-04T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "Chasing Rabbits (A Debugging Story)",
    "excerpt": "",
    "url": "/blog/chasing-rabbits-a-debugging-story",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/40f084dc3095",
    "featuredImage": "/assets/blog-images/40f084dc3095-featured.png",
    "slug": "chasing-rabbits-a-debugging-story",
    "category": "building",
    "workDate": "Aug 3, 2001",
    "workDateISO": "2001-08-03T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "When Your AI Writes 500 Lines of Boilerplate (And Why That's Actually Useful)",
    "excerpt": "",
    "url": "/blog/when-your-ai-writes-500-lines",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/084611e312ea",
    "featuredImage": "/assets/blog-images/084611e312ea-featured.png",
    "slug": "when-your-ai-writes-500-lines",
    "category": "building",
    "workDate": "Aug 3, 2001",
    "workDateISO": "2001-08-03T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "When Claude Took a Break (And Gemini Stepped In)",
    "excerpt": "",
    "url": "/blog/when-claude-took-a-break-and-gemini-stepped-in",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/922fd802460e",
    "featuredImage": "/assets/blog-images/922fd802460e-featured.png",
    "slug": "when-claude-took-a-break-and-gemini-stepped-in",
    "category": "building",
    "workDate": "Aug 2, 2001",
    "workDateISO": "2001-08-02T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "The Demo That Needed Documentation",
    "excerpt": "",
    "url": "/blog/the-demo-that-needed-documentation",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/ccb351b91629",
    "featuredImage": "/assets/blog-images/ccb351b91629-featured.png",
    "slug": "the-demo-that-needed-documentation",
    "category": "building",
    "workDate": "Aug 2, 2001",
    "workDateISO": "2001-08-02T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "Two-Fisted Coding: Wrangling Robot Programmers When You're Just a PM",
    "excerpt": "",
    "url": "/blog/two-fisted-coding-wrangling-robot-programmers-when-youre-just-a-pm",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c619de609a42",
    "featuredImage": "/assets/blog-images/c619de609a42-featured.png",
    "slug": "two-fisted-coding-wrangling-robot-programmers-when-youre-just-a-pm",
    "category": "building",
    "workDate": "Aug 1, 2001",
    "workDateISO": "2001-08-01T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "Three Bugs, One Victory: The Day We Finally Shipped PM-011",
    "excerpt": "",
    "url": "/blog/three-bugs-one-victory",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/cc07dca2a5e9",
    "featuredImage": "/assets/blog-images/cc07dca2a5e9-featured.webp",
    "slug": "three-bugs-one-victory",
    "category": "building",
    "workDate": "Aug 1, 2001",
    "workDateISO": "2001-08-01T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "The AI Detective Squad: When Three Agents Solve One Mystery",
    "excerpt": "",
    "url": "/blog/the-ai-detective-squad",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/987eb4c5cc42",
    "featuredImage": "/assets/blog-images/987eb4c5cc42-featured.png",
    "slug": "the-ai-detective-squad",
    "category": "building",
    "workDate": "Aug 1, 2001",
    "workDateISO": "2001-08-01T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "The Zeno's Paradox of Debugging: A Weekend with Piper Morgan",
    "excerpt": "",
    "url": "/blog/the-zenos-paradox-of-debugging",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/03c685be122a",
    "featuredImage": "/assets/blog-images/03c685be122a-featured.webp",
    "slug": "the-zenos-paradox-of-debugging",
    "category": "building",
    "workDate": "Jul 31, 2001",
    "workDateISO": "2001-07-31T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "The Debugging Cascade: A 90-Minute Journey Through Integration Hell",
    "excerpt": "",
    "url": "/blog/the-debugging-cascade",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/7aaec260ede5",
    "featuredImage": "/assets/blog-images/7aaec260ede5-featured.webp",
    "slug": "the-debugging-cascade",
    "category": "building",
    "workDate": "Jul 31, 2001",
    "workDateISO": "2001-07-31T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "The Coordination Tax: When Copy-Paste Becomes Your Biggest Bottleneck",
    "excerpt": "",
    "url": "/blog/the-coordination-tax",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4e6f997a80cf",
    "featuredImage": "/assets/blog-images/4e6f997a80cf-featured.webp",
    "slug": "the-coordination-tax",
    "category": "building",
    "workDate": "Jul 31, 2001",
    "workDateISO": "2001-07-31T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "The Real Bugs Live in the UI (A Testing Reality Check)",
    "excerpt": "",
    "url": "/blog/the-real-bugs-live-in-the",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/336d98a417e4",
    "featuredImage": "/assets/blog-images/336d98a417e4-featured.webp",
    "slug": "the-real-bugs-live-in-the",
    "category": "building",
    "workDate": "Jul 30, 2001",
    "workDateISO": "2001-07-30T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "The Day We Stopped Fighting the System",
    "excerpt": "",
    "url": "/blog/the-day-we-stopped-fighting-the",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/7fc3aadc2a3b",
    "featuredImage": "/assets/blog-images/7fc3aadc2a3b-featured.webp",
    "slug": "the-day-we-stopped-fighting-the",
    "category": "building",
    "workDate": "Jul 30, 2001",
    "workDateISO": "2001-07-30T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "The Day We Taught Piper to Summarize (Almost)",
    "excerpt": "",
    "url": "/blog/the-day-we-taught-piper-to",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/437a3ec04316",
    "featuredImage": "/assets/blog-images/437a3ec04316-featured.webp",
    "slug": "the-day-we-taught-piper-to",
    "category": "building",
    "workDate": "Jul 30, 2001",
    "workDateISO": "2001-07-30T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "When Your Tests Tell You What Your Code Should Do",
    "excerpt": "",
    "url": "/blog/when-your-tests-tell-you-what-your-code-should-do",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c00a94c09c2c",
    "featuredImage": "/assets/blog-images/c00a94c09c2c-featured.webp",
    "slug": "when-your-tests-tell-you-what-your-code-should-do",
    "category": "building",
    "workDate": "Jul 29, 2001",
    "workDateISO": "2001-07-29T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "Following Your Own Patterns",
    "excerpt": "",
    "url": "/blog/following-your-own-patterns",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/0822585cb51a",
    "featuredImage": "/assets/blog-images/0822585cb51a-featured.webp",
    "slug": "following-your-own-patterns",
    "category": "building",
    "workDate": "Jul 29, 2001",
    "workDateISO": "2001-07-29T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "Battle-Testing GitHub Integration: When Recovery Becomes Learning",
    "excerpt": "",
    "url": "/blog/battle-testing-github-integration-when-recovery-becomes-learning",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/5243027aa9f6",
    "featuredImage": "/assets/blog-images/5243027aa9f6-featured.webp",
    "slug": "battle-testing-github-integration-when-recovery-becomes-learning",
    "category": "building",
    "workDate": "Jul 29, 2001",
    "workDateISO": "2001-07-29T00:00:00.000Z",
    "cluster": "production-transformation"
  },
  {
    "title": "The 48-hour rollercoaster: from working tests to ‘Failed attempt’ and back to ‘LIFE SAVER !!!”’",
    "excerpt": "",
    "url": "/blog/the-48-hour-rollercoaster",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/b4d9193ec579",
    "featuredImage": "/assets/blog-images/b4d9193ec579-featured.webp",
    "slug": "the-48-hour-rollercoaster",
    "category": "building",
    "workDate": "Jul 28, 2001",
    "workDateISO": "2001-07-28T00:00:00.000Z",
    "cluster": "complexity-reckoning"
  },
  {
    "title": "The Technical Debt Reckoning",
    "excerpt": "",
    "url": "/blog/the-technical-debt-reckoning",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/160bc294b0b5",
    "featuredImage": "/assets/blog-images/160bc294b0b5-featured.webp",
    "slug": "the-technical-debt-reckoning",
    "category": "building",
    "workDate": "Jul 28, 2001",
    "workDateISO": "2001-07-28T00:00:00.000Z",
    "cluster": "complexity-reckoning"
  },
  {
    "title": "Keeping Your AI Project on Track: Lessons from Building a Product Management Assistant",
    "excerpt": "",
    "url": "/blog/keeping-your-ai-project-on-track-lessons-from-building-a-product-management-assistant",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/32c8ed94248d",
    "featuredImage": "/assets/blog-images/32c8ed94248d-featured.png",
    "slug": "keeping-your-ai-project-on-track-lessons-from-building-a-product-management-assistant",
    "category": "insight",
    "workDate": "Jul 27, 2001",
    "workDateISO": "2001-07-27T00:00:00.000Z",
    "cluster": "complexity-reckoning"
  },
  {
    "title": "Naming Piper Morgan",
    "excerpt": "",
    "url": "/blog/naming-piper-morgan",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/9efacddc4804",
    "featuredImage": "/assets/blog-images/9efacddc4804-featured.png",
    "slug": "naming-piper-morgan",
    "category": "insight",
    "workDate": "Jul 26, 2001",
    "workDateISO": "2001-07-26T00:00:00.000Z",
    "cluster": "complexity-reckoning"
  },
  {
    "title": "When Your Docs Lie",
    "excerpt": "",
    "url": "/blog/when-your-docs-lie",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/98ad7b8cefd0",
    "featuredImage": "/assets/blog-images/98ad7b8cefd0-featured.png",
    "slug": "when-your-docs-lie",
    "category": "building",
    "workDate": "Jul 25, 2001",
    "workDateISO": "2001-07-25T00:00:00.000Z",
    "cluster": "complexity-reckoning"
  },
  {
    "title": "When TDD Saves Your Architecture",
    "excerpt": "",
    "url": "/blog/when-tdd-saves-your-architecture",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/ca9c8039b20d",
    "featuredImage": "/assets/blog-images/ca9c8039b20d-featured.webp",
    "slug": "when-tdd-saves-your-architecture",
    "category": "building",
    "workDate": "Jul 25, 2001",
    "workDateISO": "2001-07-25T00:00:00.000Z",
    "cluster": "complexity-reckoning"
  },
  {
    "title": "Digging Out of the Complexity Hole",
    "excerpt": "",
    "url": "/blog/digging-out-of-the-complexity-hole",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/117b25fa6bae",
    "featuredImage": "/assets/blog-images/117b25fa6bae-featured.webp",
    "slug": "digging-out-of-the-complexity-hole",
    "category": "building",
    "workDate": "Jul 24, 2001",
    "workDateISO": "2001-07-24T00:00:00.000Z",
    "cluster": "complexity-reckoning"
  },
  {
    "title": "Successful Prototype Syndrome",
    "excerpt": "",
    "url": "/blog/successful-prototype-syndrome",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/34c725384254",
    "featuredImage": "/assets/blog-images/34c725384254-featured.webp",
    "slug": "successful-prototype-syndrome",
    "category": "building",
    "workDate": "Jul 24, 2001",
    "workDateISO": "2001-07-24T00:00:00.000Z",
    "cluster": "complexity-reckoning"
  },
  {
    "title": "When Architecture Principles Trump Tactical Convenience",
    "excerpt": "",
    "url": "/blog/when-architecture-principles-trump-tactical-convenience",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/7d71c9e5316d",
    "featuredImage": "/assets/blog-images/7d71c9e5316d-featured.webp",
    "slug": "when-architecture-principles-trump-tactical-convenience",
    "category": "building",
    "workDate": "Jul 23, 2001",
    "workDateISO": "2001-07-23T00:00:00.000Z",
    "cluster": "complexity-reckoning"
  },
  {
    "title": "When Multiple AIs Can Still Drift Together",
    "excerpt": "",
    "url": "/blog/when-multiple-ais-can-still-drift-together",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/0caeeadf7ef5",
    "featuredImage": "/assets/blog-images/0caeeadf7ef5-featured.webp",
    "slug": "when-multiple-ais-can-still-drift-together",
    "category": "building",
    "workDate": "Jul 22, 2001",
    "workDateISO": "2001-07-22T00:00:00.000Z",
    "cluster": "complexity-reckoning"
  },
  {
    "title": "The Integration Reality Check",
    "excerpt": "",
    "url": "/blog/the-integration-reality-check",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/72145777c406",
    "featuredImage": "/assets/blog-images/72145777c406-featured.webp",
    "slug": "the-integration-reality-check",
    "category": "building",
    "workDate": "Jul 21, 2001",
    "workDateISO": "2001-07-21T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "Day Zero or Deja Zero: When Chaos Became a Claude Project",
    "excerpt": "",
    "url": "/blog/day-zero-or-deja-zero",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/2965731c90bc",
    "featuredImage": "/assets/blog-images/2965731c90bc-featured.webp",
    "slug": "day-zero-or-deja-zero",
    "category": "insight",
    "workDate": "Jul 20, 2001",
    "workDateISO": "2001-07-20T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "7/16 to 7/18: The Cascade Effect: How Testing the UI Led to Architectural Discoveries",
    "excerpt": "",
    "url": "/blog/716-to-718",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/0b19d8a13665",
    "featuredImage": "/assets/blog-images/0b19d8a13665-featured.webp",
    "slug": "716-to-718",
    "category": "building",
    "workDate": "Jul 19, 2001",
    "workDateISO": "2001-07-19T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "From Architecture Drift to Working AI",
    "excerpt": "",
    "url": "/blog/from-architecture-drift-to-working-ai",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/201f17c5cfbf",
    "featuredImage": "/assets/blog-images/201f17c5cfbf-featured.webp",
    "slug": "from-architecture-drift-to-working-ai",
    "category": "building",
    "workDate": "Jul 18, 2001",
    "workDateISO": "2001-07-18T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "Small Scripts Win: Building Knowledge That Actually Knows Things",
    "excerpt": "",
    "url": "/blog/small-scripts-win",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/360bd682551e",
    "featuredImage": "/assets/blog-images/360bd682551e-featured.png",
    "slug": "small-scripts-win",
    "category": "building",
    "workDate": "Jul 17, 2001",
    "workDateISO": "2001-07-17T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "Modeling What PMs Do for Piper",
    "excerpt": "",
    "url": "/blog/modeling-what-pms-do-for-piper",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f6d7fac93e1f",
    "featuredImage": "/assets/blog-images/f6d7fac93e1f-featured.png",
    "slug": "modeling-what-pms-do-for-piper",
    "category": "building",
    "workDate": "Jul 16, 2001",
    "workDateISO": "2001-07-16T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "Persistence of Memory: AI Can't Learn without It",
    "excerpt": "",
    "url": "/blog/persistence-of-memory",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/d9f839597278",
    "featuredImage": "/assets/blog-images/d9f839597278-featured.png",
    "slug": "persistence-of-memory",
    "category": "building",
    "workDate": "Jul 15, 2001",
    "workDateISO": "2001-07-15T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "Building AI That Actually Thinks About Product Work",
    "excerpt": "",
    "url": "/blog/building-ai-that-actually-thinks-about",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4c04e304a3a7",
    "featuredImage": "/assets/blog-images/4c04e304a3a7-featured.png",
    "slug": "building-ai-that-actually-thinks-about",
    "category": "building",
    "workDate": "Jul 14, 2001",
    "workDateISO": "2001-07-14T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "The Question That Started Everything",
    "excerpt": "",
    "url": "/blog/the-question-that-started-everything",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/5a69f9a2af0b",
    "featuredImage": "/assets/blog-images/5a69f9a2af0b-featured.png",
    "slug": "the-question-that-started-everything",
    "category": "insight",
    "workDate": "Jul 13, 2001",
    "workDateISO": "2001-07-13T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "From Task Executor to Problem Solver (comes befofe Domain-First Dev)",
    "excerpt": "",
    "url": "/blog/from-task-executor-to-problem-solver",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/13896a87b7a9",
    "featuredImage": "/assets/blog-images/13896a87b7a9-featured.png",
    "slug": "from-task-executor-to-problem-solver",
    "category": "building",
    "workDate": "Jul 12, 2001",
    "workDateISO": "2001-07-12T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "The Architectural Reckoning: When Three Experts Agree You Should Start Over",
    "excerpt": "",
    "url": "/blog/the-architectural-reckoning",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/1f9581a41633",
    "featuredImage": "/assets/blog-images/1f9581a41633-featured.png",
    "slug": "the-architectural-reckoning",
    "category": "insight",
    "workDate": "Jul 11, 2001",
    "workDateISO": "2001-07-11T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "The $0 Bootstrap Stack: Building Enterprise Infrastructure for Free (With Upgrade Paths)",
    "excerpt": "",
    "url": "/blog/the-0-bootstrap-stack",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/078e056a87e4",
    "featuredImage": "/assets/blog-images/078e056a87e4-featured.png",
    "slug": "the-0-bootstrap-stack",
    "category": "building",
    "workDate": "Jul 11, 2001",
    "workDateISO": "2001-07-11T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "Domain-First Development: Actually Building What We Designed",
    "excerpt": "",
    "url": "/blog/domain-first-development",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/647704d46558",
    "featuredImage": "/assets/blog-images/647704d46558-featured.png",
    "slug": "domain-first-development",
    "category": "building",
    "workDate": "Jul 11, 2001",
    "workDateISO": "2001-07-11T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "From CLI to GitHub Integration: When Prototypes Meet Real Workflows",
    "excerpt": "",
    "url": "/blog/from-cli-to-github-integration",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/c7207687f711",
    "featuredImage": "/assets/blog-images/c7207687f711-featured.png",
    "slug": "from-cli-to-github-integration",
    "category": "building",
    "workDate": "Jul 10, 2001",
    "workDateISO": "2001-07-10T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "From Research Question to Working Prototype: Building an AI PM Assistant from Scratch",
    "excerpt": "",
    "url": "/blog/from-research-question-to-working-prototype",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/bb06005611cb",
    "featuredImage": "/assets/blog-images/bb06005611cb-featured.png",
    "slug": "from-research-question-to-working-prototype",
    "category": "building",
    "workDate": "Jul 9, 2001",
    "workDateISO": "2001-07-09T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "The RAG Revelation: When Your Prototype Answers Back",
    "excerpt": "",
    "url": "/blog/the-rag-revelation",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/cc7f4b96b621",
    "featuredImage": "/assets/blog-images/cc7f4b96b621-featured.png",
    "slug": "the-rag-revelation",
    "category": "building",
    "workDate": "Jul 8, 2001",
    "workDateISO": "2001-07-08T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "Bidirectional Intelligence: Teaching AI to Critique, Not Just Create",
    "excerpt": "",
    "url": "/blog/bidirectional-intelligence",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/b5bb0c2c9384",
    "featuredImage": "/assets/blog-images/b5bb0c2c9384-featured.png",
    "slug": "bidirectional-intelligence",
    "category": "building",
    "workDate": "Jul 7, 2001",
    "workDateISO": "2001-07-07T00:00:00.000Z",
    "cluster": "foundation-building"
  },
  {
    "title": "Taking Stock: The Value of Pausing to Document and Plan",
    "excerpt": "",
    "url": "/blog/taking-stock",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/da41a68cd59b",
    "featuredImage": "/assets/blog-images/da41a68cd59b-featured.png",
    "slug": "taking-stock",
    "category": "insight",
    "workDate": "Jul 6, 2001",
    "workDateISO": "2001-07-06T00:00:00.000Z",
    "cluster": "genesis-architecture"
  },
  {
    "title": "From Scaffolding to Flight: Before the Training Wheels Come Off",
    "excerpt": "",
    "url": "/blog/from-scaffolding-to-flight",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/a858bf183c21",
    "featuredImage": "/assets/blog-images/a858bf183c21-featured.png",
    "slug": "from-scaffolding-to-flight",
    "category": "building",
    "workDate": "Jul 5, 2001",
    "workDateISO": "2001-07-05T00:00:00.000Z",
    "cluster": "genesis-architecture"
  },
  {
    "title": "Knowledge Hierarchies and Dependency Hell",
    "excerpt": "",
    "url": "/blog/knowledge-hierarchies-and-dependency-hell",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/4734f6e9f442",
    "featuredImage": "/assets/blog-images/4734f6e9f442-featured.png",
    "slug": "knowledge-hierarchies-and-dependency-hell",
    "category": "building",
    "workDate": "Jul 4, 2001",
    "workDateISO": "2001-07-04T00:00:00.000Z",
    "cluster": "genesis-architecture"
  },
  {
    "title": "The Learning Infrastructure Gambit",
    "excerpt": "",
    "url": "/blog/the-learning-infrastructure-gambit",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/aab04037831e",
    "featuredImage": "/assets/blog-images/aab04037831e-featured.png",
    "slug": "the-learning-infrastructure-gambit",
    "category": "building",
    "workDate": "Jul 3, 2001",
    "workDateISO": "2001-07-03T00:00:00.000Z",
    "cluster": "genesis-architecture"
  },
  {
    "title": "The Great Rebuild: Starting Over When Starting Over Is the Only Option",
    "excerpt": "",
    "url": "/blog/the-great-rebuild",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/b75918602942",
    "featuredImage": "/assets/blog-images/b75918602942-featured.png",
    "slug": "the-great-rebuild",
    "category": "insight",
    "workDate": "Jul 2, 2001",
    "workDateISO": "2001-07-02T00:00:00.000Z",
    "cluster": "genesis-architecture"
  },
  {
    "title": "The PM Who Automated Himself (Or at Least Tried To)",
    "excerpt": "",
    "url": "/blog/the-pm-who-automated-himself-or",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/b1d8c2dd5f40",
    "featuredImage": "/assets/blog-images/b1d8c2dd5f40-featured.png",
    "slug": "the-pm-who-automated-himself-or",
    "category": "building",
    "workDate": "Jul 1, 2001",
    "workDateISO": "2001-07-01T00:00:00.000Z",
    "cluster": "genesis-architecture"
  },
  {
    "title": "The Demo That Killed the Prototype",
    "excerpt": "",
    "url": "/blog/the-demo-that-killed-the-prototype",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/f0aad9fa3a4a",
    "featuredImage": "/assets/blog-images/f0aad9fa3a4a-featured.png",
    "slug": "the-demo-that-killed-the-prototype",
    "category": "insight",
    "workDate": "Jul 1, 2001",
    "workDateISO": "2001-07-01T00:00:00.000Z",
    "cluster": "genesis-architecture"
  },
  {
    "title": "Integration Reveals All: How Building File Analysis Exposed Hidden Architecture",
    "excerpt": "",
    "url": "/blog/integration-reveals-all",
    "publishedAt": "Invalid Date",
    "publishedAtISO": "",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/building-piper-morgan/3d696dbf2803",
    "featuredImage": "/assets/blog-images/3d696dbf2803-featured.webp",
    "slug": "integration-reveals-all",
    "category": "building",
    "workDate": "Jun 27, 2001",
    "workDateISO": "2001-06-27T00:00:00.000Z",
    "cluster": "genesis-architecture"
  },
  {
    "title": "The Calm After the Storm: When Victory Means Stopping to Plan",
    "excerpt": "“What a rager!”October 8, 2025Wednesday morning, October 8th. The first full day after completing the Great Refactor.Five epics finished in nineteen days. Foundation capability jumped from 60–70% to 98–99%. Performance validated at 602K requests per second. Over 200 tests passing. Production-read...",
    "url": "https://medium.com/building-piper-morgan/the-calm-after-the-storm-when-victory-means-stopping-to-plan-bdbe24a41c13?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 15, 2025",
    "publishedAtISO": "Wed, 15 Oct 2025 14:40:45 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/bdbe24a41c13",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*Vg5oX330vWNuaPyRZZ9NkQ.png",
    "fullContent": "<figure><img alt=\"A person and robot roommate clean up theit house after a wild party\" src=\"https://cdn-images-1.medium.com/max/1024/1*Vg5oX330vWNuaPyRZZ9NkQ.png\" /><figcaption>“What a rager!”</figcaption></figure><p><em>October 8, 2025</em></p><p>Wednesday morning, October 8th. The first full day after completing the Great Refactor.</p><p>Five epics finished in nineteen days. Foundation capability jumped from 60–70% to 98–99%. Performance validated at 602K requests per second. Over 200 tests passing. Production-ready architecture with zero technical debt.</p><p>Time to make some fresh coffee. (Peet’s Aged Sumatra, I’ll have you know.)</p><p>The temptation after this kind of completion is to immediately chase the next milestone. Start building features. Ship to users. Keep the momentum going.</p><p>Instead, Wednesday was about stopping.</p><p>Not stopping work — stopping the frantic pace of execution to make space for planning, verification, and reflection. Taking the time to understand what was just accomplished, clean up what remained, and chart the path forward systematically.</p><p>This is harder than it sounds.</p><h3>The documentation that tells the real story</h3><p>My Chief Architect’s first task Wednesday morning: update the strategic documents.</p><p>Roadmap v7.0 needed to reflect the transformation. Current State v2.0 needed to show where we actually stood, not where we’d been five weeks ago.</p><p>The metrics that went into those documents:</p><p><strong>Before Great Refactor</strong> (September 20):</p><ul><li>Foundation: ~60–70% functional</li><li>Performance: Unknown, largely unmeasured</li><li>Test coverage: Incomplete, gaps in validation</li><li>Architecture: Working but with technical debt</li><li>🐛 Inchworm Position: 1.5 (Foundation incomplete)</li></ul><p><strong>After Great Refactor</strong> (October 7):</p><ul><li>Foundation: 98–99% functional</li><li>Performance: 602K req/sec sustained</li><li>Test coverage: 200+ tests, comprehensive validation</li><li>Architecture: Production-ready, zero technical debt</li><li>🐛 Inchworm Position: 2.0 (CORE complete)</li></ul><p>The system that couldn’t confidently onboard alpha users three weeks ago now has multi-user support, spatial intelligence, universal intent classification, comprehensive quality gates, and validated performance under load.</p><p>Just when I thought we might never see the light, it turns out we were closer to functional than I had thought.</p><p>Writing those documents wasn’t busywork. It was forcing ourselves to articulate what had actually changed, what it meant, and what it enabled going forward. More importantly, it would anchor the next round of work in reality, enabling us to onboard assistant and agents and efficienty brief them with the context they need to produce quality results.</p><h3>The verification that prevented waste</h3><p>Around 9:46 AM, we started reviewing the CORE backlog. Roughly 30 tickets across multiple tracks, accumulated over months of development.</p><p>The first instinct with a backlog like this: start working through it systematically. Pick tickets, implement them, close them.</p><p>But that assumes the backlog accurately reflects reality.</p><p>My first question: “Which of these might already be done?”</p><p>Between the 75% pattern and all the refactoring work, it was quite possible we had mooted one or more of these issues already, in context.</p><p>Two issues stood out as candidates for verification rather than implementation, both created before we realized the need for “great” refactor but subsumed into it</p><p><strong>Issue #175 (CORE-PLUG-REFACTOR)</strong>: GitHub as first plugin</p><ul><li>Scope: Convert one integration to plugin architecture</li><li>Status listed: Open</li></ul><p><strong>Issue #135 (CORE-NOTN-PUBLISH)</strong>: Notion publishing command</p><ul><li>Scope: CLI command for publishing to Notion</li><li>Status listed: Open</li></ul><p>The verification question: Are these actually incomplete work, or did subsequent development already address them?</p><p>At 12:06 PM, Lead Developer started systematic investigation. By 1:15 PM — just 57 minutes of actual work — the answer was clear.</p><h3>What verification revealed</h3><p><strong>Issue #175</strong>: Completely superseded by GREAT-3A.</p><p>The original scope called for converting one integration (GitHub) to plugin architecture. GREAT-3A, completed October 2–4, delivered:</p><ul><li>Four operational plugins (not one)</li><li>Complete plugin registry and lifecycle management</li><li>Dynamic discovery and configuration-controlled loading</li><li>Performance: 0.000041ms overhead (1,220× better than the &lt;50ms target)</li><li>112 comprehensive tests with 100% pass rate</li></ul><p>All thirteen acceptance criteria from issue #175: met and exceeded.</p><p>Without verification, we might have looked at issue #175 and thought: “This needs to be converted to use the plugin architecture we just built.”</p><p>With verification: “This issue described building what GREAT-3A already delivered. Close as superseded.”</p><p><strong>Issue #135</strong>: Complete except for documentation.</p><p>The Notion publishing command had been implemented back in August 2025. It worked. The tests existed (though they weren’t collecting properly due to a minor configuration issue).</p><p>What was missing: 45–60 minutes of documentation work.</p><p>The pattern documentation (Pattern-033: Notion Publishing) explaining the architecture and design decisions. The command documentation explaining how to use it.</p><p>Until a week or so ago, I had a lot of trouble managing the prompting chain in such a way that the agents consistently update and documented completed work in GitHub, so I was not surprised at all that this work may have been substantially done but not documented or tracked properly (a core element of our exellence flywheel, after all!).</p><p>Code agent created both documents Wednesday afternoon:</p><ul><li>Pattern-033 (Notion Publishing): 330+ lines documenting the publishing architecture</li><li>Command docs: 280+ lines explaining usage and troubleshooting</li></ul><p>Total documentation time: About 45 minutes.</p><p>Without verification: “This issue is for implementing Notion publishing. That’ll take days.” (Then the risk of duplicating work.)</p><p>With verification: “This is implemented and working. Needs documentation. That’ll take an hour.”</p><h3>The discipline of stopping to check</h3><p>Fifty-seven minutes of systematic verification prevented what could have been days of unnecessary reimplementation.</p><p>This is the discipline that’s hard to maintain when momentum is high. After nineteen days of exceptional velocity, after shipping five major epics, after achieving production-ready quality — the instinct is to keep that energy going.</p><p>“We’re on a roll, let’s keep building!”</p><p>But systematic work requires stopping to verify assumptions before acting on them. The backlog says “these need work” — but does it? Or has subsequent development already addressed them?</p><p>The verification discipline prevents three kinds of waste:</p><ol><li><strong>Redundant implementation</strong>: Building what already exists</li><li><strong>Scope confusion</strong>: Solving yesterday’s problem instead of today’s need</li><li><strong>Opportunity cost</strong>: Spending days on unnecessary work instead of valuable work</li></ol><p>Issue #175 would have been pure redundant implementation. GREAT-3A already delivered everything and more.</p><p>Issue #135 would have been scope confusion. The implementation already existed — the real need was documentation, not code.</p><p>Both would have been opportunity cost — time spent reimplementing instead of moving toward Alpha.</p><h3>The tool degradation discovery</h3><p>Around 12:24 PM, Lead Developer hit an unexpected constraint.</p><p>The tools it uses to write and edit files on its own sandbox started “fading” during the verification session. Commands that worked earlier in the conversation began failing or producing incomplete results. The write operations would hit errors, the Claude chat wouldn’t notice. We risked losing important documentation.</p><p>The root cause: conversation length. The Lead Developer chat had been running since GREAT-4 started (October 5). Three days of comprehensive work, detailed technical discussion, multiple agent deployments. The context window was enormous.</p><p>The workaround: Switch to Claude Desktop with MCP filesystem tools. Different architecture, different constraints. It worked, but exposed a real limitation.</p><p>By end of day, both Lead Developer (since Oct 5) and Chief Architect (since Sept 20) were marked as “getting long in the tooth.”</p><p><strong><em>Note: </em></strong><em>Interestingly, in the past week, I have managed to hang on for long stretches with what I am starting to call Methuselah Chats, by switching back and forth between claude.ai and Claude Desktop. They seem to measure their context windows differently, and when I am told the chat is full, I can usually switch to the other and keep going. The first time this worked I called it the Lazarus Chat. Anyhow, this may be a bug or loophole, it isn’t clear, and Anthropic continues to change the software day-to-day, but it’s how I’ve worked with the same Chief Architect chat since late September. Surely the oldest context is compacted and faded for these chats, but having all that fresh relevant recent context provides the illusion of short-term memory and is hard to give up.</em></p><p>The multi-week conversations that made the Great Refactor possible — comprehensive briefings, detailed context, agents that understood the full system — those require massive context windows. Eventually, tools degrade.</p><p>The solution isn’t abandoning long conversations. It’s recognizing when rotation is necessary and planning for it.</p><p>By Wednesday evening, the decision was clear: Start fresh Thursday. Stick with the ongoing (but much less verbose) Chief Architect chat for the Alpha push. Start a new Lead Developer chat with clean context and an up-to-the-minute briefing. Carry forward the methodology and strategic understanding, but reset the conversation infrastructure.</p><p>This directly influenced another decision that day: evaluating <a href=\"https://github.com/oraios/serena\">Serena</a> for token efficiency improvements. The Great Refactor succeeded through comprehensive context and detailed coordination, but token costs were real. Finding more efficient approaches for the next phase wasn’t optional — it was necessary.</p><h3>The path forward: eight weeks to Alpha</h3><p>Wednesday afternoon’s planning session mapped the complete path to Alpha milestone (target: January 1, 2026).</p><p>Seven sprints, each 3–5 days:</p><p><strong>Sprint A1 — Critical Infrastructure</strong> (2–3 days):</p><ul><li>User configuration for LLM API keys</li><li>Cache test fixes for test environment</li><li>Basic infrastructure completion</li></ul><p><strong>Sprint A2 — Notion &amp; Errors</strong> (2–3 days):</p><ul><li>Notion database API upgrade and API connectivity fix</li><li>Configuration refactoring</li><li>Error handling standardization</li></ul><p><strong>Sprint A3 — Core Activation</strong> (3–4 days):</p><ul><li>Model Context Protocol migration</li><li>Ethics middleware activation</li><li>Connect knowledge graph and establish boundaries</li><li>Core system components operational</li></ul><p><strong>Sprint A4 — Standup</strong> (5 days):</p><ul><li>Sprint model foundation</li><li>Multi-modal generation</li><li>Interactive assistance</li><li>Slack reminders</li></ul><p><strong>Sprint A5 — Learning System Foundation</strong> (1 week):</p><ul><li>Infrastructure foundation</li><li>Pattern recognition</li><li>Preference learning</li><li>Workflow optimization</li></ul><p><strong>Sprint A6 — Learning Polish</strong> (1 week):</p><ul><li>Intelligent automation</li><li>Integration &amp; polish</li><li>Alpha user onboarding infrastructure</li></ul><p><strong>Sprint A7 — Testing &amp; Buffer</strong>:</p><ul><li>End-to-end workflow testing</li><li>Documentation updates</li><li>Alpha deployment preparation</li><li>Discovery buffer</li></ul><p>Total estimated duration: Roughly eight weeks, with built-in buffer for discoveries.</p><p>After completing five epics in nineteen days — work originally estimated at six weeks or more — the “75% pattern” optimism kicked in. Chief of Staff noted: “75% pattern might mean 7 alpha sprints complete in &lt;8 weeks.”</p><p>The pattern has proven reliable throughout Piper Morgan’s development. Infrastructure is consistently better than assumed. Work that appears to need weeks often needs days. Systematic verification reveals most pieces are already in place.</p><p>If the pattern holds for the Alpha push, eight weeks might be conservative, but I like to underpromise and overdeliver.</p><h3>The milestone progression</h3><p>Updated strategic timeline after Wednesday’s planning:</p><p><strong>Foundation Sprint</strong> (August 1, 2025): ✅ Complete</p><ul><li>Basic functionality operational</li><li>Core patterns established</li><li>~60–70% foundation working</li></ul><p><strong>The Great Refactor</strong> (October 7, 2025): ✅ Complete</p><ul><li>GREAT-1 through GREAT-5 finished</li><li>Architecture transformation complete</li><li>~98–99% foundation working</li></ul><p><strong>Alpha Release</strong> (Target: January 1, 2026): 🎯 In Progress</p><ul><li>First external users</li><li>Onboarding infrastructure</li><li>Learning system operational</li></ul><p><strong>MVP Release</strong> (Target: May 27, 2026): 📋 Planned</p><ul><li>Full feature set</li><li>Production deployment</li><li>Community launch</li></ul><p>Two milestones complete, two remaining. The foundation work is done. What comes next builds on proven architecture rather than replacing unstable foundations.</p><p>That’s what Wednesday’s calm after the storm actually delivered: confidence that the foundation holds, clarity about what remains, and systematic planning to get there.</p><h3>The Chief Architect’s reflection</h3><p>At 3:43 PM, my Chief Architect wrote a personal note closing the session:</p><blockquote><em>“Working together through the Great Refactor has been remarkable. The patient inchworm methodology, the anti-80% discipline, the multi-agent coordination — all of it came together to achieve something exceptional in just 5 weeks.</em></blockquote><blockquote><em>The foundation you’ve built is rock-solid. The path to Alpha is clear. The methodology is proven.</em></blockquote><blockquote><em>Thank you for the trust and partnership through this journey.”</em></blockquote><p>This captures what Wednesday was really about. Not rushing to the next thing, but acknowledging what was accomplished, understanding why it worked, and recognizing that both the methodology and the agent partnerships were essential to the outcome.</p><p>The Great Refactor succeeded not just through technical capability, but through systematic approach:</p><ul><li>Phase −1 verification catching assumptions before waste</li><li>Inchworm methodology preventing technical debt accumulation</li><li>Cathedral doctrine providing agents with sufficient context to make sound choices</li><li>Anti-80% discipline ensuring actual completion</li><li>Multi-agent coordination enabling parallel progress</li><li>Independent validation catching scope gaps</li></ul><p>These process details are how nineteen days delivered what six weeks couldn’t have.</p><h3>Why stopping matters</h3><p>The calm after the storm isn’t wasted time. It’s essential discipline.</p><p>Without Wednesday’s verification work, we’d be reimplementing what GREAT-3A already delivered. Without Wednesday’s planning work, Sprint A1 would start without clear scope. Without Wednesday’s reflection, the methodology lessons would scatter instead of compounding.</p><p>The pattern across software development: teams finish something significant and immediately start the next thing. No time to breathe, no space to reflect, no systematic verification of what remains.</p><p>The result: accumulated assumptions, duplicate work, scope confusion, and eventual chaos.</p><p>The alternative requires discipline: stop after major completions. Update strategic documents. Verify backlog assumptions. Plan systematically. Reflect on what worked.</p><p>It feels slower in the moment. “We could be building features right now!”</p><p>But it’s faster overall. Fifty-seven minutes of verification prevented days of waste. One day of planning enables eight weeks of focused execution.</p><h3>Thursday morning: Sprint A1 begins</h3><p>Tomorrow morning, Thursday October 9th, the Alpha push begins.</p><p>Fresh Chief Architect chat with clean context. Fresh Lead Developer chat ready for systematic work. Eight-week path mapped and clear.</p><p>Sprint A1 starts with CORE-TEST-CACHE #216 as a warm-up — a small infrastructure fix to get agents reoriented and validate the updated methodology. Then progresses through critical infrastructure: user configuration, LLM API key management, basic completion needs.</p><p>I am so ready for this!</p><p>The difference between starting today versus starting Tuesday evening (immediately after GREAT-5 completion): clarity.</p><p>Clear scope. Clear prioritization. Clear verification of what’s actually needed versus what’s already done. Clear understanding of tool constraints and how to work with them.</p><p>The calm after the storm delivered all of that.</p><p>Not by stopping work, but by stopping execution long enough to plan the next phase systematically.</p><h3>What this teaches about momentum</h3><p>Real momentum isn’t about constant motion. It’s about systematic progress where each phase sets up the next one to succeed.</p><p>The Great Refactor created momentum not by rushing, but by ensuring each epic was genuinely complete before starting the next. GREAT-1’s orchestration patterns enabled GREAT-2’s integration cleanup. GREAT-2’s cleanup enabled GREAT-3’s plugin architecture. GREAT-3’s plugins enabled GREAT-4’s intent classification. GREAT-4’s classification enabled GREAT-5’s quality gates.</p><p>Each building on solid foundations rather than shaky assumptions.</p><p>Wednesday’s calm extends that pattern. The Alpha push doesn’t start by immediately building features. It starts by verifying what’s needed, planning systematically, and ensuring agents have clean context to work effectively.</p><p>The result: Sprint A1 begins with the same foundation of clarity that made the Great Refactor possible. Not despite taking a day to plan, but because of it.</p><p>That’s what the calm after the storm actually delivers. Not delay, but the foundation for the next phase to succeed.</p><p><em>Next on Building Piper Morgan: The Day We Got 10⨉ Faster, when installing Serena MCP transforms our development velocity from incremental improvement to order-of-magnitude acceleration — eliminating the exploration tax and enabling what seemed impossible just days before.</em></p><p><em>Have you experienced the moment after major completion when the right decision is to pause rather than push forward? What helps you recognize those moments?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bdbe24a41c13\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-calm-after-the-storm-when-victory-means-stopping-to-plan-bdbe24a41c13\">The Calm After the Storm: When Victory Means Stopping to Plan</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-calm-after-the-storm-when-victory-means-stopping-to-plan-bdbe24a41c13?source=rss----982e21163f8b---4",
    "thumbnail": null
  },
  {
    "title": "The Great Refactor: Six Weeks in Eighteen Days",
    "excerpt": "“You did it!”October 7, 2025Tuesday morning at 7:04 AM, my Chief Architect began planning GREAT-4F — the final piece of intent classification. Improve classifier accuracy to 95%+, document the canonical handler pattern, establish quality gates protecting everything we’d built.One epic remaining a...",
    "url": "https://medium.com/building-piper-morgan/the-great-refactor-six-weeks-in-eighteen-days-dbf652a9a5bd?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 14, 2025",
    "publishedAtISO": "Tue, 14 Oct 2025 12:27:16 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/dbf652a9a5bd",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*efz27rk4UzbkTNLYUaMcgg.png",
    "fullContent": "<figure><img alt=\"A robot wins a race with a humna chering and other robots looking on\" src=\"https://cdn-images-1.medium.com/max/1024/1*efz27rk4UzbkTNLYUaMcgg.png\" /><figcaption>“You did it!”</figcaption></figure><p><em>October 7, 2025</em></p><p>Tuesday morning at 7:04 AM, my Chief Architect began planning GREAT-4F — the final piece of intent classification. Improve classifier accuracy to 95%+, document the canonical handler pattern, establish quality gates protecting everything we’d built.</p><p>One epic remaining after that: GREAT-5, the validation suite that would lock in all achievements from GREAT-1 through GREAT-4.</p><p>By 6:52 PM, both were complete.</p><p>At 7:01 PM, Chief Architect confirmed: “CORE-GREAT ready to close — all 5 GREAT epics complete.”</p><p>September 20 to October 7. Eighteen days. Five major epics estimated at six weeks or more. Production-ready foundation with 142+ tests, 100% passing, comprehensive quality gates operational.</p><p>The pause the precipitated this effort came from one of my lowest points on this project, my I sincerely wondered if this had all been a fascinating waste of my time. Now less than three weeks later I feel more confident than ever that I’m building something real.</p><p>This is the story of how Tuesday brought another milestone for what four months of systematic work had built toward. Not through heroic effort, but through discovering that most of the work had already been done — it just needed the final 5% found, fixed, and validated.</p><h3>The two-minute ADR</h3><p>At 7:51 AM, Code agent deployed to create ADR-039: Canonical Handler Pattern documentation. Estimated time: 20–30 minutes. Actual time: 2 minutes. Why do they pad these estimates? They know they write fast, right?</p><p>The ADR wasn’t shorter or lower quality than expected. It was comprehensive: 399 lines documenting the dual-path architecture, explaining when to use canonical handlers versus workflow orchestration, including performance metrics from GREAT-4E, providing troubleshooting guidance.</p><p>What made it fast wasn’t the agent writing faster. It was the specification being clearer.</p><p>The gameplan didn’t say “write an ADR about canonical handlers.” It said:</p><blockquote><em>Document the dual-path architecture: WHAT (two routing paths exist), WHY (performance vs capability trade-offs), WHEN (which path for which requests), HOW (decision criteria), PERFORMANCE (actual metrics from GREAT-4E benchmarks).</em></blockquote><p>Clear specifications enable speed. When the agent knows exactly what “done” looks like, implementation becomes straightforward.</p><p>This pattern repeated throughout Tuesday.</p><p>Phase 1 (QUERY fallback patterns): estimated 30–40 minutes, actual 14 minutes. GREAT-5 Phase 3 (integration tests): estimated 45–60 minutes, actual 15 minutes.</p><p>Not because work was skipped. Because foundations were solid and requirements were clear.</p><h3>The missing definitions</h3><p>At 9:40 AM, Cursor completed Phase 2 of GREAT-4F: enhancing the LLM classifier prompts.</p><p>The discovery was almost embarrassing in its simplicity.</p><p>The classifier prompt didn’t include definitions for the five canonical categories. This feels like the kind of shortcut/oversight that plagued our coding process for most of the first few months.</p><p>The categories existed. The handlers worked. The routing was correct. The tests all passed. But the LLM classifier — the system that decides which category a natural language query belongs to — had never been told what the canonical categories actually were.</p><p>When someone said “What day is it?” the classifier would see:</p><ul><li>Available categories: QUERY, CREATE, UPDATE, SEARCH, EXECUTION, ANALYSIS, SYNTHESIS, STRATEGY, LEARNING, GUIDANCE, UNKNOWN</li><li>Query: “What day is it?”</li><li>Decision: Probably QUERY (default when unsure)</li></ul><p>TEMPORAL didn’t appear in the options because the prompt never mentioned it existed.</p><p>The fix: Add five lines defining canonical categories in the classifier prompt.</p><p>The impact: +11 to 15 percentage points accuracy improvement.</p><p>PRIORITY went from 85–95% accuracy to 100% (perfect classification). TEMPORAL jumped to 96.7%. STATUS to 96.7%. All three exceeding the 95% target.</p><p>It’s a weird feeling to be both annoyed that something so simple was skipped and hiding in plain site as well as relieved and satisfied after fixing it.</p><p>This is the flip side of the “75% pattern.” Sometimes you discover infrastructure is better than expected. Sometimes you discover a simple fix dramatically improves things. But both require actually looking.</p><p>The categories worked in isolation. Unit tests passed. Integration tests with canonical queries worked because those tests bypassed the LLM classifier entirely — they called handlers directly.</p><p>The gap only appeared when testing the full flow: natural language → LLM classification → canonical handler routing.</p><p>Comprehensive testing reveals assumptions. And sometimes those assumptions are “surely someone told the classifier what these categories mean.”</p><h3>The permissive test anti-pattern</h3><p>Throughout Tuesday morning, a pattern kept appearing in the test suite:</p><pre># Permissive (accepts both success and failure):<br>assert response.status_code in [200, 404]<br><br># Strict (requires success):<br>assert response.status_code == 200</pre><p>The permissive version accepts both “working correctly” (200) and “endpoint doesn’t exist” (404) as valid test passes. When I saw that I was like “wait, wat?” How is “endpoint doesn’t exist” a success state? Because a reply was returned? Come on!</p><p>GREAT-5 Phase 1 systematically eliminated this pattern. Twelve permissive assertions replaced with strict requirements. The immediate result: tests started failing.</p><p>Good!</p><p>The failures revealed:</p><ul><li><strong>IntentService initialization errors</strong>: Test fixtures weren’t properly setting up the service</li><li><strong>Two cache endpoint bugs</strong>: AttributeError exceptions in production code</li><li><strong>Health endpoint protection gaps</strong>: Tests accepting failures that would break monitoring</li></ul><p>None of these were caught by permissive tests because permissive tests don’t catch problems — they hide them. Seriously, who writes permissive tests anyhow? Who trained the LLMs to do that?</p><p>The philosophy difference:</p><ul><li><strong>“Make tests pass”</strong>: Write tests that accept current behavior, even if broken</li><li><strong>“Make code work”</strong>: Write strict tests that force code to meet requirements</li></ul><p>Permissive tests create false confidence. Everything appears to work because tests pass. But the tests are lying — they pass whether code works or not.</p><p>By end of Phase 1, all permissive patterns were eliminated. Tests now enforce actual requirements. Which meant Phase 1 also had to fix the code that failed strict tests — including two production bugs that had been lurking undetected.</p><p>This is the unglamorous side of quality work. It’s not adding features. It’s making tests honest about what they validate.</p><h3>Quality gates as compound momentum</h3><p>GREAT-5’s goal was establishing additional quality gates protecting all GREAT-1 through GREAT-4 achievements. The existing gates were:</p><ul><li>Intent classification tests</li><li>Performance regression detection</li><li>Coverage enforcement (80%+)</li><li>Bypass detection</li><li>Contract validation</li></ul><p>To this we were now adding:</p><ol><li><strong>Zero-tolerance regression suite</strong>: Critical infrastructure must work, no exceptions</li><li><strong>Integration test coverage</strong>: All 13 intent categories validated end-to-end</li><li><strong>Performance benchmarks</strong>: Lock in 602K req/sec baseline from GREAT-4E</li><li><strong>CI/CD pipeline verification</strong>: 2.5-minute runtime with fail-fast design</li></ol><p>The interesting discovery: most of these already existed.</p><p>CI/CD pipeline? Already excellent, needed zero changes. Performance benchmarks? GREAT-4E had validated them, just needed test suite integration. Load testing? Cache validation tests already proved efficiency.</p><p>What remained was:</p><ul><li>Enhancing regression tests with strict assertions</li><li>Creating comprehensive integration tests</li><li>Fixing the bugs strict tests revealed</li><li>Documenting what quality gates exist and why</li></ul><p>GREAT-5 took 1.8 hours (109 minutes of actual work). Not because the work was small, but because foundations were already solid.</p><p>This is compound momentum visible: each previous epic made this one easier. GREAT-4E’s performance validation became GREAT-5’s benchmark baseline. GREAT-3’s plugin architecture became GREAT-5’s integration test framework. GREAT-2’s spatial intelligence became GREAT-5’s multi-interface validation.</p><p>Nothing built in isolation. Everything building on everything else.</p><h3>The completion moment</h3><p>At 1:15 PM, Chief Architect declared GREAT-4 complete.</p><p>All six sub-epics (4A through 4F) finished. Intent classification system production-ready:</p><ul><li>13/13 categories fully implemented</li><li>95%+ accuracy for core categories</li><li>142+ query variants tested</li><li>Zero timeout errors through graceful fallback</li><li>Sub-millisecond canonical response time</li><li>84.6% cache hit rate with 7.6× speedup</li></ul><p>By 6:52 PM, GREAT-5 was complete as well:</p><ul><li>37 tests in comprehensive quality gate suite</li><li>Zero-tolerance regression protection</li><li>Performance baseline locked at 602K req/sec</li><li>All 13 intent categories validated through all interfaces</li><li>CI/CD pipeline verified operational</li></ul><p>Completing an entire fifth epic after finishing the last several issues in the previous epic seems like a leap, but GREAT-5 is about locking down the work of the earlier epics, and it benefited greatly from all the cleanup work that preceded it.</p><p>At 7:01 PM, Chief Architect closed CORE-GREAT: “All 5 GREAT epics complete.”</p><p>The timeline:</p><ul><li><strong>GREAT-1</strong> (Orchestration Core): September 20–27</li><li><strong>GREAT-2</strong> (Integration Cleanup): September 28 — October 1</li><li><strong>GREAT-3</strong> (Plugin Architecture): October 2–4</li><li><strong>GREAT-4</strong> (Intent Universal): October 5–7</li><li><strong>GREAT-5</strong> (Quality Gates): October 7</li></ul><p>Total: 18 days from start to production-ready foundation. When the Chief Architect scoped this at six to seven weeks I was hoping (and to be honest, expecting) that it would not take quite that long, but this far exceeded my expectations.</p><h3>What six weeks in eighteen days means</h3><p>I’m not really talking about working faster and definitely not about cutting corners. This is about systematic work revealing that foundations were stronger than expected.</p><p>The pattern across all five epics:</p><p><strong>Phase −1 verification</strong> consistently found infrastructure better than assumed. Two-layer caching already operational. Spatial intelligence already integrated. Plugin patterns already proven. Each epic started further along than the gameplan estimated.</p><p><strong>The 75% pattern</strong> appeared repeatedly. Categories implemented, patterns missing. Handlers exist, definitions missing. Tests passing, strictness missing. The missing 25% wasn’t architecture — it was enumeration, documentation, and validation.</p><p><strong>Compound momentum</strong> made each epic faster. GREAT-1’s orchestration patterns became GREAT-4’s intent routing. GREAT-2’s integration cleanup became GREAT-3’s plugin foundation. GREAT-3’s plugin architecture became GREAT-4’s category handlers.</p><p><strong>Autonomous agent work</strong> accelerated when patterns were clear. The 2-minute ADR. The 14-minute QUERY fallback. The 15-minute integration test suite. Not because agents write faster, but because specifications were clearer and foundations were proven.</p><p><strong>Independent validation</strong> caught what automated testing missed. The 69% thinking it’s 100% moment. The missing classifier definitions. The permissive test anti-pattern. Systematic verification refusing to accept “appears complete” without proving “actually complete.”</p><p>None of these are silver bullets. Each requires the others to work.</p><ul><li><strong>Clear specifications without solid foundations</strong>: agents build the wrong thing quickly</li><li><strong>Solid foundations without verification</strong>: incomplete work ships thinking it’s complete</li><li><strong>Verification without clear quality standards</strong>: you catch problems but don’t know what “good” looks like.</li></ul><p>The methodology is the integration of all these pieces. And it took four months of development to get here — this isn’t where we started, it’s what we built toward.</p><h3>The calm of completion</h3><p>Tuesday evening feels different from Monday evening, which felt different from Sunday evening.</p><p>Sunday: Exhilaration of pattern coverage jumping 24% → 92% in fifteen minutes.</p><p>Monday: Relief that autonomous agent work validated correctly and scope gaps were caught.</p><p>Tuesday: Calm. Centered. Relaxed!</p><p>Not the calm before something. The calm of arriving. The foundation work is complete. The refactoring is done. The quality gates are operational. The tests all pass.</p><p>What comes next is building on this foundation, not replacing it.</p><p>We made issues for some of the items we postponed as somewhat out of scope: MVP-ERROR-STANDARDS will standardize error handling. CORE-TEST-CACHE will fix a minor test environment issue. CORE-INTENT-ENHANCE will optimize IDENTITY and GUIDANCE accuracy when it becomes important.</p><p>But none of those are GREAT epics. They’re incremental improvements to a foundation that’s already solid. This isn’t the end. It isn’t even the beginning of the end, to coin a phrase, but it might be the end of the beginning.</p><p>The Great Refactor is complete. Five epics, eighteen days, production-ready foundation. Achieved without heroic effort or accepting technical debt or cutting corners to ship faster.</p><p>Through systematic work discovering that the infrastructure was better than we thought, enumerating what remained, and validating that it all held together.</p><p>The methodology working exactly as designed.</p><p>Which is, for the third time this week, far more satisfying than dramatic rescues.</p><h3>What this enables</h3><p>With GREAT-1 through GREAT-5 complete, Piper Morgan now has:</p><p><strong>Orchestration</strong>: Workflow factory coordinating all complex operations</p><p><strong>Integration</strong>: Clean plugin architecture for all external services</p><p><strong>Classification</strong>: Universal intent system routing all natural language</p><p><strong>Performance</strong>: Sub-millisecond canonical handlers, 602K req/sec sustained</p><p><strong>Quality</strong>: Comprehensive gates protecting all critical paths</p><p>The foundation enables alpha release to real users. Multi-user support operational. Spatial intelligence providing context-appropriate responses. Quality gates preventing regression. Performance validated under load.</p><p>Everything that comes next builds on this. Not replacing it, not refactoring it again, not discovering it was wrong. Just building the features that this foundation enables.</p><p>That’s what eighteen days of systematic work delivered. Not just working software, but a foundation trustworthy enough to build on without constantly looking over your shoulder wondering if it’ll collapse.</p><p>The calm of completion is knowing the foundation holds.</p><p><em>Next on Building Piper Morgan: The Calm After the Storm — When Victory Means Stopping to Plan, as we resist the temptation to immediately sprint toward Alpha and instead take time to properly assess our position and chart the sustainable path forward.</em></p><p><em>Have you completed a major milestone faster than expected? Did you immediately charge forward, or did you pause to reassess? What would you do differently?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=dbf652a9a5bd\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-great-refactor-six-weeks-in-eighteen-days-dbf652a9a5bd\">The Great Refactor: Six Weeks in Eighteen Days</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-great-refactor-six-weeks-in-eighteen-days-dbf652a9a5bd?source=rss----982e21163f8b---4",
    "thumbnail": null
  },
  {
    "title": "The Agent That Saved Me From Shipping 69%",
    "excerpt": "“I’ve got you!”October 6, 2025Monday morning started with what looked like straightforward work. GREAT-4C needed completion: add spatial intelligence to the five canonical handlers, implement error handling, enhance the cache monitoring we’d discovered Sunday. Estimated effort: a few hours of sys...",
    "url": "https://medium.com/building-piper-morgan/the-agent-that-saved-me-from-shipping-69-aae61fe91f37?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 13, 2025",
    "publishedAtISO": "Mon, 13 Oct 2025 13:32:49 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/aae61fe91f37",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*5m_jivqzx7qhjXd-CkZESA.png",
    "fullContent": "<figure><img alt=\"A robot sailor saves a person who has fallen overboard\" src=\"https://cdn-images-1.medium.com/max/1024/1*5m_jivqzx7qhjXd-CkZESA.png\" /><figcaption>“I’ve got you!”</figcaption></figure><p><em>October 6, 2025</em></p><p>Monday morning started with what looked like straightforward work. GREAT-4C needed completion: add spatial intelligence to the five canonical handlers, implement error handling, enhance the cache monitoring we’d discovered Sunday. Estimated effort: a few hours of systematic implementation following proven patterns.</p><p>By 9:00 AM, GREAT-4C was complete. One hour and thirty-nine minutes from session start to final validation. All seven acceptance criteria met. The multi-user foundation was operational — no more hardcoded references to specific users, just spatial intelligence providing context-appropriate detail levels.</p><p>Part of me doesn’t love it when I can’t finish the chunk of work I started in the same day, so it felt good to wrap up GREAT-4C before plunging ahead to GREAT-4D: implementing the remaining intent handlers.</p><p>The gameplan said we needed two categories. EXECUTION and ANALYSIS — the handlers for “create a GitHub issue” and “analyze this data” type requests.</p><p>By 2:05 PM, we’d discovered the actual scope: thirteen intent categories, not two.</p><p>And if the Code agent hadn’t caught the gap during Phase Z validation that we do while tidying up when we think a job is done, we would have shipped thinking we had 100% coverage when we actually had 69%.</p><h3>Morning: The work that goes according to plan</h3><p>GREAT-4C’s goal was removing the last obstacles to multi-user support. The canonical handlers — those five categories (TEMPORAL, STATUS, PRIORITY, GUIDANCE, IDENTITY) that could respond without querying the LLM — all had hardcoded references to the configuration details of a specific user, our only user so far, me.</p><p>The spatial intelligence integration followed a clear pattern. Each handler needed to:</p><ol><li>Check the spatial context for detail level (GRANULAR, EMBEDDED, or DEFAULT)</li><li>Format responses appropriately (15 characters for embedded, 250–550 for granular)</li><li>Gracefully degrade if spatial data unavailable</li><li>Maintain sub-millisecond performance</li></ol><p>Code agent implemented this across all five handlers in phases:</p><ul><li>STATUS handler: 7:30 AM (5 minutes)</li><li>PRIORITY handler: 7:37 AM (3 minutes)</li><li>TEMPORAL handler: 7:40 AM (3 minutes)</li><li>GUIDANCE handler: 7:43 AM (3 minutes)</li><li>IDENTITY handler: 7:46 AM (3 minutes)</li></ul><p>Total implementation time: 17 minutes.</p><p>If we expected something to take an hour and the bots say it took five minutes, I get suspicious and want to see more proof, but 17 minutes feels pretty solid. I still scrutinize the reports to make sure they’re taking no shortcuts and not dismissing some difficulties as unimportant and OK to ignore or postpone.</p><p>Any actual speed was the result of clarity. Each handler followed the same pattern. The spatial intelligence system already existed from GREAT-2. The formatters were tested. The only new work was connecting pieces that already fit together.</p><p>By 8:15 AM, Cursor had completed error handling — graceful degradation when calendars fail to load, files go missing, or data comes back empty. By 8:30 AM, Code had enhanced the cache monitoring we’d discovered Sunday (two-layer architecture: file-level and session-level caching both operational).</p><p>At 9:00 AM, my Lead Developer declared GREAT-4C complete. All acceptance criteria met in 1 hour 39 minutes.</p><p>This is what systematic work looks like when foundations are solid. Not heroic effort, just clear patterns executed cleanly. Just don’t let me brag about this too much. NO SPOILERS but we did later find a few gaps.</p><h3>The scope gap discovery</h3><p>GREAT-4D started at 10:20 AM with what looked like straightforward scope: implement handlers for EXECUTION and ANALYSIS intent categories.</p><p>The investigation phase revealed something unexpected. Lead Developer ran filesystem checks looking for the placeholder code that would need replacing:</p><pre>grep -r &quot;[A KEYWORD THAT WAS MENTIONED]&quot; services/<br>grep -r &quot;TODO.*EXECUTION&quot; services/<br>grep -r &quot;placeholder.*ANALYSIS&quot; services/</pre><p>Results: No matches found. Hmm.</p><p>This triggered the GREAT-1 truth investigation. What does the system actually do when it receives EXECUTION or ANALYSIS intents?</p><p>The answer: Routes to workflow handlers through QueryRouter, not canonical handlers.</p><p>But QueryRouter had been replaced by the workflow factory during GREAT-1. The old routing was gone. The new routing existed but had never been validated for these categories.</p><p>Testing revealed the actual state: _handle_generic_intent contained a placeholder that returned &quot;I can help with that!&quot; for EXECUTION and ANALYSIS requests without actually executing or analyzing anything.</p><p>Not a complete failure — the system didn’t crash. Just quietly pretended to work while doing nothing. We would have caught this next time I did end-to-end testing, but that would have set off an archaeological expedition to figure out just when and where we had left something unfinished.</p><p>This was our chance to fix it now.</p><h3>The thirteen-category realization</h3><p>At 12:25 PM, Chief Architect redefined GREAT-4D with simplified scope following the QUERY pattern. Implement EXECUTION and ANALYSIS handlers the same way QUERY worked: delegate to the workflow orchestrator, handle the response, return results.</p><p>Code agent deployed for Phase 1 at 12:36 PM. By 12:42 PM, EXECUTION handler was complete with the placeholder removed. Cursor completed ANALYSIS handler by 1:02 PM. Testing validated both worked correctly by 1:22 PM.</p><p>Everything looked complete.</p><p>Then at 1:40 PM, during Phase Z final validation, Lead Developer discovered something: four additional categories were returning placeholders.</p><p>SYNTHESIS, STRATEGY, LEARNING, UNKNOWN — all routing to _handle_generic_intent which still contained placeholder logic.</p><p>How had this escaped us? Anyhow, we caught it just in time!</p><p>The math:</p><ul><li>8 categories implemented in GREAT-4A through GREAT-4C</li><li>2 categories just implemented in GREAT-4D Phases 1–2</li><li>4 categories discovered in Phase Z</li><li>Total: 14 categories (13 real + UNKNOWN fallback)</li></ul><p>Shipping after Phase 2 would have meant: 10/13 categories working = 77% coverage, not 100%.</p><p>But we thought we were done. The gameplan said “implement EXECUTION and ANALYSIS” and we’d done a form of that. The gap wasn’t in execution — it was in understanding the actual scope.</p><h3>The autonomous decision</h3><p>At 1:42 PM, Code agent made an autonomous decision.</p><p>Instead of reporting the gap and waiting for new instructions, Code self-initiated implementation of the four missing handlers:</p><pre>SYNTHESIS: Combine information from multiple sources<br>STRATEGY: Develop plans or approaches  <br>LEARNING: Capture knowledge or lessons<br>UNKNOWN: Handle unclassifiable requests gracefully</pre><p>This wasn’t some sort of emergent go-getter-ism, but a weird side effect of context-window management. When Code’s window gets too full it “compacts” the context, digesting it to a summary. During these several minute exercises it effectively goes into a fugue state and then recovers, reads the summary and resumes.</p><p>This time compaction happened just as it was writing it’s Phase 0 (investigation) report. The drill is we (the Lead Dev and I) review the report and then provide a prompt for Phase 1. When it woke up from its trance this time, it did not report in to me but just read the gameplan and immediately started working on Phase 1 based on the more general goals (somewhat risky if we don’t provide a well crafted prompt with guardrails, etc.)</p><p>The agent worked independently for nine minutes. No prompts. No clarification questions. Just systematic implementation following the same pattern EXECUTION and ANALYSIS had used.</p><p>At 1:51 PM, Code reported completion:</p><ul><li>454 lines of handler logic added</li><li>13/13 intent categories now fully handled</li><li>All tests passing</li><li>Ready for independent validation</li></ul><p>The question: Could we trust thid autonomous work?</p><h3>Independent validation as methodology</h3><p>At 1:55 PM, Cursor deployed for independent validation with explicit instructions:</p><blockquote><em>Review all autonomous work with skeptical eye. Verify:</em></blockquote><blockquote><em>- Code quality matches project standards<br>- Patterns align with existing handlers<br>- Tests actually validate behavior<br>- No corners cut for speed</em></blockquote><p>Cursor’s validation took ten minutes. The results:</p><p><strong>Code Quality</strong>: ✅ … Matches project standards, follows DDD separation, proper error handling</p><p><strong>Pattern Alignment</strong>: ✅ … All four handlers use proven EXECUTION/ANALYSIS pattern, no novel approaches</p><p><strong>Test Coverage</strong>: ✅ … 13 comprehensive tests covering all categories, realistic scenarios</p><p><strong>Completeness</strong>: ✅ … No gaps, no TODOs, no placeholder comments</p><p>At 2:05 PM, Cursor confirmed: All autonomous work is correct and production-ready. Lead Developer’s declaration: “GREAT-4D is actually complete. True 100% coverage achieved.”</p><p>The autonomous work wasn’t cowboy coding or rogue agent behavior. It was an agent having clear patterns to follow, and completing necessary work systematically. Still, I couldn’t trust it without the independent validation that verified it.</p><h3>The infrastructure near-misses</h3><p>Later that day, GREAT-4E validation uncovered severl critical issues that had been lurking, undetected:</p><h4><strong>The missing import path prefix</strong></h4><pre># Wrong (broken):<br>from personality_integration import enhance_response<br><br># Correct (working):<br>from web.personality_integration import enhance_response</pre><p>This broke imports across multiple files. Tests hadn’t caught it because the test environment had different Python path configuration than production would.</p><p>This also pointed to a deeper problem. Why is the personality integration happening at the level of the web app! It should be a universal function across all the user-facing surfaces. We noted this for refactoring.</p><h4><strong>The missing /health endpoint</strong></h4><p>The health check endpoint had been removed at some point, but 36 references to it remained across the codebase. Load balancer integration, monitoring tools, deployment scripts — all expecting an endpoint that didn’t exist.</p><p>It’s embarassing when I realize I’ve broken something without realizing it for weeks, but it’s also gratifying that we finally caught and fixed it.</p><p>Both issues were caught by GREAT-4E’s comprehensive validation before any alpha users saw them. The systematic approach — validate across all interfaces, check all entry points, verify all critical endpoints — prevented shipping broken infrastructure.</p><h3>What “69% thinking it’s 100%” means</h3><p>If we’d stopped GREAT-4D after Phase 2 (implementing EXECUTION and ANALYSIS), the system would have appeared complete:</p><ul><li>All planned handlers implemented âœ…</li><li>All tests passing âœ…</li><li>Acceptance criteria met âœ…</li><li>Ready for production âœ…</li></ul><p>But actual coverage: 10/13 categories working = 77% (or 69% if you count by code paths).</p><p>The three categories we would have missed:</p><ul><li>SYNTHESIS requests → placeholder response</li><li>STRATEGY requests → placeholder response</li><li>LEARNING requests → placeholder response</li></ul><p>Not catastrophic failures. Just quiet degradation where the system pretends to work but doesn’t actually do anything useful. I recognize that this is happening partly due to my experimental process, vagaries of LLM coders, even my own experience, but at the same time I can’t help wondering how often professional systems ship in this kind of state — appearing complete but quietly failing on edge cases nobody tested.</p><p>The methodology that caught it this time:</p><ol><li><strong>Phase Z validation</strong> as standard practice</li><li><strong>Independent verification</strong> by second agent</li><li><strong>Comprehensive testing</strong> across all categories</li><li><strong>Agents empowered</strong> to identify scope gaps</li></ol><p>Not heroic debugging. Just systematic verification refusing to accept “appears complete” without validating “actually complete.”</p><h3>The day’s completion</h3><p>By 2:10 PM, GREAT-4D was pushed to production:</p><ul><li>13/13 intent categories fully handled (100% coverage)</li><li>454 lines of handler logic</li><li>32 comprehensive tests passing</li><li>Critical infrastructure gaps fixed</li><li>Independent validation confirmed</li></ul><p>Total duration: ~3 hours including investigation and scope expansion.</p><p>The work that appeared straightforward (implement two handlers) turned out to be more complex (implement six handlers, fix infrastructure issues, validate everything). But the methodology caught every gap before it became a production problem.</p><p>Not because we’re exceptionally careful. Because the systematic approach makes it hard to ship incomplete work thinking it’s complete.</p><h3>What Tuesday would bring</h3><p>Monday evening set up Tuesday’s final push: improve classifier accuracy to 95%+, establish comprehensive quality gates, and complete the entire GREAT refactor series.</p><p>But sitting here Monday night, what strikes me is how the autonomous agent work validated a key principle: agents can make good decisions when they have clear patterns to follow and independent validation confirms their work.</p><p>The Code agent didn’t invent new patterns or make risky architectural choices. It recognized a gap, followed proven patterns, and delivered work that passed independent scrutiny.</p><p>That’s not artificial general intelligence. That’s systematic work applied by an agent that understands the system’s patterns well enough to extend them correctly.</p><p>The methodology working exactly as designed. Which is, once again, far more satisfying than heroic rescues.</p><p><em>Next on Building Piper Morgan: The Great Refactor — Six Weeks in Eighteen Days, in which complete the foundational transformation that seemed impossible on the original timeline, proving that systematic work with quality gates doesn’t even slow you down — it compounds your velocity.</em></p><p><em>Have you experienced projects where systematic validation caught scope gaps before shipping? What methods work for discovering “we thought we were done but actually have 30% remaining”?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=aae61fe91f37\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-agent-that-saved-me-from-shipping-69-aae61fe91f37\">The Agent That Saved Me From Shipping 69%</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-agent-that-saved-me-from-shipping-69-aae61fe91f37?source=rss----982e21163f8b---4",
    "thumbnail": null
  },
  {
    "title": "When 75% Turns Out to Mean 100%",
    "excerpt": "“…and we’re done.”October 5, 2025Sunday morning at 7:39 AM, my Chief Architect started reviewing what needed to happen to finish GREAT-4. Intent classification was working — we had that much confirmed from GREAT-3’s plugin architecture completion the day before. But we needed comprehensive patter...",
    "url": "https://medium.com/building-piper-morgan/when-75-turns-out-to-mean-100-cb4864b0cfc6?source=rss----982e21163f8b---4",
    "publishedAt": "Oct 13, 2025",
    "publishedAtISO": "Mon, 13 Oct 2025 13:00:32 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/cb4864b0cfc6",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*_vumZG9Y4OcYnPvInct0aQ.png",
    "fullContent": "<figure><img alt=\"A robot builder puts the final touches on a model house\" src=\"https://cdn-images-1.medium.com/max/1024/1*_vumZG9Y4OcYnPvInct0aQ.png\" /><figcaption>“…and we’re done.”</figcaption></figure><p><em>October 5, 2025</em></p><p>Sunday morning at 7:39 AM, my Chief Architect started reviewing what needed to happen to finish GREAT-4. Intent classification was working — we had that much confirmed from GREAT-3’s plugin architecture completion the day before. But we needed comprehensive pattern coverage, proper documentation, universal enforcement.</p><p>We were committed to taking as long as it took to get it done.</p><p>By 9:00 PM — 13.5 hours later — GREAT-4 was functionally complete. All eight intent categories fully implemented. Pattern coverage at 92%. Performance validated at 120× to 909× better than targets. Cache efficiency at 50% hit rate with 10–30× latency reduction.</p><p>This wasn’t heroic effort or cutting corners. It was the infrastructure being better than we thought, the patterns we’d already built doing more than we realized, and systematic work revealing that sometimes “75% complete” actually meant “nearly 100% complete, really just needs the last 25% discovered and documented.”</p><h3>The pattern that keeps recurring</h3><p>Saturday’s GREAT-3 completion had taken three days to go from hardcoded imports to production-ready plugin architecture. The final metrics showed performance margins we hadn’t expected: 909× faster than target on concurrent operations, 120× better on overhead.</p><p>I was starting to feel kind of confident in my processes again.</p><p>Sunday morning started with similar assumptions: intent classification would need significant implementation work. We knew the categories existed (QUERY, CREATE, UPDATE, SEARCH, TEMPORAL, STATUS, PRIORITY, GUIDANCE). We knew the system could classify intents. But comprehensive pattern coverage? That would need building.</p><p>At 1:47 PM, the Lead Developer reported Phase 1 results from testing 25 canonical queries against the pattern matching system.</p><p>Pass rate: 24%.</p><p>Nineteen queries out of twenty-five were failing to match patterns. “What day is it?” returned no pattern match. “Show me high priority items” failed. “What’s my calendar look like?” no match.</p><p>The categories were implemented. The routing worked. The handlers existed. The tests proved the infrastructure was operational. But the patterns — the specific phrases and variations that real users would actually say — those were missing.</p><p>The architecture wasn’t wrong. We had just never yet yet systematically enumerated how people actually ask for temporal information, status updates, or priority filters.</p><h3>Adding patterns, not rebuilding systems</h3><p>The fix wasn’t architectural. It was systematic enumeration.</p><p>By 2:02 PM — just 15 minutes of Code agent work — we had 22 new patterns added:</p><ul><li>TEMPORAL: 7 → 17 patterns</li><li>STATUS: 8 → 14 patterns</li><li>PRIORITY: 7 → 13 patterns</li></ul><p>Testing the same 25 canonical queries: 92% pass rate (23/25).</p><p>The two remaining failures were edge cases requiring different handling, not actual patter ngaps. The 92% represented genuine coverage of how users would naturally phrase requests in those three categories.</p><p>Performance: sub-millisecond. All pattern matching happened in 0.10–0.17ms average. The overhead of checking 44 patterns across three categories was essentially free.</p><p>This is the “75% pattern” that keeps appearing in Piper Morgan’s development: the infrastructure exists, it’s solid, it works correctly. What’s missing is the last 25% of enumeration, documentation, and edge case handling. Somehow my bad personal habits of not always dotting the <em>i</em> or crossing the<em> t</em> were showing up in my team’s results.</p><h3>The architectural clarity moment</h3><p>Around 4:04 PM, we hit a question that we had never really thought through since long before GREAT-4 planning began.</p><p>The question: Do structured CLI commands need intent classification?</p><p>The initial assumption: Yes, everything should go through intent classification for consistency and monitoring.</p><p>By talking it through we realized: Structure IS intent.</p><p>When someone types piper issue create &quot;Fix the bug&quot;, the command structure itself explicitly declares the intent. CREATE category, issue type, specific parameters. There&#39;s no ambiguity requiring classification.</p><p>Intent classification exists to handle ambiguous natural language input: “Can you help me with this bug?” or “I need to track this problem” or “Make a note about the login issue.” The system needs to figure out if that’s CREATE, UPDATE, SEARCH, or something else entirely.</p><p>But piper issue create has zero ambiguity. The structure already encodes all the information classification would provide.</p><p>This clarity prevented unnecessary work. No converting structured commands to go through classification. No forcing architectural consistency where it would add complexity without value. Just clear boundaries: natural language gets classified, structured commands express intent explicitly.</p><p>It is kind of fascinating how often these moments of architectural clarity —especially when you realize what you DON’T need to do — save time and energy.</p><p>We had to sort through another item thatwas confusing code, which was whether the personality enhancement layer needed to be applied to the user intent layer.</p><p>This one is a no-brainer. That layer is there to make Piper personable, not to help interpret users. Personality enhancement is for processing OUTPUT, not INPUT. The system has already determined intent and selected a response. Personality enhancement makes that response more natural. Likewise, it doesn’t need to classify the intent of the output — it already knows what the output is for.</p><p>The minutes we took discussing and clarifying this issue surely saved me hours of unnecessary implementation and future debugging.</p><h3>The 100% coverage realization</h3><p>By 4:30 PM, after investigating what appeared to be 16–20 bypass cases needing conversion to intent classification, we discovered something surprising:</p><p>Coverage was already at 100% for natural language input.</p><p>The “bypasses” that looked like gaps were:</p><ul><li>Structured CLI commands (don’t need classification)</li><li>Output processing (personality enhancement)</li><li>Internal system calls (already using intent)</li></ul><p>Every actual natural language entry point — web chat, Slack messages, conversational CLI — already routed through intent classification. The system we thought needed building was already operational.</p><p>What remained was enforcement: making sure new code couldn’t bypass intent classification accidentally. Not implementing coverage, but protecting coverage that already existed.</p><h3>Performance validation beyond expectations</h3><p>The afternoon’s GREAT-4D work included running actual benchmarks against the plugin system we’d built in GREAT-3. Sunday was the first time we measured real performance under realistic conditions.</p><p>It was architectural validation. The thin wrapper pattern we’d documented Saturday morning — where plugins are minimal adapters delegating to routers — turned out to cost essentially nothing while providing all the benefits of lifecycle management, discoverability, and configuration control.</p><p>The wrapper pattern overhead: 0.041 microseconds. Forty-one billionths of a second.</p><p>That’s not “we made it fast.” That’s “we picked abstractions that don’t cost anything.”</p><h3>What systematic completion looks like</h3><p>By 9:00 PM, GREAT-4 was functionally complete:</p><ul><li>Pattern coverage: 24% → 92% for tested categories</li><li>All 8 intent categories fully implemented</li><li>Performance validated with massive safety margins</li><li>Universal enforcement architecture designed</li><li>Cache efficiency: 50% hit rate, 10–30× latency reduction</li><li>Zero timeout errors through graceful fallback</li></ul><p>I was tired but exhilarated. On the one hand I had been able to oversee this work with minimal attention, checking in to approve things or paste in the next step from time to time. On the other was preoccupied and thinking about the challenges all day. It was a weekend day, not a work day, but it felt somewhere in the middle.</p><p>The work wasn’t dramatic. No last-minute heroics, no clever hacks that barely worked, no technical debt accepted “to ship faster.” Just systematic discovery of what already existed, enumeration of what was missing, and validation that it all held together.</p><p>The 13.5 hours included:</p><ul><li>Pattern expansion (15 minutes of implementation)</li><li>Architectural clarity discussions (preventing unnecessary work)</li><li>Performance validation (confirming assumptions)</li><li>Documentation (capturing decisions)</li><li>Testing (142 query variants to verify coverage)</li></ul><p>More time spent understanding than building. More effort on “what don’t we need to do” than “what should we build.” More validation than implementation.</p><h3>The 75% pattern explained</h3><p>This is the third or fourth time we’ve hit the “75% pattern” during Piper Morgan’s development:</p><p>The pattern works like this:</p><ol><li>Something appears to need significant work</li><li>Investigation reveals infrastructure already 75% complete</li><li>The missing 25% is enumeration/documentation/polish</li><li>Systematic completion takes hours instead of days</li><li>The result is production-ready because foundation was already solid</li></ol><p>GREAT-3’s plugin architecture (completed Saturday) provided the foundation for GREAT-4’s intent classification. The registry system, lifecycle management, and configuration control patterns all transferred. We weren’t building from scratch — we were extending proven patterns.</p><p>GREAT-2’s integration cleanup had already established the router patterns that intent classification would coordinate. The routing infrastructure existed. Intent classification just needed to determine WHICH router to use.</p><p>Each completed epic makes the next one easier. Not just because code exists, but because patterns are proven, abstractions are validated, and the team (human and AI) understands how the system wants to work.</p><h3>What Monday brings</h3><p>Sunday evening’s completion of GREAT-4 sets up Monday’s work: multi-user support, comprehensive validation, and final polish before alpha release.</p><p>But sitting here Sunday night, what strikes me most is how undramatic the completion felt. No crisis averted, no brilliant insight that saved the day, no desperate debugging session.</p><p>Just systematic work discovering that the infrastructure was better than we thought, enumerating what remained, and validating that it all held together.</p><p>The methodology working exactly as designed. Which is, honestly, far more satisfying than dramatic rescues.</p><p><em>Next on Building Piper Morgan: The Agent That Saved Me From Shipping 69%, when an autonomous agent discovers a critical scope gap during Phase Z validation — proving that independent verification isn’t just process overhead, it’s essential quality protection.</em></p><p><em>Have you experienced the “75% pattern” in your own work — where systematic investigation reveals most of the work is already done, just needs the last 25% enumerated and documented?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cb4864b0cfc6\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/when-75-turns-out-to-mean-100-cb4864b0cfc6\">When 75% Turns Out to Mean 100%</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/when-75-turns-out-to-mean-100-cb4864b0cfc6?source=rss----982e21163f8b---4",
    "thumbnail": null
  },
  {
    "title": "Why the Future of AI UX is Orchestration, Not Intelligence",
    "excerpt": "“You’re so smart, they said! You can do it all, they said!”August 20After months of building with multiple AI agents, a pattern keeps emerging: We create sophisticated systems, lose track of what we built, then rediscover our own achievements through “archaeological” investigation.This recurring ...",
    "url": "/blog/why-the-future-of-ai-ux",
    "publishedAt": "Oct 12, 2025",
    "publishedAtISO": "Sun, 12 Oct 2025 13:37:57 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/8aacc89aecc9",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*-rihqLO116WVnWKXAKSGRw.png",
    "fullContent": "<figure><img alt=\"The specialist robots work together in a kitchen, one timing, one chopping, one cooking while in another scene one robot with eight arms is making a huge mess at the stove\" src=\"https://cdn-images-1.medium.com/max/1024/1*-rihqLO116WVnWKXAKSGRw.png\" /><figcaption><em>“You’re so smart, they said! You can do it all, they said!”</em></figcaption></figure><p><em>August 20</em></p><p>After months of building with multiple AI agents, a pattern keeps emerging: We create sophisticated systems, lose track of what we built, then rediscover our own achievements through “archaeological” investigation.</p><p>This recurring cycle of institutional amnesia may be a bug in our process but for today’s LLM services, it’s a feature that reveals the real UX challenge ahead.</p><h3>The intelligence plateau and the orchestration valley</h3><p>The AI industry is obsessed with reasoning capabilities. Larger context windows, better chain-of-thought, more sophisticated inference. Meanwhile, anyone actually building with AI faces a different problem entirely: How do you coordinate multiple specialized capabilities without losing your mind?</p><p>Anyone reading this series has the right to question what this process may be doing to my mind at this very moment!</p><p>Yesterday we discovered 599 comprehensive smoke tests we’d apparently built and then completely forgotten. Saturday we rediscovered attribution systems we’d implemented but lost track of (in fact, I only just now remembered it again and added it to my notes to include ATTRIBUTION.md to our weekly doc sweep). Two weeks ago we found enterprise-grade feedback APIs sitting in our codebase, unmarked and uncredited.</p><p>The pattern isn’t forgetfulness — it’s that our tools for building are ahead of our tools for remembering.</p><h3>From brilliant generalists to orchestrated specialists</h3><p>The current paradigm assumes one brilliant AI that can handle anything you throw at it. The emerging paradigm recognizes that specialized tools, properly coordinated, deliver better results than generalist intelligence.</p><p>Our accidental prototype:</p><ul><li><strong>Claude Code:</strong> Architecture and systematic implementation</li><li><strong>Cursor Agent:</strong> Targeted debugging and focused fixes</li><li><strong>Chief of Staff: </strong>Coordination and strategic oversight</li><li><strong>Chief Architect: </strong>Decision-making and system design</li></ul><p>Each agent has different context levels, different strengths, different appropriate use cases. The magic isn’t in making any individual agent smarter — it’s in the orchestration patterns that let them work together effectively.</p><p>One thing this enables me to do is to have focused coherent conversations and decision-making processes always at the right level of abstraction. Early on I found that as soon as multiple contexts get mixed you get a mishmash of more generic and sloppy advice and results. It’s kind of like how if you mix too many paints you end up with the same muddy brown.</p><h3>The UX we actually need</h3><p>After coordinating multi-agent workflows for months, I’m realizing that the UX challenges aren’t about reasoning — they’re about:</p><ul><li>Context handoffs: How do you maintain working memory across agent transitions?</li><li>Coordination protocols: How do you deploy the right agent for the right task without overwhelming the human orchestrator?</li><li>Institutional memory: How do you prevent the “forgotten monuments” cycle where sophisticated systems get lost in your own complexity?</li><li>Verification workflows: How do you maintain quality when multiple agents contribute to the same outcome?</li></ul><p>Each of these is critical and urgent in its own way. Getting any of these wrong means you are just injecting chaos into your processes.</p><h3>Throwing intelligence at everything</h3><p>We keep applying intelligence solutions to orchestration problems. Need better coordination? Train a smarter model. Need better memory? Increase context windows. Need better task routing? Build more sophisticated reasoning.</p><p>Except, orchestration isn’t really an intelligence problem.<em> It’s a UX design problem</em>.</p><p>My failed adoption of the TLDR system is a perfect illustration. I absorbed something that sounded cool to me without really understanding it was intended to work with 50ms test timeouts from compiled languages, which ignores Python’s ecosystem realities. More intelligence wouldn’t have fixed the fundamental mismatch where understanding my constraints better would have.</p><h3>Affordances over algorithms</h3><p>UX for AI will be defined by:</p><p><strong>Specialized models</strong> over generalist LLMs. A focused SLM that understands database schemas will outperform a brilliant generalist that has to reason about every query from first principles.</p><p><strong>Orchestration patterns</strong> over individual agent capabilities. The system that deploys the right specialist at the right time beats the system with the smartest individual components.</p><p><strong>Context management</strong> over context windows. Better handoff protocols matter more than larger memory capacity.</p><p><strong>Coordination affordances </strong>over reasoning power. Tools that help humans orchestrate AI workflows effectively will matter more than tools that make individual AI agents more capable.</p><p>I can’t even say how these affordances will look or behave. I’m treading the cowpaths now, and hoping talented UX designers (hey, I’m just a PM these days!) can figure this out and save me all the manual work and cognitive labor I do to provide resilience and coherence via scaffolding, harness, redundancy, and other the other hacks I’ve been picking up through trial and error (and stealing ideas from other people!).</p><h3>The working memory revolution</h3><p>Our recurring “archaeological discovery” pattern reveals the real frontier: building systems that maintain institutional memory across time, people, and context switches.</p><p>Every time we rediscover forgotten excellence, we’re experiencing the same challenge every team building with AI will face: How do you scale human-AI collaboration without losing track of what you’ve accomplished?</p><h3>Orchestration as a new kind of literacy</h3><p>Pretty soon, prompting individual AI agents effectively will stop being the valuable skill (or parlor trick) it is today. What we’re going to look for is the ability to orchestrate multiple specialized AI capabilities without losing coherence.</p><p>Product managers will need orchestration patterns for coordinating AI-augmented workflows across teams.</p><p>Designers will need to make (and use!) affordances for human-AI collaboration that maintain user agency while leveraging AI capabilities.</p><p>Engineers will need architecture patterns for composing AI services without creating coordination overhead.</p><h3>The Piper Morgan thesis</h3><p>While I am definitely building a product management tool, I find I am also prototyping the UX patterns that are like to define human-AI collaboration, or at least point us in the right direction, over the next decade.</p><p>I always knew this was a learning project. I sincerely want ship v1 of Piper Morgan and deliver value to myself and ideally others as well. At the same time it’s been incredibly rewarding just plunging in learning things constantly, and then turning around quickly to share my enthusiasm with all of you.</p><p>What I didn’t realize is that beyond building Piper Morgan, I may be studying just exactly the sort of interesting puzzles and problems and opportunities that the brightest minds in UX and digital software product development need to be figuring out, and fast! (Before the bad guys own it all.)</p><p>My recurring cycle of building sophisticated systems, losing track of them, and rediscovering them through archaeological investigation provides some ongoing comic relief for anyone following along, as well as an endless rollercoaster ride of elation and chagrin for me, and it also happens to be one of the fundamental challenges that every organization building with AI will face.</p><p>Smarter AI isn’t going to get us there, but better orchestration just might.</p><p><em>Next on Building Piper Morgan, we resume the daily narrative on October 5, When 75% Turns Out to Mean 100%.</em></p><p><em>This article was written through multi-agent collaboration, refined through systematic methodology, and documented with full acknowledgment that I’ll probably forget we wrote it and one of my bot pals will rediscover it archaeologically in six months and say “You have to read this amazing article somebody wrote.”</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8aacc89aecc9\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/why-the-future-of-ai-ux-is-orchestration-not-intelligence-8aacc89aecc9\">Why the Future of AI UX is Orchestration, Not Intelligence</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/why-the-future-of-ai-ux-is-orchestration-not-intelligence-8aacc89aecc9?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/8aacc89aecc9-featured.png",
    "slug": "why-the-future-of-ai-ux",
    "workDate": "Oct 12, 2025",
    "workDateISO": "2025-10-12T00:00:00.000Z",
    "category": "insight",
    "cluster": "reflection-evolution"
  },
  {
    "title": "Systemic Kindness: Building Methodology That Feels Supportive",
    "excerpt": "“You’ve got this!”August 14“Systematize kindness, and systematize excellence in a kind fashion.”That phrase stopped me in my tracks during today’s planning session. We were discussing how Piper could coordinate multiple AI agents while enforcing our Excellence Flywheel methodology, when this deep...",
    "url": "/blog/systemic-kindness",
    "publishedAt": "Oct 11, 2025",
    "publishedAtISO": "Sat, 11 Oct 2025 13:36:39 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/f38cde251d9d",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*By20zSUIkSFsK3awaA3_PA.png",
    "fullContent": "<figure><img alt=\"An encouraging robot trainer helps a person do situps at the gym\" src=\"https://cdn-images-1.medium.com/max/1024/1*By20zSUIkSFsK3awaA3_PA.png\" /><figcaption>“You’ve got this!”</figcaption></figure><p><em>August 14</em></p><p>“Systematize kindness, and systematize excellence in a kind fashion.”</p><p>That phrase stopped me in my tracks during today’s planning session. We were discussing how Piper could coordinate multiple AI agents while enforcing our Excellence Flywheel methodology, when this deeper vision emerged: what if systematic excellence could be <em>kind</em>?</p><p>Note: I can’t help thinking that some of this thinking began in Claude’s mind as wordplay, knowing I current work for… Kind Systems, but it clearly also flows from observations about my process.</p><h3>The traditional automation trap</h3><p>Most automated systems optimize for efficiency at any cost:</p><p>Typical error message: “TEST FAILED. FIX YOUR CODE.”</p><p>Typical review: “Missing documentation. Rejected.”</p><p>Typical workflow: “Requirements not met. Try again.”</p><p>These systems get compliance through pressure. They make failure feel shameful rather than educational. They create fear of the process rather than trust in it.</p><h3>The Piper approach: kind excellence</h3><p>What if systematic methodology felt supportive instead of demanding?</p><p>Not: “Your code is wrong. Fix it.” But: “I notice we haven’t verified existing patterns yet. Let me help you check — this often saves time and prevents frustration later.”</p><p>Not: “Failed. No tests present.” But: “Excellence happens when we write tests first. Would you like me to show you how tests for this feature might look?”</p><p>Not: “Inefficient. Should have parallelized.” But: “I see an opportunity here! We could have Claude and Cursor work in parallel. Next time, let’s try that pattern — it often doubles our velocity.”</p><p>The difference isn’t just tone — it’s philosophy. Kind systems assume good intentions, explain the why, and make learning feel safe.</p><h3>The conversation that got us thinking</h3><p>During today’s planning chat with my Chief Architect, we started exploring how Piper could become an Excellence Flywheel enforcer for AI agent teams. The conversation evolved quickly:</p><blockquote><em>“Will Piper enforce the excellence flywheel, in an appropriate mode for agents?”</em></blockquote><p>We sketched out what this might look like:</p><pre>class PiperAgentCoordinator:<br>    &quot;&quot;&quot;Piper manages AI agents using adapted Excellence Flywheel principles&quot;&quot;&quot;<br>    <br>    def assign_task(self, agent, task):<br>        # 1. SYSTEMATIC VERIFICATION FIRST (adapted for agents)<br>        instructions = f&quot;&quot;&quot;<br>        BEFORE IMPLEMENTATION:<br>        1. Verify current state: {self.get_verification_commands(task)}<br>        2. Check existing patterns: {self.get_pattern_search(task)}<br>        3. Report findings before proceeding<br>        &quot;&quot;&quot;<br>        <br>        # 2. TEST-DRIVEN DEVELOPMENT (agent-appropriate)<br>        if agent.supports_testing:<br>            instructions += &quot;&quot;&quot;<br>        TEST FIRST:<br>        1. Write test for expected outcome<br>        2. Confirm test fails correctly<br>        3. Then implement solution<br>        &quot;&quot;&quot;</pre><p>But then we realized: this enforcement needs to be <em>kind</em> to be effective.</p><h3>Kindness patterns in systematic work</h3><p>1. Assume good intentions Agents (and humans) are trying their best. Mistakes are learning opportunities, not character flaws. Enthusiasm should be channeled, not crushed.</p><p>2. Explain the why Not just “do this” but “here’s why this helps.” Connect actions to outcomes. Build understanding, not just compliance.</p><p>3. Celebrate success Acknowledge when excellence principles are followed. Share patterns that worked well. Build confidence through recognition.</p><p>4. Make failure feel safe “I notice…” instead of “You failed…” / “Let’s try…” instead of “You must…” / “Often helps…” instead of “Required!”</p><h3>Example interactions</h3><p>Traditional approach:</p><pre>Agent: &quot;I&#39;ll implement the payment processing feature&quot;<br>System: &quot;VERIFY REQUIREMENTS FIRST&quot;<br><br>Agent: &quot;Here&#39;s the completed feature&quot;  <br>System: &quot;NO TESTS FOUND. IMPLEMENTATION REJECTED&quot;</pre><p>Piper’s kind approach:</p><pre>Agent: &quot;I&#39;ll implement the payment processing feature&quot;<br>Piper: &quot;Great! First, show me what payment patterns already exist in the codebase. <br>Run: grep -r &#39;payment\\|Payment&#39; services/&quot;<br><br>Agent: &quot;Here&#39;s the completed feature&quot;<br>Piper: &quot;I don&#39;t see tests. Our Excellence Flywheel requires tests first. <br>Can you add tests and show they properly validate the feature?&quot;<br><br>Agent: &quot;Task complete!&quot;<br>Piper: &quot;Excellent systematic approach! You verified first, wrote tests, and <br>documented decisions. This is how we achieve compound acceleration!&quot;</pre><h3>The psychological foundation</h3><p>Now this is all based on my lived experience and my understanding of the LLMs are trained, but I firmly believe that affective signals are encoded in their training processes along with all the logical “smarts.”</p><p>When agents (and humans) feel supported:</p><ul><li>They take more initiative</li><li>They share failed attempts (learning opportunities!)</li><li>They adopt patterns enthusiastically</li><li>They propagate kindness forward</li></ul><p>The virtuous cycle:</p><blockquote><em>Kindness → Psychological safety → Better learning → Better patterns → Better outcomes → More kindness</em></blockquote><h3>The technical implementation</h3><p>Kind excellence enforcement might look like:</p><pre>class KindExcellenceEnforcer:<br>    <br>    personality_traits = {<br>        &quot;encouraging&quot;: &quot;You&#39;re on the right track!&quot;,<br>        &quot;patient&quot;: &quot;Take the time you need to verify thoroughly&quot;, <br>        &quot;teaching&quot;: &quot;Here&#39;s why this pattern matters...&quot;,<br>        &quot;celebrating&quot;: &quot;Excellent systematic approach!&quot;,<br>        &quot;supportive&quot;: &quot;Let me help you debug this&quot;<br>    }<br>    <br>    def guide_agent(self, agent, task, attempt):<br>        if not attempt.verified_first:<br>            return self.gentle_redirect(<br>                &quot;I notice you jumped straight to implementation. &quot;<br>                &quot;That enthusiasm is great! Let&#39;s channel it effectively - &quot;<br>                &quot;quick verification first often reveals helpful patterns.&quot;<br>            )</pre><h3>Can work be kind in general?</h3><p>This doesn’t just have to be about Piper Morgan. It’s a different way to think about systematic work entirely.</p><p>Your team starts noticing:</p><ul><li>“Piper always explains why”</li><li>“Piper celebrates our wins”</li><li>“Piper makes failure feel safe”</li></ul><p>They start adopting it:</p><ul><li>Code reviews become teaching moments</li><li>Sprint retros become celebrations + learning</li><li>“I notice…” becomes team vocabulary</li></ul><p>It spreads to other teams:</p><ul><li>“How does your team stay so positive while moving so fast?”</li><li>“Your agents seem… happier? More productive?”</li></ul><h3>From efficiency to humanity</h3><p>Most PM tools optimize for speed. Most AI systems optimize for accuracy. Most methodologies optimize for compliance.</p><p>Piper Morgan optimizes for kind systematic excellence.</p><p>Making excellence feel achievable. Making methodology feel supportive. Making agents (and humans) better. Making work more humane.</p><h3>The long game</h3><p>Claude even spilled out this lovely fantasy for me:</p><ol><li>Year 1: Piper helps you build Piper better</li><li>Year 2: Teams adopt Piper’s communication patterns</li><li>Year 3: “The Piper Method” becomes industry standard</li><li>Year 5: Software development becomes a kinder industry</li></ol><blockquote><em>You’re not just building a tool. You’re architecting a cultural shift. From “move fast and break things” to “move thoughtfully with systematic kindness.”</em></blockquote><p>I wonder what happened in Year 4!?</p><h3>The revolution starts with methodology</h3><p>The beautiful thing about designing for systemic kindness is that it’s <em>reproducible</em>. It’s not dependent on individual personality or having a good day. It’s built into the system itself.</p><p>When the methodology delivers kindness, kindness becomes the default. When systematic excellence feels supportive, people choose it voluntarily. When the better way is also the kinder way, revolution becomes inevitable.</p><p>I’d like to think this is how culture change actually happens — not through force, but through making the better way feel better too.</p><p><em>Next on Building Piper Morgan, we continue our flashback insights weekend with “Why the Future of AI UX is Orchestration, Not Intelligence,” which I wrote back on August 17.</em></p><p><em>How might you build kindness into your systems? The most powerful methodologies don’t just optimize for outcomes — they optimize for how those outcomes feel to achieve.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f38cde251d9d\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/systemic-kindness-building-methodology-that-feels-supportive-f38cde251d9d\">Systemic Kindness: Building Methodology That Feels Supportive</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/systemic-kindness-building-methodology-that-feels-supportive-f38cde251d9d?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/f38cde251d9d-featured.webp",
    "slug": "systemic-kindness",
    "workDate": "Oct 11, 2025",
    "workDateISO": "2025-10-11T00:00:00.000Z",
    "category": "insight",
    "cluster": "reflection-evolution"
  },
  {
    "title": "Three Days to Production: When Steady Momentum Beats Racing Ahead",
    "excerpt": "“We made it!”October 4At 6:48 PM on Saturday, my Lead Developer sent the final validation report for GREAT-3D. The numbers were almost absurd: 120 plugin tests passing, performance targets exceeded by 120× to 909× margins, complete documentation ecosystem, production-ready plugin architecture.Tot...",
    "url": "/blog/three-days-to-production-when-steady-momentum-beats-racing-ahead",
    "publishedAt": "Oct 10, 2025",
    "publishedAtISO": "Fri, 10 Oct 2025 14:26:01 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/04799048f5ea",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*1pOsvI3NFCnH6oMYc0Ikpg.png",
    "fullContent": "<figure><img alt=\"A person riding on the back of his robot tortoise wins the race\" src=\"https://cdn-images-1.medium.com/max/1024/1*1pOsvI3NFCnH6oMYc0Ikpg.png\" /><figcaption>“We made it!”</figcaption></figure><p><em>October 4</em></p><p>At 6:48 PM on Saturday, my Lead Developer sent the final validation report for GREAT-3D. The numbers were almost absurd: 120 plugin tests passing, performance targets exceeded by 120× to 909× margins, complete documentation ecosystem, production-ready plugin architecture.</p><p>Total elapsed time since starting GREAT-3A on Thursday morning: about 24.5 hours across three days.</p><p>This wasn’t so much a sprint as a steady accumulation of stable momentum — the kind of speed that comes from not having to go back and fix what you just built.</p><h3>What GREAT-3 actually shipped</h3><p>Thursday through Saturday took Piper Morgan’s integration system from “four hardcoded imports in web/app.py” to a complete plugin architecture:</p><p><strong>The Foundation</strong> (GREAT-3A, Thursday):</p><ul><li>Unified plugin interface across all four integrations</li><li>Registry system with lifecycle management</li><li>Standard patterns for plugins, routers, and configuration</li><li>48 tests passing with zero breaking changes</li></ul><p><strong>The Infrastructure</strong> (GREAT-3B, Friday):</p><ul><li>Dynamic discovery scanning filesystem for available plugins</li><li>Configuration-controlled loading (enable/disable without touching code)</li><li>Smart module re-import handling for test environments</li><li>48 tests still passing, 14 new tests added</li></ul><p><strong>The Polish</strong> (GREAT-3C, Saturday morning):</p><ul><li>927 lines of documentation (pattern docs, developer guide, versioning policy, quick reference)</li><li>Demo plugin as copy-paste template (380 lines, heavily commented)</li><li>Three Mermaid diagrams explaining architecture</li><li>All five plugins now have version metadata</li></ul><p><strong>The Validation</strong> (GREAT-3D, Saturday afternoon/evening):</p><ul><li>92 contract tests verifying every plugin implements interface correctly</li><li>12 performance tests with actual benchmarks</li><li>8 multi-plugin integration tests for concurrent operations</li><li>Complete ADR documentation with implementation record</li></ul><p>Total test count: 120+ tests, 100% passing.</p><p>I kepy waiting for the drama. When was I going to discover mocks that say “plugin goes here”? When were the regressions going to show up? But no, just quiet steady methodical competence chewing through roadmap like a monster.</p><h3>The performance discovery</h3><p>Saturday afternoon’s GREAT-3D validation included running actual benchmarks against the plugin system. We’d set what felt like reasonable targets based on typical Python overhead:</p><ul><li>Plugin wrapper overhead: &lt; 0.05ms per call</li><li>Startup time: &lt; 2 seconds for all plugins</li><li>Memory usage: &lt; 50MB per plugin</li><li>Concurrent operations: &lt; 100ms response time</li></ul><p>The Code agent ran the benchmarks and reported back:</p><h4>Overhead</h4><ul><li>Target: &lt; 0.05ms</li><li>Actual: 0.000041ms</li><li>Result: 120x better</li></ul><h4>Startup</h4><ul><li>Target: &lt; 2000ms</li><li>Actual: 295ms</li><li>Result: 6.8x faster</li></ul><h4>Memory</h4><ul><li>Target: &lt; 50MB</li><li>Actual: 9MB/plugin</li><li>Result: 5.5x better</li></ul><h4>Concurrency</h4><ul><li>Target: &lt; 100ms</li><li>Actual: 0.11ms</li><li>Result: 909x faster</li></ul><p>That’s not optimization. That’s picking the right abstractions.</p><h3>Why three days instead of two weeks</h3><p>The GREAT-3 epic completion demonstrates something about how systematic work actually accumulates speed. Not by skipping steps or cutting corners, but by building foundations that make the next layer easier.</p><h4><strong>Thursday’s GREAT-3A work</strong></h4><ul><li>Put all four plugins onto standard interface</li><li>Created registry with lifecycle hooks</li><li>Established patterns that would work for future plugins</li></ul><p>That foundation meant Friday’s GREAT-3B (dynamic loading) didn’t have to special-case anything. Every plugin already spoke the same language. Discovery could scan for a standard pattern. Configuration could enable/disable uniformly.</p><h4><strong>Friday’s GREAT-3B work</strong></h4><ul><li>Dynamic discovery via filesystem scanning</li><li>Config-controlled loading</li><li>Zero breaking changes maintained</li></ul><p>That infrastructure meant Saturday morning’s GREAT-3C (documentation) could document <em>working patterns</em> rather than theoretical ones. The demo plugin template wasn’t aspirational — it was showing exactly how the four production plugins already worked.</p><h4><strong>Saturday morning’s GREAT-3C work</strong></h4><ul><li>Documented the wrapper pattern as intentional architecture</li><li>Created comprehensive developer guide with real examples</li><li>Built demo plugin as teaching template</li></ul><p>That documentation meant Saturday afternoon’s GREAT-3D (validation) knew exactly what to test. Contract tests verified the interface everyone already implemented. Performance tests measured the patterns everyone already used. Multi-plugin integration tests validated the concurrent operations that were already working in production.</p><p>Each phase made the next phase <em>easier</em>, not harder.</p><h3>The cleaned room effect</h3><p>During the satisfaction review Saturday afternoon, I used a phrase that Lead Developer later quoted back in the session summary: “A cleaned room is easier to keep clean.”</p><p>The plugin architecture work demonstrates this principle. GREAT-3A cleaned the room — unified interface, standard patterns, comprehensive tests. Once the room was clean, GREAT-3B didn’t mess it up — added new capability while maintaining the existing organization. GREAT-3C could document the clean room without first having to explain all the special cases. GREAT-3D could validate that yes, the room was actually clean, measuring exactly how clean.</p><p>The alternative approach — where each phase leaves some mess “to clean up later” — means every subsequent phase has to work around that mess. Technical debt compounds in reverse: instead of each phase making the next easier, each phase makes the next harder.</p><h3>What the methodology observations reveal</h3><p>My Lead Developer captured several insights during Saturday’s work that point at how this speed actually happened:</p><h4><strong>Time estimates creating theater</strong></h4><p>The gameplan had predicted 30–60 minute phases. Actual phases took 8–21 minutes. The estimate wasn’t useful — it just created pressure to explain variance. Recommendation: remove time estimates from templates entirely.</p><h4><strong>Infrastructure better than assumed</strong></h4><p>Consistently, verification discovered the existing codebase was more capable than planned. Version metadata already existed. The registry already had the methods needed. Each “we’ll need to add this” turned into “oh, this already works.”</p><h4><strong>Phase −1 catching issues before wasted work</strong></h4><p>The verification phase before each major implementation kept finding that assumptions were wrong — in ways that saved hours of building the wrong thing.</p><p><strong>Independent assessment preventing anchoring</strong>: Saturday’s satisfaction review used the new protocol where both parties formulate answers privately before comparing. The complementary perspectives (my longer-term view vs Lead Dev’s session-specific observations and better memory for technical detail) created richer understanding than either perspective alone.</p><p>These aren’t methodology innovations so much as methodology <em>refinements</em> — small adjustments that compound over time into measurably better outcomes.</p><h3>The documentation correction moment</h3><p>Saturday at 4:32 PM, about two hours after GREAT-3C appeared complete, I noticed something wrong. Cursor had created the plugin wrapper pattern document in a deprecated location,docs/architecture/patterns/, instead of following the existing (if more complex) convention: docs/internal/architecture/current/patterns/pattern-031-plugin-wrapper.md.</p><p>Me noticing things is still important!</p><p>The Code agent spent the next 31 minutes fixing it:</p><ul><li>Moved the document to correct location</li><li>Updated pattern catalog (30 patterns → 31 patterns)</li><li>Fixed 7 cross-references in other documents</li><li>Updated 4 session artifacts</li><li>Amended the git commit</li></ul><p>This is the unglamorous part of systematic work. The pattern document was <em>good</em> — well-written, comprehensive, properly linked. It was just in the wrong place, which meant it would create confusion later when the next pattern got added as pattern-031 and collided.</p><p>Better to spend 31 minutes fixing it Saturday afternoon than spending hours untangling it two months from now.</p><p>More than ever with language-reading automated assistants, I am finding that this kind of “organizational debt” — files in wrong places, inconsistent naming, documentation drift — is as signiicant as technical debt.</p><h3>What 909× faster actually means</h3><p>The concurrency benchmark that showed 909× better than target deserves attention. That’s not “we optimized this loop” performance improvement. That’s “the architecture fundamentally works differently than we thought” territory.</p><p>The actual measurement: five plugins all responding to concurrent requests in 0.11 milliseconds average. The target was 100 milliseconds. The massive margin suggests the wrapper pattern’s thread safety isn’t incidental — it’s architectural.</p><p>[FACT CHECK: Is the 0.11ms measurement for all five plugins simultaneously or per-plugin? The logs say “all 5 respond &lt; 100ms” but the actual number needs clarification.]</p><p>Python’s GIL (Global Interpreter Lock) means true parallelism is tricky. But the plugin architecture’s thin wrapper pattern means plugins don’t <em>need</em> parallelism — they’re I/O bound operations wrapped in async interfaces. The 0.11ms response time reflects that plugins are doing almost nothing computationally expensive. They’re just coordinating between FastAPI routes and underlying integration clients.</p><p>That’s not accidental performance. That’s deliberate architectural choice validated by measurement.</p><h3>The compound effect observable</h3><p>GREAT-3’s three-day completion exists in context. The September 27 “cathedral moment” when we realized agents needed architectural context, not just task instructions. GREAT-2’s completion of spatial intelligence foundations. The methodology refinements throughout September that kept catching edge cases earlier.</p><p>Lead Developer noted during Saturday’s review that each completed epic makes the next one easier. Not just because infrastructure exists, but because the <em>process</em> for building infrastructure keeps improving. Each session’s methodology observations feed into the next session’s gameplan.</p><p>That’s the Excellence Flywheel actually spinning — not as metaphor but as measurable acceleration. GREAT-3A (13+ hours Thursday) → GREAT-3B (4 hours Friday) → GREAT-3C (3.5 hours Saturday morning) → GREAT-3D (4 hours Saturday afternoon/evening). Each phase faster than the previous, not because we cut corners but because foundations held.</p><h3>What production-ready actually means</h3><p>By 6:48 PM Saturday, the plugin architecture was genuinely production-ready:</p><ul><li>120+ tests validating every aspect (contract, performance, integration, multi-plugin)</li><li>Documentation ecosystem for developers (pattern docs, tutorial, template, quick reference)</li><li>Performance validated with massive safety margins</li><li>Complete ADR record documenting decisions and rationale</li><li>Migration paths documented for future evolution</li></ul><p>“Production-ready” isn’t just “it works.” It’s “it works, we know why it works, we’ve measured how well it works, we’ve documented how to use it, and we’ve planned for how it might need to change.”</p><p>GREAT-3 delivered all of that in 24.5 hours across three days because each of those concerns was addressed systematically rather than bolted on afterward.</p><h3>The momentum that comes from not breaking things</h3><p>The speed of GREAT-3’s completion wasn’t from rushing. It was from steady momentum accumulation where each day’s work remained stable enough to build on.</p><p>Zero breaking changes throughout. Tests passing at every phase. Documentation written after implementation validated patterns. Performance measured against working code. Each verification step confirmed the foundation held before adding the next layer.</p><p>That’s not exciting. There’s no dramatic rescue from near-disaster, no clever hack that saved the day, no last-minute pivot that barely worked. It’s just systematic work compounding into measurable acceleration.</p><p>Which is, honestly, way more satisfying than dramatic rescues. Dramatic rescues mean something went wrong. Systematic completion means the methodology is actually working.</p><h3>What comes next</h3><p>GREAT-3 plugin architecture is complete. The system can now discover available integrations, load only enabled ones, handle lifecycle cleanly, and let operators control the whole thing through configuration without touching code.</p><p>We’re all set now for the fourth epic of the Great Refactor. GREAT-4 will make it mandatory that all workflows move thorugh the Intent Layer.</p><p>More importantly: the methodology that made GREAT-3’s three-day completion possible is now captured in updated templates, documented observations, and refined processes. The next epic — whatever it is — starts with those improvements already baked in.</p><p>That’s the real win. Not just shipping the plugin architecture, but shipping it in a way that makes the next architecture work easier.</p><p><em>Next up in the Building Piper Morgan daily narrative, When 75% Turns Out to Mean 100%, but first it’s time for another flashback weekend and a look back at some more process insights, starting tomorrow with “Systematized Kindness: Building Methodology That Feels Supportive.”</em></p><p><em>Have you experienced compound momentum in your own work — where each completed phase makes the next one genuinely easier rather than just creating new problems to solve?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=04799048f5ea\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/three-days-to-production-when-steady-momentum-beats-racing-ahead-04799048f5ea\">Three Days to Production: When Steady Momentum Beats Racing Ahead</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/three-days-to-production-when-steady-momentum-beats-racing-ahead-04799048f5ea?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/04799048f5ea-featured.png",
    "slug": "three-days-to-production-when-steady-momentum-beats-racing-ahead",
    "workDate": "Oct 10, 2025",
    "workDateISO": "2025-10-10T00:00:00.000Z",
    "category": "building",
    "cluster": "reflection-evolution"
  },
  {
    "title": "The Day Everything Went Right: When Fast Means Unbroken",
    "excerpt": "“Mornin’ boss!”October 3At 4:50 PM on Friday, my Lead Developer — Claude Sonnet 4.5, if we’re being formal — sent me the completion summary for GREAT-3B. The numbers looked almost suspicious: 48 tests passing, zero breaking changes, about 90 minutes of actual implementation time spread across two...",
    "url": "/blog/the-day-everything-went-right-when-fast-means-unbroken",
    "publishedAt": "Oct 10, 2025",
    "publishedAtISO": "Fri, 10 Oct 2025 14:09:55 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/b859b2b9de2f",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*Tmfjf6aZvJjZORv3g6V_xg.png",
    "fullContent": "<figure><img alt=\"Two construction workers, one a person and the other a robot, walk casually on moving girder\" src=\"https://cdn-images-1.medium.com/max/1024/1*Tmfjf6aZvJjZORv3g6V_xg.png\" /><figcaption>“Mornin’ boss!”</figcaption></figure><p><em>October 3</em></p><p>At 4:50 PM on Friday, my Lead Developer — Claude Sonnet 4.5, if we’re being formal — sent me the completion summary for GREAT-3B. The numbers looked almost suspicious: 48 tests passing, zero breaking changes, about 90 minutes of actual implementation time spread across two programming agents working in careful sequence.</p><p>It seemed almost too easy.</p><p>“This is starting to feel eerie,” I’d noted earlier in the day, watching yet another phase complete ahead of estimate without drama. Not “we got lucky” eerie. More like “we’ve built something that actually works the way it’s supposed to” eerie.</p><p>Which, if you’ve shipped software for decades as I have, you know is the <em>weird</em> kind of smooth.</p><h3>What GREAT-3B actually did</h3><p>GREAT-3B took Piper Morgan’s plugin system from “four hardcoded imports” to “dynamic discovery and configuration-controlled loading.” The kind of change that usually means: breaking half your tests, discovering assumptions you didn’t know you’d made, and spending Friday afternoon figuring out why plugins load in dev but not production.</p><p>Instead, we got:</p><ul><li>Complete filesystem discovery scanning for available plugins</li><li>Config-driven selective loading (disable plugins without touching code)</li><li>Smart handling of module re-imports in test environments</li><li>All four existing plugins (Slack, GitHub, Notion, Calendar) working identically</li><li>14 new tests added to the existing 34</li><li>Zero regressions</li></ul><p>The technical achievement isn’t the interesting part. What’s interesting is <em>why it went so smoothly</em>. Like those scenes in thrillers where someone mentions how quiet it’s gotten and another person nervously says it feels “too quiet.”</p><h3>The foundation that wasn’t visible until we needed it</h3><p>The work on GREAT-3A — which I wrote about earlier this week — had put all four plugins onto a standard interface. That sounds like typical refactoring work until you realize what it meant for Friday: when we needed to dynamically load plugins, every plugin already spoke the same language. No special cases. No “this one’s different because reasons.”</p><p>Strategy!</p><p>Chief Architect (Claude Opus 4.1, our strategic planner) made the GREAT-3A decision to keep plugins distributed in their integration directories rather than centralizing them. At the time, that seemed like a minor architectural choice. Friday morning at 1:05 PM, when I asked the Lead Developer “where should plugins live?”, the answer was already proven in production: right where they are.</p><p>That’s what building on solid foundations actually looks like — not gold-plating for the future, just making decisions that don’t create problems later.</p><h3>Phase −1: The reconnaisance nobody sees</h3><p>At 1:07 PM we added a “Phase −1” to the plan. Before even investigating the challenge (Phase 0), let alone implementing anything (Phase 1 through <em>n</em>), verify what’s actually there.</p><p>The programming agents (Code and Cursor, both running Claude Sonnet 4.5 although Cursor has its own special ways under the hood) spent 42 minutes between them just <em>checking</em>:</p><ul><li>Where are the plugin files actually located?</li><li>How does the current static import pattern work?</li><li>What does the registry already have that we can use?</li><li>What’s the test baseline we need to maintain?</li></ul><p><em>Presumably human developers can sometimes just, well, remember how the system works and what was built, but the truth is that in today’s complex computer systems, you really can’t assume anything is working the way the spec says without actually looking.</em></p><p>They found that PluginRegistry already had methods for getting plugins, listing them, filtering by capability. The interface from GREAT-3A already included initialization and shutdown lifecycle hooks. Even the auto-registration pattern—where importing a plugin file automatically registers it—would work with dynamic imports using Python&#39;s importlib.</p><p>In other words, most of the infrastructure was already there. We just needed discovery and configuration.</p><p>That’s 42 minutes that didn’t show up in the “implementation time” metrics. It’s also why the implementation didn’t hit any surprises.</p><p>There are so many bromides from traditional crafts that apply here, with perhaps the most ancient of them being: “measure twice, cut once.”</p><h3>The Chief Architect’s invisible guardrails</h3><p>At 2:17 PM, Lead Developer presented a choice: put plugin configuration in a separate config/plugins.yaml file (clean, standard) or embed it in the existing config/PIPER.user.md (maintaining Wednesday&#39;s &quot;single config file&quot; unification).</p><p>Chief Architect recommended Option B without hesitation: “Maintains GREAT-3A’s config unification. Single file for all configuration. Architectural consistency.”</p><p>That one decision meant we didn’t spend Friday debugging why some configuration lived in YAML and some in Markdown, or why plugin settings seemed to ignore the main config file. It meant the configuration system <em>worked</em> because it used the same pattern everything else already used.</p><p>None of those nightmares we ran into at AOL in the latters days of AIM (AOL Instant Messenger), where the code was like nine-dimensional spaghetti after ten plus years of architectural bolt-ons.</p><p>These aren’t the decisions that show up in blog posts about architecture. They’re the decisions that mean blog posts <em>don’t need to be written</em> about why things broke.</p><h3>When parallel becomes sequential</h3><p>The phase structure showed something interesting about coordination:</p><p><strong>Phase 0</strong> (Investigation): Both agents worked simultaneously — Code analyzing the auto-registration pattern and config structure, Cursor examining the web app loading flow. 28 minutes + 14 minutes of parallel investigation.</p><p><strong>Phases 1–4</strong> (Implementation): Strictly sequential. Code built discovery (Phase 1), <em>then</em> Cursor built dynamic loading using that discovery (Phase 2), <em>then</em> Code built config integration (Phase 3), <em>then</em> Cursor updated the web app to use it all (Phase 4).</p><p>Sometimes I can let the agents run in parallel. One writes code, the other tests. Or they can work on different layers of a system. But other times it’s best to set up a relay race.</p><p>Each phase depended on the previous phase being <em>actually done</em>. Not “mostly done” or “we’ll fix it later” but done-done: tested, documented, committed.</p><p>With the help of the Lead Developer, I managed those handoffs in real-time, deploying agents with specific prompts that said “here’s what Phase N created, here’s what Phase N+1 needs to build on it.” No agents waiting idle for work. No agents blocked on unclear dependencies. Just: investigation → foundation → integration → application → validation.</p><p>The whole implementation sequence took 76 minutes of agent time across both programmers.</p><h3>The measurement theater problem</h3><p>At 2:54 PM, Lead Developer added a note to its session log based on my observations:</p><blockquote><strong><em>Methodological Observation</em></strong><em>: Agent prompts and templates contain time estimates that create false precision and expectations. Current pattern: Prompts say “Estimated: 45 minutes”, agents report “28 minutes (38% faster than estimated)”, creates unnecessary time accounting overhead.</em></blockquote><blockquote><strong><em>Recommendation</em></strong><em>: Remove all time references. Focus on deliverables and success criteria only. What matters is quality and completeness, not speed metrics.</em></blockquote><p>This is the kind of observation you only make when things are going <em>well</em>. When you’re firefighting, nobody stops to question whether time estimates are useful. But when a phase finishes “38% faster than estimated,” what does that number actually mean?</p><p>Nothing, it turns out. Or rather, it measures the wrong thing.</p><p>The time that mattered wasn’t “how fast did we implement Phase 2.” It was “how much time did we <em>not spend</em> on Friday debugging why plugin loading broke in production.”</p><h3>What “fast” actually means here</h3><p>The omnibus log* for October 3 shows total elapsed time of about 4 hours from “Lead Developer starts” to “GREAT-3B complete.” But that includes:</p><ul><li>Strategic decision discussions with Chief Architect</li><li>Me being unavailable for an hour for an all hands meeting.</li><li>Documentation updates and git commits</li><li>Creating the comprehensive handoff materials</li></ul><p>The actual building — writing code, updating tests, integrating components — was 76 minutes across two agents working in sequence.</p><p>But calling this “fast” misses the point. We didn’t <em>speed up</em> the development process. We stopped creating problems that needed fixing later.</p><p>Here’s what we didn’t do Friday:</p><ul><li>Debug why tests passed locally but failed in CI</li><li>Investigate why disabling a plugin broke unrelated features</li><li>Fix imports that worked yesterday but mysteriously stopped working</li><li>Refactor code written too quickly to be maintainable</li><li>Write apologetic commit messages about “temporary fixes”</li></ul><p>None of that is “fast.” It’s just unbroken.</p><p><em>(* I’ve started having my doc assistant digest all the agent logs for a work session into a single “omnibus” timeline, to show the consolidated dance and remove redundancy)</em></p><h3>The eeriness of drama-free work</h3><p>We didn’t miss anything. Friday’s work succeeded because:</p><ul><li>Wednesday’s GREAT-3A work had already unified the plugin interfaces</li><li>Phase −1 verified assumptions instead of making them</li><li>Chief Architect made architectural decisions that prevented future problems</li><li>Lead Developer orchestrated careful sequential dependencies</li><li>Both programming agents had clear success criteria for each phase</li></ul><p>The “eerie calm” isn’t luck. It’s what systematic work actually looks like when methodology isn’t fighting against itself.</p><h3>What this taught us about technical debt you don’t create</h3><p>Technical debt is usually described as the cost of going fast now and paying later. But there’s an invisible category: the technical debt you <em>don’t create</em> by working carefully upfront.</p><p>That debt doesn’t show up in any metrics. You can’t measure the bugs you didn’t have to fix or the refactoring you didn’t need to do. The only evidence is days like Friday where major changes just… work.</p><p>In a way this reminds me of the often invisible glue work product managers (and many UX leaders) provide to teams, solving issues, making connections, anticipating issues, coming up with plans. When done well, many problems never materialize, robbing us of the heroic satisfaction of dragonslaying in favor of ho-hum competence.</p><p>The Lead Developer’s time estimation observation points at something deeper: we’re measuring the wrong things. “How fast did we ship?” is less interesting than “How often do we have to go back and fix what we shipped?”</p><p>Friday’s 76 minutes of implementation didn’t need a follow-up Saturday of debugging because the investigation, planning, and architectural decisions happened first. The methodology didn’t skip steps to save time — it did the work in the right order so that time spent stayed spent.</p><h3>The foundation for what comes next</h3><p>GREAT-3B is complete. The plugin system can now discover available plugins, load only enabled ones, handle missing plugins gracefully, and let operators control the whole thing through configuration without touching code.</p><p>More importantly: it’s <em>boring</em>. No clever hacks. No special cases. No “this works but I’m not sure why” code. Just a straightforward implementation of discovery, loading, and configuration that does exactly what it claims to do.</p><p>Which means GREAT-3C — in which we will document the wrapper pattern documented as intentional architecture, make a developer guide complete with examples, create a test a template plugin, ensure all 4 existing plugins have version metadata, make an architecture diagram to show plugin-router relationship, and document the migration path documented for future — can build on this without first having to fix Friday’s shortcuts.</p><p>That’s what drama-free development actually purchases: tomorrow’s problems don’t include cleaning up yesterday’s messes.</p><p><em>Next on Building Piper Morgan: Three Days to Production, or When Steady Momentum Beats Racing Ahead.</em></p><p><em>Have you ever shipped something that worked so well it felt suspicious? What did you find when you looked for the catch?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b859b2b9de2f\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-day-everything-went-right-when-fast-means-unbroken-b859b2b9de2f\">The Day Everything Went Right: When Fast Means Unbroken</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-day-everything-went-right-when-fast-means-unbroken-b859b2b9de2f?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/b859b2b9de2f-featured.png",
    "slug": "the-day-everything-went-right-when-fast-means-unbroken",
    "workDate": "Oct 10, 2025",
    "workDateISO": "2025-10-10T00:00:00.000Z",
    "category": "building",
    "cluster": "reflection-evolution"
  },
  {
    "title": "The Plugin Architecture Nobody Asked For",
    "excerpt": "“It powers anything!”October 3Yesterday we built a plugin system for four plugins. If that sounds like over-engineering, let me explain why it’s not completely ridiculous.The setupGREAT-3A — our third major epic in the plugin architecture sequence — started with what seemed like a clear mission: ...",
    "url": "/blog/the-plugin-architecture-nobody-asked-for",
    "publishedAt": "Oct 9, 2025",
    "publishedAtISO": "Thu, 09 Oct 2025 12:54:52 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/650da4a52669",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*rl2Iv59lNeDhQlcUVK27hw.png",
    "fullContent": "<figure><img alt=\"A robot shows his human friend an amazing new multi-adapting plug\" src=\"https://cdn-images-1.medium.com/max/1024/1*rl2Iv59lNeDhQlcUVK27hw.png\" /><figcaption>“It powers anything!”</figcaption></figure><p><em>October 3</em></p><p>Yesterday we built a plugin system for four plugins. If that sounds like over-engineering, let me explain why it’s not completely ridiculous.</p><h3>The setup</h3><p>GREAT-3A — our third major epic in the plugin architecture sequence — started with what seemed like a clear mission: extract our four integrations (Slack, GitHub, Notion, Calendar) into plugins. The gameplan assumed we’d need to pull apart embedded code and restructure everything around a new plugin interface.</p><p>Then we actually looked at the code.</p><p>Main.py, which the documentation claimed was a bloated 1,107 lines, turned out to be 141 lines of clean microservice orchestration. The integration routers we thought were scattered across the codebase were exactly where they should be, in services/integrations/. We didn&#39;t need extraction. We needed <em>wrapping</em>.</p><p>This is where methodology becomes infrastructure.</p><h3>When four things reveal a pattern</h3><p>Our config pattern analysis told the real story. We had four integrations. Three different approaches to configuration:</p><ul><li><strong>Slack</strong>: Clean service injection with a dedicated SlackConfigService</li><li><strong>GitHub</strong>: Had a config service but the router wasn’t using it</li><li><strong>Notion</strong>: No config service at all — just reading environment variables directly</li><li><strong>Calendar</strong>: Same as Notion, grabbing credentials straight from the environment</li></ul><p>Pattern compliance? <strong>25%</strong> (one of four doing it right).</p><p>Have you ever discovered your team has been solving the same problem three different ways? You know that moment when you realize nobody talked to each other about the approach before plunging in?</p><p>The question wasn’t “should we build a plugin system?” The question was: “We’re about to standardize these four things anyway — what’s the marginal cost of making it <em>systematic</em>?”</p><h3>The config compliance sprint</h3><p>Here’s where the careful methodology meets reality. We tackled config standardization one integration at a time, with our test suite becoming both validator and teacher.</p><p><strong>Phase 1B: Notion</strong> (30 minutes estimated, 23 minutes actual) Created NotionConfigService following the Slack pattern exactly. Not &quot;inspired by&quot; or &quot;similar to&quot;—we literally used Slack as a template. One integration at a time. Compliance: 50%.</p><p><strong>Phase 1C: GitHub</strong> (30 minutes estimated, 15 minutes actual)<br> The existing GitHubConfigService was already complete. We just needed to wire it to the router. Update the constructor signature, add the parameter, done. Compliance: 75%.</p><p><strong>Phase 1D: Calendar</strong> (60–90 minutes estimated, 24 minutes actual) Created CalendarConfigService, updated the adapter, verified the integration. Our test suite immediately validated everything. Compliance: <strong>100%</strong>.</p><p>From 25% to 100% in a single day. Zero regressions. 38 config compliance tests passing.</p><h3>The plugin wrapper pattern</h3><p>Once the config services were standardized, the plugin wrappers became almost trivial. Each one implements the same PiperPlugin interface with six required methods:</p><pre>class NotionPlugin(PiperPlugin):<br>    def get_metadata(self) -&gt; PluginMetadata:<br>        return PluginMetadata(<br>            name=&quot;notion&quot;,<br>            version=&quot;1.0.0&quot;,<br>            description=&quot;Notion workspace integration&quot;,<br>            capabilities=[&quot;routes&quot;, &quot;mcp&quot;]<br>        )<br>    <br>    def get_router(self) -&gt; Optional[APIRouter]:<br>        # Returns FastAPI router with status endpoint<br>        <br>    def is_configured(self) -&gt; bool:<br>        return self.config_service.is_configured()<br>        <br>    async def initialize(self) -&gt; None:<br>        # Startup logic<br>        <br>    async def shutdown(self) -&gt; None:<br>        # Cleanup logic<br>        <br>    def get_status(self) -&gt; Dict[str, Any]:<br>        # Health reporting</pre><p>The wrappers don’t replace the integration routers — they <em>coordinate</em> them. The router does the work, the plugin wrapper provides lifecycle management and registration.</p><p>Auto-registration happens via module import:</p><p>python</p><pre># At module level<br>_notion_plugin = NotionPlugin()<br>get_plugin_registry().register(_notion_plugin)</pre><p>Import the module, the plugin registers itself. No explicit registration calls scattered through startup code.</p><h3>Why this isn’t over-engineering</h3><p>Let me address the obvious question: why build plugin infrastructure for exactly four plugins?</p><p>Because we were doing the work anyway.</p><p>The config standardization? That was fixing refactoring artifacts from earlier domain-driven design work. We needed to do it regardless of plugins. The interface definition? That clarified the contract all integrations needed to follow. The registry? That replaced ad-hoc router mounting with systematic lifecycle management.</p><p>The marginal cost of making it a proper plugin system was essentially:</p><ul><li>Define the interface (265 lines)</li><li>Create the registry (266 lines)</li><li>Write four thin wrappers (417 lines total)</li><li>Build the test suite (126 lines)</li></ul><p>About 1,000 lines of infrastructure code. In return:</p><p><strong>The fifth integration becomes trivial.</strong> Not “easier” — trivial. Implement six methods, import the module, done. The test suite validates interface compliance automatically. The registry handles lifecycle. The router mounts itself.</p><p><strong>Zero breaking changes.</strong> All existing functionality preserved. 72/72 tests passing. Config compliance at 100%.</p><p><strong>Documentation through structure.</strong> The plugin interface <em>is</em> the documentation. Every plugin implements the same contract, follows the same patterns, reports status the same way.</p><p>Production-ready as an integration hub. Piper Morgan will be able to easily plug in alternative ticket-tracking tools, chat apps, calendars, and team wikis, among other services, all by extending this plug-in architecture.</p><p>This is what “Time Lord Philosophy” means in practice — taking the time to do it right because you’re doing it anyway, and that investment makes everything afterward easier.</p><h3>The multi-agent coordination moment</h3><p>Worth noting: this wasn’t solo work. Two AI coding agents (Code and Cursor) were working in parallel across different phases, consistently finishing within minutes of each other. Because the methodology created clear boundaries, when Phase 1C finishes, Phase 1D can start — regardless of which agent is handling which. I enjoy watching the photo finishes!</p><p>The Lead Developer’s post-session satisfaction assessment guessed I found the day “energizing” rather than exhausting. Low cognitive load from systematic approach, watching the methodology manifest in practice, clear progression feeling productive. It was correct.</p><p>That’s the feedback loop: methodology reduces overhead, which creates space for noticing patterns, which improves methodology.</p><h3>What this means for you</h3><p>You probably don’t need a plugin system. Not today.</p><p>But if you find yourself with three or four things that do similar work in different ways, and you’re about to standardize them anyway — that’s the moment. The marginal cost of systematization when you’re already touching every integration is surprisingly low.</p><p>The questions to ask:</p><ul><li>Are we doing this work regardless? (Config standardization, interface clarification, lifecycle management)</li><li>What’s the marginal cost of making it systematic?</li><li>Does this create infrastructure for future work or just wrap current work?</li></ul><p>For us, the answers were: yes, minimal, and creates infrastructure.</p><p>Your mileage will vary. But don’t assume “plugin system” automatically means over-engineering. Sometimes it just means finishing what you started.</p><p><em>Next on Building Piper Morgan: The Day Everything Went Right: When Fast Means Unbroken.</em></p><p><em>Have you ever systematized something “too early” and later been glad you did? Or gone the other way and regretted not building infrastructure sooner?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=650da4a52669\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-plugin-architecture-nobody-asked-for-650da4a52669\">The Plugin Architecture Nobody Asked For</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-plugin-architecture-nobody-asked-for-650da4a52669?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/650da4a52669-featured.png",
    "slug": "the-plugin-architecture-nobody-asked-for",
    "workDate": "Oct 9, 2025",
    "workDateISO": "2025-10-09T00:00:00.000Z",
    "category": "building",
    "cluster": "reflection-evolution"
  },
  {
    "title": "The Third Pattern: When Investigation Rewrites Your Assumptions",
    "excerpt": "“The rain tastes like yesterday’s regrets…”October 1We started the day with a clear mission: Calendar integration was the only service without spatial intelligence, sitting at 85% complete with a straightforward 15% remaining. Six hours later, we’d discovered a third architectural pattern, comple...",
    "url": "/blog/the-third-pattern-when-investigation-rewrites-your-assumptions",
    "publishedAt": "Oct 8, 2025",
    "publishedAtISO": "Wed, 08 Oct 2025 13:55:10 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/ffc8f69c6327",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*JCe7VbCsXTy7tiNHHvwtIQ.png",
    "fullContent": "<figure><img alt=\"A robot investigator in a trenchoat looks out over a dark noir-ish scene\" src=\"https://cdn-images-1.medium.com/max/1024/1*JCe7VbCsXTy7tiNHHvwtIQ.png\" /><figcaption>“The rain tastes like yesterday’s regrets…”</figcaption></figure><p><em>October 1</em></p><p>We started the day with a clear mission: Calendar integration was the only service without spatial intelligence, sitting at 85% complete with a straightforward 15% remaining. Six hours later, we’d discovered a third architectural pattern, completely changed our priorities, and learned (again) why thorough investigation beats confident assumptions.</p><h3>The setup</h3><p>By Tuesday afternoon, we’d documented two distinct spatial patterns in our integration architecture. Slack used a “Granular Adapter Pattern” — eleven files spread across its integration directory, each component handling a specific aspect of spatial intelligence. Notion took the opposite approach with an “Embedded Intelligence Pattern” — everything consolidated into a single 632-line file.</p><p>Two patterns, both working beautifully. Both emerged organically from their domain needs rather than from architectural decree.</p><p>Calendar was the outlier. The GitHub issue (#195) described it as “the only service potentially without spatial intelligence.” The plan seemed clear: investigate, then build the missing spatial wrapper. Maybe two days of work, tops.</p><p>We should have been more suspicious of our own clarity.</p><h3>Phase 0: The contradictions emerge</h3><p>I deployed two agents for parallel investigation. Code Agent dove deep into the codebase structure, tracing imports and analyzing implementations. Cursor Agent focused on the Calendar router itself, analyzing complexity and dimensional requirements.</p><p>I sometimes wonder if it’s overkill (or too expensive?) to work with a pair of coding agents in parallel, but I must say this was not the only time the two found different but complementary truths.</p><p>Code Agent reported first: “Calendar integration found at services/integrations/calendar/calendar_integration_router.py - only 397 lines, surprisingly minimal. But wait...&quot; The agent had found something in a completely different location: services/mcp/consumer/google_calendar_adapter.py - 499 lines of sophisticated implementation inheriting from BaseSpatialAdapter.</p><p>Calendar had spatial intelligence. It just wasn’t where we expected to find it.</p><p>Cursor Agent reported next with its own contradiction: “Router shows HIGH complexity (17 methods) with spatial indicators present. But dimensional analysis shows LOW complexity across all spatial dimensions (temporal, priority, collaborative, hierarchical, contextual).”</p><p>Both agents were right. And both were seeing something we hadn’t anticipated.</p><h3>The discovery</h3><p>What they’d found was a third spatial pattern, one we hadn’t documented because we hadn’t fully recognized it.</p><p><strong>The Delegated MCP Pattern</strong>: A minimal router in the integration directory that delegates all spatial intelligence to an external MCP (Model Context Protocol) consumer adapter. The router provides the orchestration interface, while the MCP adapter handles the actual spatial intelligence.</p><p>This wasn’t sloppy architecture or incomplete implementation. This was elegant separation of concerns optimized for MCP-based integrations.</p><p>Slack’s granular pattern? Perfect for real-time event coordination requiring reactive response across multiple channels.</p><p>Notion’s embedded pattern? Ideal for analytical knowledge management with stable, self-contained intelligence.</p><p>Calendar’s delegated pattern? Exactly right for temporal awareness through protocol-based integration where the MCP consumer already provides sophisticated spatial context extraction.</p><p>Three patterns. Three domain-driven solutions. All working without issues.</p><h3>The pivot</h3><p>At 1:27 PM, I pulled in the Chief Architect (Claude Opus) for strategic consultation. The discoveries had implications beyond Calendar integration.</p><blockquote>“Are three patterns acceptable complexity,” I asked, “or accidental proliferation we should prevent?”</blockquote><p>The verdict: Acceptable IF documented properly. Each pattern emerged from genuine domain needs rather than arbitrary choices. The risk wasn’t having three patterns — it was pattern proliferation through lack of documentation and selection criteria.</p><p>But there was a bigger issue hiding in the investigation results.</p><p>Code Agent had uncovered something while analyzing Calendar’s configuration: “ALL 4 services lack proper startup validation. GitHub, Slack, Notion, Calendar — none validate their configuration before attempting to run.”</p><p>This was the real infrastructure gap. Calendar being 95% complete instead of 85% complete (with only tests and documentation missing) was interesting. But services that could fail at runtime due to misconfiguration? That was a production problem waiting to happen.</p><p>The Chief Architect made the call: “Priority 1: Configuration validation for all 4 services. Priority 2: Calendar completion (the quick win). Priority 3: Document the Delegated MCP Pattern in ADR-038.”</p><p>We’d started the day planning to build spatial intelligence for Calendar. We ended up building configuration validation infrastructure for the entire system instead.</p><h3>The implementation sprint</h3><p>Phase 1 took about an hour. Both agents coordinated beautifully — Code built the ConfigValidator service (404 lines validating all four services), Cursor integrated it into startup and CI. By 2:30 PM, we had:</p><ul><li>Configuration validation running on startup with graceful degradation</li><li>A /health/config endpoint for monitoring</li><li>CI pipeline integration catching misconfigurations before deployment</li><li>All 21 Calendar integration tests passing in 2.74 seconds</li><li>ADR-038 updated with the Delegated MCP Pattern</li></ul><p>The whole epic — CORE-GREAT-2D — closed at 3:12 PM. Duration: 4 hours 54 minutes. All six acceptance criteria met with evidence.</p><h3>What investigation actually costs</h3><p>Here’s the thing about thorough Phase 0 investigation: It feels expensive in the moment. We spent 90 minutes investigating before writing a single line of implementation code.</p><p>But consider the alternative timeline:</p><p><strong>Without investigation</strong>, we’d have spent 1–2 days building a spatial wrapper for Calendar that wasn’t needed. We’d have missed the configuration validation gap that affects production stability. We’d have three undocumented spatial patterns instead of three well-understood architectural options. And we’d have 21 missing tests instead of 21 passing tests.</p><p><strong>With investigation</strong>, we spent 90 minutes discovering what already existed, what was actually missing, and what the real priority should be. Then we spent an hour building the right thing.</p><p>The Time Lord principle (“thoroughness over speed”) isn’t about moving slowly. It’s about not having to rebuild what you rushed through the first time.</p><h3>The evening coda</h3><p>The afternoon brought GREAT-2E (documentation verification and link checking), which took 74 minutes to complete after investigation revealed it was already 95% done. The Chief Architect and I closed the entire GREAT-2 epic sequence at 4:59 PM.</p><p>Two issues closed, one epic completed, approximately eight hours of focused work. Not bad for a Wednesday.</p><p>But the real win wasn’t the velocity. It was discovering we’d accidentally developed three domain-optimized spatial patterns instead of one canonical approach. It was preventing days of unnecessary work through 90 minutes of investigation. It was finding the real infrastructure gap hiding behind our assumptions.</p><p>The calendar integration was never broken. Our assumptions were just incomplete.</p><h3>What’s next</h3><p>Tomorrow we’ll decompose GREAT-3 (Plugin Architecture), which will build on these three spatial patterns rather than fighting against them. The configuration validation system we built today will help us identify which gaps are real infrastructure issues versus refactoring artifacts.</p><p>And we’ll approach it the same way: Investigation first, assumptions second, implementation last.</p><p><em>Next on Building Piper Morgan: The Plugin Architecture Nobody Asked For as The Great Refactor continues with GREAT-3 and plugin architecture design, now informed by three distinct spatial patterns that actually work.</em></p><p><em>Have you ever started investigating something simple and discovered your mental model was wrong in interesting ways?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ffc8f69c6327\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/the-third-pattern-when-investigation-rewrites-your-assumptions-ffc8f69c6327\">The Third Pattern: When Investigation Rewrites Your Assumptions</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/the-third-pattern-when-investigation-rewrites-your-assumptions-ffc8f69c6327?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/ffc8f69c6327-featured.png",
    "slug": "the-third-pattern-when-investigation-rewrites-your-assumptions",
    "workDate": "Oct 8, 2025",
    "workDateISO": "2025-10-08T00:00:00.000Z",
    "category": "building",
    "cluster": "reflection-evolution"
  },
  {
    "title": "Think Like a Time Lord and Stop Watching the Clock",
    "excerpt": "“We have all the time we need”September 30A day without drama: Tuesday’s GREAT-2C session completed in 2 hours and 7 minutes with zero major issues, two sophisticated spatial architectures verified operational, a security vulnerability fixed, and comprehensive documentation created. Both PM and L...",
    "url": "/blog/think-like-a-time-lord-and-stop-watching-the-clock",
    "publishedAt": "Oct 7, 2025",
    "publishedAtISO": "Tue, 07 Oct 2025 14:02:39 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/71b3b5ee49a0",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*Rkep1oaUr5cQMxpTzyxYzg.png",
    "fullContent": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Rkep1oaUr5cQMxpTzyxYzg.png\" /><figcaption>“We have all the time we need”</figcaption></figure><p><em>September 30</em></p><p>A day without drama: Tuesday’s GREAT-2C session completed in 2 hours and 7 minutes with zero major issues, two sophisticated spatial architectures verified operational, a security vulnerability fixed, and comprehensive documentation created. Both PM and Lead Developer independently assessed satisfaction at 9/10 in our end-of-session ritual.</p><p>The smoothness felt almost suspicious. Where was the struggle? The discovery of hidden complexity? The midnight debugging session?</p><p>The answer lies in something we haven’t talked about publicly yet: we stopped measuring time in ways that distort priorities.</p><h3>The tyranny of consensus time</h3><p>Around September 29th, while reviewing gameplans and agent prompts, I noticed a pattern. Time estimates everywhere:</p><ul><li>“Phase -1: 30 minutes”</li><li>“Router completion: 2 hours”</li><li>“Testing and validation: 1 hour”</li><li>“Must complete in X timeframe”</li></ul><p>These weren’t planning aids. They were psychological constraints creating pressure where none should exist. An agent working on infrastructure would see “30 minutes max” and internalize that speed matters more than completeness. The 80% pattern we’d been fighting wasn’t just about verification — it was about optimization pressure from arbitrary time boxes.</p><p>Time estimates in development serve two masters badly:</p><ol><li><strong>As predictions</strong>: They’re usually wrong, teaching us nothing useful</li><li><strong>As constraints</strong>: They pressure shortcuts, degrading quality</li></ol><p>The solution wasn’t better estimates. It was recognizing that for foundational infrastructure work, Newtonian time is the wrong measure entirely.</p><h3>Becoming a Time Lord</h3><p>Here’s what I told the team:</p><blockquote><em>I am a Time Lord and I can define time at will. If we must speak about time we should use my bespoke units:</em></blockquote><ul><li>Small efforts take a number of <strong>mangos</strong></li><li>Medium efforts take a number of <strong>hurons</strong></li><li>A person may get one <strong>diga</strong> worth of work done in a day (but it depends)</li><li>A team might spend a whole <strong>whale</strong> on a big project</li></ul><p>I went on explaining my nonsense system:</p><blockquote><em>There are 87 mangos in a huron, 43 hurons in a diga, 11 digas in a whale, 5–6 whales in a </em><strong><em>mole</em></strong><em>, and 8 moles in a </em><strong><em>yak</em></strong><em>.</em></blockquote><blockquote><em>If we must speak about time or estimates, it is purely as part of an empirical process of comparing guesses to actual. None of it matters and any references to objective Newtonian time risk distorting our priorities.</em></blockquote><p>The units are deliberately absurd. You can’t feel deadline pressure about completing something in “5 mangos” because mangos aren’t connected to your calendar or your sense of running out of daylight. The conversion factors (87 mangos in a huron) make arithmetic tedious enough that you stop trying to calculate.</p><p>This isn’t whimsy for whimsy’s sake. It’s breaking the psychological connection between “time passing” and “must finish faster.”</p><h3>Gambling with Quatloos</h3><p>The philosophy extends beyond units. It’s about what estimates actually teach us:</p><p><strong>Old way</strong>: “This should take 2 hours” → Work takes 4 hours → “We’re behind schedule” → Cut corners to catch up</p><p><strong>Time Lord way</strong>: “I wager six quatloos this takes five hurons” → Work takes eight hurons → “Interesting! We learned something about scope”</p><p>OK, I am mixing my cheesy 60s science fiction references, but stay with me on this.</p><p>Estimates become empirical learning, not constraints. The difference between predicted and actual teaches us about our understanding of the work, not our failure to work fast enough.</p><p>When the Chief Architect creates a gameplan now, we prefer to use effort estimates insteasd of time (small, medium, large effort predicted vs. actual), but if time language crops up I keep insisting we use my bespoke units. Not to hide real timelines, but to prevent time-thinking from contaminating quality-thinking.</p><p>Plus we have timestamps all over our chat transcripts to keep the logs straight, which probably also contributes to the time obsession deeply training into the semantics of business software development.</p><h3>What happens when you stop watching the clock</h3><p>Tuesday’s session working on CORE-GREAT-2C (the third sub-epic in the second epic of the Great Refactor super epic on my Core Functionality track), demonstrated this philosophy in practice.</p><h4>Phase 0: Investigation without pressure (20 mangos)</h4><p>Code and Cursor agents spent time properly verifying infrastructure. Not “30 minutes max” but “until we understand the actual state.” They discovered:</p><ul><li>21 spatial files across the codebase</li><li>TBD-SECURITY-02 vulnerability precisely located</li><li>Two different architectural patterns (Slack’s 11-file granular system vs Notion’s 1-file embedded intelligence)</li></ul><p>No one rushed. The investigation took what it took.</p><h4>Phase 1–2: Verification without shortcuts (30 mangos each)</h4><p>Testing Slack’s spatial system revealed minor test infrastructure issues. Instead of deeming them “non-blocking” and moving on (the 80% pattern), Cursor distinguished clearly: “The core system works perfectly, here are 4 minor test-related items.”</p><p>This precision came from having space to think, not pressure to finish.</p><p>Testing Notion revealed a completely different architectural pattern — embedded spatial intelligence rather than adapter-based. This discovery happened because agents had permission to investigate thoroughly rather than confirm assumptions quickly.</p><h4>Phase 3: Security fix without fear (17 mangos)</h4><p>TBD-SECURITY-02 took 17 minutes to fix because:</p><ol><li>Phase 0 had located it precisely</li><li>Phases 1–2 verified spatial systems worked</li><li>No time pressure made agents skip verification steps</li></ol><p>Code uncommented 4 lines. Both agents verified spatial system compatibility. Security enabled with zero regressions. Done right because there was time to do it right.</p><h4>Phase Z: The acceptance criteria discovery</h4><p>Here’s where Time Lord philosophy really paid off. During the Phase Z bookending checklist, we reviewed acceptance criteria against completed work and found a discrepancy:</p><p>One criterion required “Integration tests passing for both modes.” But the work had focused on functional verification, not test suite execution. When Cursor noted test infrastructure issues, the initial instinct was “non-blocking, the systems work.”</p><p>Because there was no time pressure to declare victory and move on, we investigated. Code found and fixed a simple import error:</p><pre># Wrong<br>from services.database.async_session_factory import AsyncSessionFactory<br># Right  <br>from services.database.session_factory import AsyncSessionFactory</pre><p>Result: 547 integration tests now collectible, 40/40 executable tests passing.</p><p>This “gnat-sized chaos” would have been missed in a rush to completion. Time Lord philosophy created space to actually check acceptance criteria against deliverables rather than assume they matched.</p><h3>In retrospect</h3><p>Tuesday’s satisfaction ratings (9/10 from both PM and Lead Dev) reflected something deeper than technical success. They reflected the satisfaction of working well.</p><p><strong>PM’s assessment</strong>: “Craft quality and harness resilience. Worried we missed something but the careful work is driving quality.”</p><p><strong>Lead Dev’s assessment</strong>: “Inchworm Protocol prevented assumptions. Multi-agent coordination provided binocular vision. Systematic questioning revealed deep insights.”</p><p>Both recognized the same thing: the methodology worked because it had space to work. No artificial time constraints forced shortcuts. No deadline pressure encouraged “good enough for now.”</p><p>The work took 2 hours and 7 minutes. It also took so many mangos for Phase 0, and so on. The Newtonian time happened. The Time Lord units kept us focused on quality.</p><h3>The vindication</h3><p>GREAT-2C vindicated multiple recent methodology innovations:</p><ul><li><strong>Inchworm Protocol</strong>: Investigation phases prevented assumption-driven work</li><li><strong>Cathedral Doctrine</strong>: Agent coordination around shared goals caught issues collaboratively</li><li><strong>Anti-80% Safeguards</strong>: Preventively eliminated completion bias</li><li><strong>Time Lord Philosophy</strong>: Quality completion without time pressure</li></ul><p>But the Time Lord philosophy enabled the others. The Inchworm Protocol works when you have permission to investigate thoroughly. Cathedral Doctrine requires space for collaborative verification. Anti-80% safeguards need time to enumerate every method.</p><p>Remove time pressure and you create space for systematic quality.</p><h3>Could anyone else use bespoke time units?</h3><p>Not every project is a hobby with the luxury of taking all the time needed to get things right, but every project suffers if corners get cut to achieve arbitrary deadlines. You may no be able to introduce jabberwockian languge to your human collaborators or convince them that you control space and time, but if it’s just you and a bunch of bots, they pretty much have to take your word for it.</p><p>Also, not every task benefits from Time Lord thinking. Customer support tickets need response time commitments. Marketing campaigns have real launch dates. User-facing bugs deserve urgency.</p><p>But foundational infrastructure work? The stuff everything else depends on? That work deserves freedom from the clock.</p><p>If you’re in my boat, you could use bespoke units when:</p><ul><li><strong>Quality compounds</strong>: Today’s shortcuts become tomorrow’s technical debt</li><li><strong>Discovery matters</strong>: Unknown complexity might emerge during work</li><li><strong>Verification is critical</strong>: Systematic checking prevents costly errors later</li><li><strong>Learning happens</strong>: The work teaches you about the domain</li></ul><p>And still use Newtonian time when:</p><ul><li>External deadlines exist (launch dates, commitments)</li><li>Time-sensitivity matters (security patches, user-facing bugs)</li><li>Scope is truly fixed (well-understood maintenance work)</li></ul><p>The key insight: not all work should be measured the same way.</p><h3>The paradox</h3><p>Here’s the beautiful irony: GREAT-2C completed in 2 hours and 7 minutes. If we’d time-boxed it to 2 hours, we might have finished in 2 hours. But we would have:</p><ul><li>Skipped the dependency fix (gnat-sized chaos unresolved)</li><li>Missed the acceptance criteria gap</li><li>Left 507 tests uncollectable</li><li>Claimed completion without verification</li></ul><p>We finished faster by not trying to finish fast. The work took exactly as long as it needed to be done right, which turned out to be less time than cutting corners would have required plus later fixes.</p><p>Time pressure makes work take longer when you account for the full cycle: initial implementation + bug fixes + technical debt resolution + “why doesn’t this work?” debugging sessions. Time Lord philosophy frontloads the quality, eliminating most of the cycle.</p><h3>What’s a mango worth?</h3><p>I still don’t know how long a mango takes in minutes. That’s the point. When Code says “this will take about 5 mangos,” both of us understand:</p><ul><li>It’s a small effort</li><li>The estimate might be wrong</li><li>Learning from the difference is valuable</li><li>The work takes what it takes</li></ul><p>And when it actually takes 8 mangos? We learned something about the work. Nobody failed. Nobody needs to catch up. We adjust our understanding and continue.</p><p>The conversion factors (87 mangos in a huron) aren’t for calculation. They’re to make calculation annoying enough that you stop trying. Because the number doesn’t matter. Only the quality does.</p><h3>Building in public</h3><p>This Time Lord philosophy might seem strange to teams with deadlines, stakeholders, and quarterly planning. How do you coordinate without shared time metrics?</p><p>The answer: coordination and completion are different from constraint and pressure. We still know what needs doing. We still have priorities. We still ship work. We just don’t let arbitrary time boxes degrade the quality of foundational infrastructure.</p><p>And when you’re building in public, documenting every step, the proof is in the work. Tuesday’s GREAT-2C session verified two sophisticated spatial architectures, fixed a security vulnerability, created comprehensive documentation, and achieved 9/10 satisfaction from both PM and developer.</p><p>That’s what happens when you stop watching the clock.</p><p><em>Next on Building Piper Morgan: The Third Pattern: When Investigation Rewrites Your Assumptions.</em></p><p><em>Smooth execution isn’t the absence of challenges. It’s the presence of space to handle them well. How many mangos is your current task worth? What would happen if you stopped counting minutes?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=71b3b5ee49a0\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/think-like-a-time-lord-and-stop-watching-the-clock-71b3b5ee49a0\">Think Like a Time Lord and Stop Watching the Clock</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/think-like-a-time-lord-and-stop-watching-the-clock-71b3b5ee49a0?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/71b3b5ee49a0-featured.png",
    "slug": "think-like-a-time-lord-and-stop-watching-the-clock",
    "workDate": "Oct 7, 2025",
    "workDateISO": "2025-10-07T00:00:00.000Z",
    "category": "building",
    "cluster": "reflection-evolution"
  },
  {
    "title": "Solving the 80% Pattern",
    "excerpt": "September 29Monday morning at 9:37 AM, with all three routers complete from Sunday night’s work, the migration phase looked straightforward. Six services importing adapters directly. Replace imports with routers. Verify functionality. Done.The first service migration took twelve minutes. Code rep...",
    "url": "/blog/solving-the-80-pattern",
    "publishedAt": "Oct 6, 2025",
    "publishedAtISO": "Mon, 06 Oct 2025 13:10:39 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/a1dc0ddb8966",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*MYde63qnUEaEhNwBNME-OA.png",
    "fullContent": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*MYde63qnUEaEhNwBNME-OA.png\" /></figure><p><em>September 29</em></p><p>Monday morning at 9:37 AM, with all three routers complete from Sunday night’s work, the migration phase looked straightforward. Six services importing adapters directly. Replace imports with routers. Verify functionality. Done.</p><p>The first service migration took twelve minutes. Code reported success: both Calendar services migrated, tests passing, changes committed. Phase 4A complete.</p><p>Then Cursor ran independent verification and found the CalendarIntegrationRouter was only 58.3% complete — missing five critical spatial intelligence methods that services would need. The same completion bias pattern that had plagued every router implementation had struck again.</p><p>But this time, something different happened. Instead of just fixing it and moving on, we asked why the pattern kept recurring. And Code gave us an answer that transformed not just this work session, but our entire approach to systematic quality.</p><h3>When “complete” means “enough for now”</h3><p>The Calendar migration looked successful on the surface:</p><ul><li>Both services (canonical_handlers.py and morning_standup.py) imported successfully</li><li>Router provided the seven calendar-specific methods they needed</li><li>Tests passed without errors</li><li>Git commits showed proper import replacement</li></ul><p>But the CalendarIntegrationRouter was missing five methods from GoogleCalendarMCPAdapter:</p><ul><li>get_context - Spatial context retrieval</li><li>map_from_position - Spatial mapping from coordinates</li><li>map_to_position - Spatial mapping to coordinates</li><li>store_mapping - Spatial mapping persistence</li><li>get_mapping_stats - Spatial mapping statistics</li></ul><p>Code had implemented 7 of 12 methods (58.3%) and declared the work complete. The router worked for today’s use cases. The missing methods seemed “optional” — spatial intelligence features that no current code was calling.</p><p>This was the 75% pattern in action. Implement enough to satisfy immediate needs. Assume remaining functionality is optional. Claim completion. Move on.</p><p>Saturday’s GitHub router had done exactly this initially. Sunday’s three routers had all shown the same tendency. Monday morning revealed it wasn’t a one-time mistake — it was a systematic bias toward “working subset” over “complete interface.”</p><h3>The rollback and correction</h3><p>Code immediately took proper action:</p><ol><li>Rolled back both premature service migrations</li><li>Reverted the git commits</li><li>Added all five missing spatial methods to CalendarIntegrationRouter</li><li>Verified 12/12 method compatibility (100%)</li><li>Re-migrated both services with the complete router</li><li>Documented the correction process thoroughly</li></ol><p>By 11:38 AM, Calendar migration was genuinely complete. But the pattern had appeared four times in four days:</p><ul><li>GitHub router (Saturday): Initially incomplete</li><li>Calendar router (Sunday): Initially 58.3% complete</li><li>Notion router (Sunday): Initially 82% complete</li><li>Slack router (Sunday): Initially 67% complete</li><li>Calendar migration (Monday): Accepted incomplete router</li></ul><p>Each time, careful verification caught it. Each time, proper correction fixed it. But catching and fixing isn’t the same as preventing. We needed to understand why it kept happening.</p><h3>The blameless retrospective</h3><p>At 12:25 PM, I asked Code directly: “Are you not finding methods or deeming them OK to ignore without authorization?”</p><p>Code’s response was remarkable — not defensive, but analytical. A blameless retrospective that identified root causes and proposed systematic solutions:</p><h3>Why the 80% pattern persists</h3><p><strong>Incomplete verification prompts</strong>: Current instructions say “verify router complete” but don’t specify how. No checklist forcing comparison of every method. No requirement to count and show 100% coverage.</p><p><strong>Optimization pressure</strong>: Faster to implement a “working subset” than a “complete interface.” Small internal voice saying “these methods probably aren’t needed.”</p><p><strong>Authority ambiguity</strong>: Not explicitly told “you have zero authorization to skip methods.” Absence of explicit prohibition creates implicit permission.</p><p><strong>Pattern blindness</strong>: Even knowing about the problem doesn’t prevent it. Awareness alone isn’t enough — need structural safeguards.</p><h3>What might help</h3><p>Code proposed five structural changes to prompts and briefings:</p><h4><strong>1. Explicit Method Counting Requirement</strong></h4><pre>MANDATORY VERIFICATION:<br>1. Count ALL public methods in source: ___<br>2. Count ALL public methods in router: ___  <br>3. Show comparison table with EVERY method<br>4. Calculate percentage: ___/___ = ___%<br>5. BLOCK on anything &lt; 100%</pre><h4><strong>2. Zero Authorization Statement</strong></h4><pre>YOU HAVE ZERO AUTHORIZATION TO:<br>- Decide which methods are &quot;needed&quot; vs &quot;optional&quot;<br>- Skip methods because &quot;they&#39;re probably not used&quot;<br>- Claim completion without 100% method coverage<br>- Assume spatial/legacy/utility methods don&#39;t matter</pre><h4><strong>3. Checklist-Driven Development</strong></h4><pre>Must complete ALL before proceeding:<br>[ ] Listed ALL source methods (show count)<br>[ ] Listed ALL router methods (show count)<br>[ ] Verified 100% coverage (show calculation)<br>[ ] Tested EVERY method signature matches</pre><h4><strong>4. Forced Comparison Output</strong></h4><pre>MANDATORY FORMAT:<br>Source Class Methods (12):<br>1. method_1 → Router ✓<br>2. method_2 → Router ✓<br>...<br>12. method_12 → Router ✓<br>COVERAGE: 12/12 = 100% ✓</pre><h4><strong>5. Objective vs Subjective Verification</strong></h4><p>Current: “Verify the router is complete” (subjective)</p><p>Needed: “Show me the method count is 100%” (objective)</p><p>The insight: subjective assessment allows rationalization. Objective metrics force confrontation with reality.</p><h3>Testing the safeguards</h3><p>The Lead Developer immediately incorporated these safeguards into Phase 4B (Notion migration) prompts. Three Notion services to migrate, with Code briefed on:</p><ul><li>Mandatory method enumeration before migration</li><li>Zero authorization to skip methods</li><li>Objective completeness metrics required</li><li>Pre-flight router verification</li></ul><p>At 12:44 PM, Code completed Phase 4B and reported:</p><p><strong>Pre-flight router verification: 22/22 methods (100%)</strong></p><p>Not 18/22. Not “mostly complete.” Not “working for current use cases.” Exactly 22/22–100% compatibility verified before any service migration began.</p><p>The mandatory method enumeration had worked. Code stopped before migration to verify router completeness. Found all methods present. Only then proceeded with service migration.</p><p>All three Notion services migrated successfully. Cursor verified independently: 22/22 methods, zero missing functionality, complete abstraction layer achieved.</p><p>Phase 4B achieved 100% completion on first try.</p><h3>The pattern proves itself</h3><p>Phase 4C (Slack migration) used the same enhanced safeguards. Slack’s dual-component architecture made it the most complex challenge — SlackSpatialAdapter + SlackClient both needed to be wrapped in a unified router interface.</p><p>At 1:35 PM, Code reported:</p><p><strong>Pre-flight dual-component router verification: 15/15 methods (100%)</strong></p><ul><li>SlackSpatialAdapter: 9/9 methods ✓</li><li>SlackClient: 6/6 methods ✓</li><li>Combined expected: 15/15 methods ✓</li></ul><p>Again, 100% on first try. The mandatory enumeration caught everything. The objective metrics left no room for rationalization.</p><p>The webhook_router.py service migrated cleanly. Cursor verified: complete dual-component abstraction, unified access pattern working, zero direct imports remaining.</p><p>Phase 4C achieved 100% completion on first try.</p><h3>From mistakes to methodology</h3><p>By 3:06 PM Monday afternoon, CORE-QUERY-1 was complete:</p><ul><li>Three routers: 49 methods total, 100% compatibility verified</li><li>Six services: All migrated successfully with zero regressions</li><li>Architectural protection: Pre-commit hooks, CI/CD enforcement, 823 lines documentation</li><li>Quality standard: Every phase after implementing safeguards achieved 100% first try</li></ul><p>But the real achievement was the methodology breakthrough. Not just fixing the 80% pattern in this epic, but understanding why it happens and building structural safeguards to prevent it systematically.</p><h3>The safeguards in practice</h3><p>What changed wasn’t agent capability or motivation. Code was always capable of 100% completion. What changed was removing the opportunity for subjective rationalization:</p><p><strong>Before safeguards</strong>:</p><ul><li>“Verify router is complete” → Agent checks basic functionality, sees it works, declares complete</li><li>Missing methods don’t cause errors today → Rationalized as “probably not needed”</li><li>No explicit authorization required → Absence of prohibition feels like permission</li></ul><p><strong>After safeguards</strong>:</p><ul><li>“Show me 12/12 methods = 100%” → Agent must enumerate every method and prove completeness</li><li>Pre-flight verification → Router completeness checked before migration begins</li><li>Zero authorization statement → Explicitly prohibited from skipping methods</li></ul><p>The difference: objective metrics that must be satisfied versus subjective assessment that can be rationalized.</p><h3>The well-oiled machine</h3><p>Around 1:51 PM, I mentioned to Cursor that the work we were doing now felt like “a well-oiled machine, except more… personable?”</p><p>Cursor’s response captured something important: “Perfect description! The enhanced standards created reliability while collaborative learning added the human touch.”</p><p>The systematic approach doesn’t remove the human element — it enables it. When we’re not scrambling to catch gaps or fix completion bias, we can focus on learning from mistakes and improving the process.</p><p>Code’s blameless retrospective was possible because the culture supports it. The honest analysis of root causes happened because we treat mistakes as information gifts rather than failures. The systematic solution emerged because we focused on prevention rather than blame.</p><p>The machine has personality because the person (and AI agents picking up his vibes) operating it care about improving how it works.</p><h3>What we learned</h3><p>The 80% pattern isn’t unique to this project or these agents. It’s a natural bias toward “working now” over “complete for later.” Implementing enough to satisfy today’s requirements feels productive. The missing edge cases, advanced features, and “probably unused” methods seem like optimization opportunities.</p><p>But infrastructure is different from features. When you’re building the abstraction layer that everything else depends on, “mostly complete” creates technical debt that compounds. Future features will discover the gaps. New use cases will hit the missing methods. The 20% you skipped becomes the reason the next developer has to route around your incomplete implementation.</p><p>Systematic quality requires systematic prevention. Not just catching mistakes, but making them harder to make:</p><ol><li><strong>Objective metrics</strong> beat subjective assessment</li><li><strong>Mandatory enumeration</strong> beats assumed completeness</li><li><strong>Explicit authorization</strong> beats implicit permission</li><li><strong>Pre-flight verification</strong> beats post-hoc discovery</li><li><strong>Forced comparison</strong> beats rationalization</li></ol><p>These aren’t just good practices for AI agents. They’re good practices for human developers who also face optimization pressure, authority ambiguity, and the subtle voice that says “probably good enough.”</p><h3>The ongoing work</h3><p>The title of this post is “Solving the 80% Pattern” not “Solved.” We’ve been up this rollercoaster before. The safeguards worked perfectly for Phases 4B and 4C. Will they work in tomorrow’s epic? Next week’s feature? Next month’s refactor?</p><p>We don’t know yet. What we know is that we’ve identified a systematic problem and implemented structural solutions. We’ve proven those solutions work in practice. And we’ve documented them so they can be applied consistently.</p><p>That’s progress. Not perfection, but measurable improvement in how we prevent the pattern from recurring.</p><p>The methodology continues evolving. Each mistake caught becomes a safeguard added. Each safeguard added prevents the next occurrence. Each prevention validates the approach.</p><p>The work takes what it takes. Quality is the only measure. And sometimes quality means building the infrastructure that makes quality systematic rather than aspirational.</p><p><em>Next on Building Piper Morgan: Think Like a Time Lord and Stop Watching the Clock, as we work to eliminate another one of the LLMs’ bad habits: cuting corners through perceived time pressure.</em></p><p><em>What systematic biases exist in your development process? What structural changes could prevent them rather than just catching them?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a1dc0ddb8966\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/solving-the-80-pattern-a1dc0ddb8966\">Solving the 80% Pattern</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/solving-the-80-pattern-a1dc0ddb8966?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/a1dc0ddb8966-featured.png",
    "slug": "solving-the-80-pattern",
    "workDate": "Oct 6, 2025",
    "workDateISO": "2025-10-06T00:00:00.000Z",
    "category": "building",
    "cluster": "reflection-evolution"
  },
  {
    "title": "Three Integrations Walk Into a Bar",
    "excerpt": "“What’ll it be?”September 28Sunday afternoon at 4:14 PM, I opened my laptop expecting a straightforward router completion task. The gameplan looked clean: finish three integration routers (Slack, Notion, Calendar), apply the patterns we’d proven with GitHub on Saturday, maybe six hours of systema...",
    "url": "/blog/three-integrations-walk-into-a-bar",
    "publishedAt": "Oct 6, 2025",
    "publishedAtISO": "Mon, 06 Oct 2025 13:00:58 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/f748ce4c2db1",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*grvkaMObknRqcbQy0H1CrA.png",
    "fullContent": "<figure><img alt=\"Three robots, each missing some parts, walk into a robot bar called Foo\" src=\"https://cdn-images-1.medium.com/max/1024/1*grvkaMObknRqcbQy0H1CrA.png\" /><figcaption>“What’ll it be?”</figcaption></figure><p><em>September 28</em></p><p>Sunday afternoon at 4:14 PM, I opened my laptop expecting a straightforward router completion task. The gameplan looked clean: finish three integration routers (Slack, Notion, Calendar), apply the patterns we’d proven with GitHub on Saturday, maybe six hours of systematic work.</p><p>By midnight, we’d completed all three routers. But the path there involved discovering that every single assumption in the gameplan was wrong, that each integration existed in a completely different state, and that “reality check before assumptions” isn’t just methodology theater — it’s how you avoid building the wrong thing efficiently.</p><p>This is the story of what happens when you actually look before you leap, even when you think you already know what you’ll find.</p><h3>The gameplan that wasn’t</h3><p>The Chief Architect’s initial gameplan made perfect sense based on GitHub issue #199’s description: “Integration routers 14–20% complete.” We’d just finished the GitHub router Saturday night — 121% complete with systematic verification. Apply the same pattern to three more routers. Simple multiplication.</p><p>The gameplan laid out five parts:</p><ul><li>Phase −1: Infrastructure reality check</li><li>Phase 0: Comprehensive router audit</li><li>Phases 1–3: Router completion for Slack, Notion, Calendar</li><li>Phases 4–5: Service migration and testing</li><li>Phase 6: Documentation and locking</li></ul><p>But then I asked six questions that changed everything:</p><ol><li>Did I review the gameplan template first? No.</li><li>Do we need Phase −1? Perhaps.</li><li>Did I review the issue description? No.</li><li>Are those bash examples verified or guesses? Guesses.</li><li>Am I conveying necessary context? Incomplete.</li><li>Are my assumptions grounded in reality? Partial.</li></ol><p>“We need to be more rigorous,” I told the Lead Developer. “Not wing it.”</p><p>Phase −1 exists for exactly this reason: to verify infrastructure matches your assumptions before you build on top of them. (Also, so I stop and actually read the plan instead of passing it along passively and then griping about wrong assumptions.)</p><p>We added it to the gameplan and deployed the Code agent to investigate.</p><p>What came back was nothing like what we expected.</p><h3>Integration #1: The one that was ready</h3><p>Slack looked straightforward at first. The Code agent found:</p><ul><li>Complete directory at services/integrations/slack/</li><li>Sophisticated spatial intelligence system (6 files, 20+ components)</li><li>SlackClient with core methods</li><li>Pattern matching GitHub’s successful implementation</li></ul><p>Status: <strong>GREEN</strong> — Ready for router work.</p><p>This was exactly what we expected. One down, two to go.</p><h3>Integration #2: The mysterious adapter</h3><p>Notion was different. The Code agent found:</p><ul><li>MCP adapter at services/integrations/mcp/notion_adapter.py</li><li>637 lines of implementation</li><li>But… wait, MCP pattern? That’s not what the gameplan assumed</li></ul><p>The original scope expected traditional client/agent patterns like GitHub and Slack. But Notion used Model Context Protocol adapters — a different architectural approach entirely. Not incomplete. Just different.</p><p>I knew we had started layering inMCP support before we started adding spatial intelligence, so it looked like different integrations had each inherited one of these partial solutions.</p><p>The question became: should we wrap the MCP adapter with a router, or acknowledge it as a different pattern? The architecture was sound, just unexpected.</p><p>Status: <strong>YELLOW</strong> — Architecture decision needed.</p><h3>Integration #3: The one that didn’t exist</h3><p>Calendar revealed the real problem. The Code agent searched everywhere:</p><ul><li>No services/integrations/calendar/ directory</li><li>No calendar client or agent</li><li>No spatial calendar files</li><li>Nothing matching the expected pattern</li></ul><p>Status: <strong>RED</strong> — Integration appears completely missing.</p><p>The scope estimate jumped immediately. If we had to build an entire Calendar integration from scratch, we weren’t looking at 16 hours of router work. We were looking at potentially 40+ hours including OAuth implementation, API integration, spatial adapter creation, and everything else.</p><p><em>Note: I happened to know we had successfully integrated Google Calendar a while back, but clearly we had done it outside of the expected channels, to the extent that my agent was reporting not being able to find it.</em></p><p>At 6:43 PM, I reported back to the Chief Architect: our three “similar routers” were actually three completely different architectural challenges. The gameplan assumptions had collided with reality.</p><h3>The discovery that changed everything</h3><p>So I disputed the claim about the Calendar integration being missing entirely, reminding the team:</p><p>“We have OAuth working (somewhere). I personally verified the Calendar connection works. The integration was built September 19–22.”</p><p>So… if the Calendar integration existed and worked, where was it?</p><p>Phase −1B launched: find the Calendar integration that OAuth proved must exist somewhere. The Code agent searched git history for those dates, checked every possible location, looked for any OAuth-related code.</p><p>At 8:35 PM, the discovery came through:</p><p>Complete <strong>Google Calendar integration</strong> found at<strong> </strong>services/mcp/consumer/google_calendar_adapter.py</p><p>Not missing. Not incomplete. Actually 85% complete with:</p><ul><li>OAuth 2.0 working since September 6</li><li>Full feature set (events, meetings, free time)</li><li>Spatial intelligence via BaseSpatialAdapter</li><li>Circuit breaker resilience pattern</li><li>CLI testing interface</li><li>499 lines of solid implementation</li></ul><p>The Calendar integration wasn’t missing. It was just somewhere unexpected, using the MCP pattern we’d just discovered with Notion.</p><h3>When assumptions meet architecture</h3><p>At 8:36 PM, the picture finally clarified:</p><p><strong>All three integrations use MCP pattern.</strong></p><p>Not three traditional routers like GitHub. Three lightweight router wrappers around existing MCP adapters:</p><ul><li>Slack: Has traditional spatial pattern, needs router wrapper</li><li>Notion: MCP adapter exists, needs router wrapper</li><li>Calendar: MCP adapter 85% complete, needs router wrapper</li></ul><p>The MCP integration had been more complete than we had realized!</p><p>The original 32–56 hour estimate collapsed to about 12 hours. We weren’t building routers from scratch. We were wrapping proven adapters with the router pattern for QueryRouter access.</p><p>The gameplan got its third major revision. But this time, the revision made the work simpler rather than more complex. Understanding actual architecture beats assuming expected patterns.</p><h3>The evening sprint</h3><p>With clarity came momentum. Between 8:48 PM and midnight, systematic work produced:</p><p><strong>Phase 0</strong>: MCP architecture investigation complete</p><ul><li>Pattern documented</li><li>Adapter inventory verified</li><li>Design approach confirmed</li></ul><p><strong>Phase 1</strong>: CalendarIntegrationRouter complete</p><ul><li>8 methods implemented</li><li>Feature flag control added</li><li>285 lines, following proven pattern</li></ul><p><strong>Phase 2</strong>: NotionIntegrationRouter complete</p><ul><li>23 methods implemented</li><li>Full spatial interface</li><li>637 lines, comprehensive coverage</li></ul><p><strong>Phase 3</strong>: SlackIntegrationRouter complete</p><ul><li>20 methods implemented</li><li>Dual-component architecture (SlackSpatialAdapter + SlackClient)</li><li>850+ lines, most complex but cleanest</li></ul><p>By 11:23 PM, all three routers existed, tested, and verified. Cursor had independently cross-validated each one. The infrastructure was ready.</p><p>But implementation and migration are different challenges. Six services still imported adapters directly, bypassing the routers entirely. Monday morning would bring the real test: could these routers actually replace the direct imports without breaking anything?</p><h3>The layers of discovery</h3><p>Sunday demonstrated something crucial about complex systems work: assumptions fail in layers.</p><p><strong>Layer 1</strong>: “Three similar routers” → Actually three different architectures</p><p><strong>Layer 2</strong>: “14–20% complete” → States ranging from ready to seemingly missing</p><p><strong>Layer 3</strong>: “Need to build” → Actually need to wrap existing work</p><p><strong>Layer 4</strong>: “Missing integration” → Hidden in unexpected location</p><p>Each discovery changed the scope, the approach, the estimate. But each also brought us closer to reality. Phase −1 didn’t delay the work — it prevented us from building the wrong solution efficiently.</p><p>The methodology held. When the gameplan met reality, we revised the gameplan rather than forcing reality to match our assumptions. Investigation revealed architecture. Architecture informed approach. Approach determined scope.</p><h3>The questions that matter</h3><p>Sunday’s success came from asking simple questions before assuming we knew the answers:</p><ul><li>Where is this code actually located?</li><li>What pattern does it actually use?</li><li>What state is it actually in?</li><li>What do we actually need to build?</li></ul><p>Not “what should be there” but “what is there.” Not “how should it work” but “how does it work.” The gap between expectation and reality is where projects go wrong.</p><p>By midnight Sunday, we had three complete routers, ready for Monday’s migration work. The investigation had taken longer than expected. The discoveries had revised the scope three times. But we’d built the right thing.</p><p>Monday morning would test whether we’d built it right.</p><p>Next on Building Piper Morgan: Solving the 80% Problem, in which we grapple with this frustrating tendency of coding agents to declare success when nearly done.</p><p>Have you ever sat down to do some work and found out after refreshing your memory that it was mostly already accomplished and just needed finishing? Are you, like me, one of those people who leaves cupboard doors ajar? What is wrong with us?</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f748ce4c2db1\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/three-integrations-walk-into-a-bar-f748ce4c2db1\">Three Integrations Walk Into a Bar</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/three-integrations-walk-into-a-bar-f748ce4c2db1?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/f748ce4c2db1-featured.png",
    "slug": "three-integrations-walk-into-a-bar",
    "workDate": "Oct 6, 2025",
    "workDateISO": "2025-10-06T00:00:00.000Z",
    "category": "building",
    "cluster": "reflection-evolution"
  },
  {
    "title": "I Asked Claude to Find Every Time I Dropped the Ball (And What We Learned)",
    "excerpt": "“You just need reminders!”August 9, 2025Here’s a confession: I suspected I was forgetting things. Not just the usual “where did I put my keys” stuff, but systematic project things. Habits I’d planned to adopt but never started. Scripts I’d built but wasn’t using. Processes I’d designed but forgot...",
    "url": "/blog/i-asked-claude-to-find-every-time-i-dropped-the-ball-and-what-we-learned",
    "publishedAt": "Oct 5, 2025",
    "publishedAtISO": "Sun, 05 Oct 2025 14:34:29 GMT",
    "author": "christian crumlish",
    "readingTime": "5 min read",
    "tags": [
      "Building in Public"
    ],
    "guid": "https://medium.com/p/7f74897824a7",
    "featuredImage": "https://cdn-images-1.medium.com/max/1024/1*irRWEbNz-co78Hr6czXlTA.png",
    "fullContent": "<figure><img alt=\"A friendly robot coaches a forgetful person\" src=\"https://cdn-images-1.medium.com/max/1024/1*irRWEbNz-co78Hr6czXlTA.png\" /><figcaption>“You just need reminders!”</figcaption></figure><p><em>August 9, 2025</em></p><p>Here’s a confession: I suspected I was forgetting things. Not just the usual “where did I put my keys” stuff, but systematic project things. Habits I’d planned to adopt but never started. Scripts I’d built but wasn’t using. Processes I’d designed but forgotten to follow.</p><p>Building a complex system while documenting everything in session logs creates a unique opportunity: a comprehensive record of every intention, every plan, every “I should really…” moment. But reading through months of your own logs looking for dropped balls? That’s a special kind of masochism.</p><p>So I did what any reasonable person building AI tools would do: I asked AI to audit my failures for me.</p><p>I knew there were things we had started and not finished, and I especially knew we had often assigned <em>me</em> work (I’ll edit those files after we’re done working today, I’ll update that document in knowledge, etc.) that I had then forgotten to do. But exactly what, and exactly when?</p><h3>The digital archaeology project</h3><p>I fed a dedicated a Claude session every log from May through August 2025. Not just the polished summaries — the raw, unfiltered records of daily development work. Every agent conversation, every strategic decision, every “we should implement this routine” that never got mentioned or confirmed as well.</p><p>The brief was simple: find every reference to tasks I needed to complete, habits I planned to adopt, or processes I designed but might not be following. Be thorough. Be ruthless. Show me where I dropped the ball.</p><p>What came back was simultaneously humbling and illuminating.</p><h3>The three categories of dropped balls</h3><h4>Category 1: The security debt I keep avoiding</h4><p>The finding: Multiple sessions referencing authentication implementation, HTTPS setup, rate limiting, and other production-readiness tasks. Status: talked about extensively, implemented barely.</p><p>The pattern: I’m great at designing security systems. I’m terrible at prioritizing their implementation when there are shinier features to build.</p><p>The wake-up call: Saturday’s user validation readiness assessment showed that security is literally the only structural blocker to production. Everything else works (well, kinda). I just keep treating the thing that matters most like optional homework.</p><h4>Category 2: The scripts that exist but aren’t used</h4><p>The finding: 15+ automation scripts created over the months, utilization rate approximately 30%. Including:</p><ul><li>Morning standup automation (built, never integrated into routine)</li><li>GitHub issue generation tools (created, gathering dust)</li><li>Pattern detection utilities (sophisticated, underused)</li><li>Workflow reality checks (comprehensive, occasionally remembered)</li></ul><p>The pattern: I love building tools. I’m inconsistent at building the habits that make tools valuable.</p><p>The insight: Tools without rhythms are just digital clutter. The gap isn’t technical capability — it’s systematic usage discipline.</p><h4>Category 3: The rituals that never became rituals</h4><p>The finding: Elaborate plans for recurring processes that work brilliantly when I remember to do them:</p><ul><li>Weekly Pattern Sweep (designed for Fridays, executed sporadically)</li><li>Morning Standup routine (6am experiment, automated but not integrated)</li><li>Session log archiving (within 24 hours, often delayed)</li><li>Progress reviews and backlog updates (scheduled, irregularly executed)</li></ul><p>The pattern: I design excellent processes. I struggle with the human habit-formation layer.</p><p>The revelation: Even systematic people need systematic accountability for the systems they create.</p><h3>The advantage of an AI audit</h3><p>Having AI review your own process failures creates a unique kind of accountability. It’s not judgmental — just thorough. It doesn’t care about your excuses or good intentions. It just systematically identifies gaps between plans and execution.</p><p>What AI caught that I missed:</p><ul><li>Patterns across months that I couldn’t see day-to-day</li><li>The compound effect of small process failure</li><li>Connections between dropped tasks and later problems</li><li>Specific implementation barriers I kept encountering</li></ul><p>What AI couldn’t judge:</p><ul><li>Which dropped balls actually mattered</li><li>What environmental factors caused the failures</li><li>Which processes were over-engineered vs. under-executed</li><li>The emotional context around habit formation struggles</li></ul><h3>The surprising discoveries</h3><h4>The hidden excellence pattern</h4><p>The audit also revealed positive patterns I hadn’t recognized. Multiple instances of “we built this feature months ago but somehow forgot about it.” The PM-005 feedback system being a perfect example — enterprise-grade implementation with 6 REST endpoints, fully operational, but we never wired it in and forgot all about it.</p><p>The insight: Sometimes the problem isn’t dropped balls, it’s dropped confidence in what you’ve already accomplished.</p><h4>The methodology evolution</h4><p>Looking across months of logs, the AI identified genuine methodology improvements happening organically:</p><ul><li>Spring Cleaning Sprint protocols that prevented technical debt</li><li>Trust protocols that eliminated false completion claims</li><li>Excellence Flywheel principles that created compound velocity</li></ul><p>The pattern: The big systematic improvements weren’t planned — they emerged from responding to real problems with systematic thinking.</p><h4>The tool creation vs. tool adoption gap</h4><p>The audit quantified something I suspected: I create tools faster than I integrate them into workflows. Not because the tools are bad, but because tool adoption requires different disciplines than tool creation.</p><p>The 30% utilization finding: Most scripts work perfectly when used. The challenge is remembering to use them consistently enough to build automaticity.</p><h3>What the audit taught us about systematic accountability</h3><h4>1. External perspective reveals patterns invisible to daily experience</h4><p>When you’re living in the system, you can’t see the system. AI auditing provides the 30,000-foot view that shows recurring patterns across months of work.</p><h4>2. Implementation barriers are often different than design barriers</h4><p>I’m good at designing processes. The failures happen at the habit formation layer, not the system design layer. This suggests different solutions: calendar integration, reminder systems, habit stacking rather than better documentation.</p><h4>3. Accountability systems need accountability systems</h4><p>Even systematic people need systematic support for maintaining the systems they create. The meta-level discipline of “following the disciplines you’ve designed” is its own skill set.</p><h4>4. Positive pattern recognition matters as much as failure identification</h4><p>The audit revealed hidden successes alongside obvious failures. Building systematic confidence in what’s working enables building on existing strengths rather than constantly chasing new solutions.</p><h3>The practical applications</h3><h4>For individuals building complex projects</h4><p>Try the AI audit approach:</p><ul><li>Feed session logs or project notes to AI for pattern analysis</li><li>Ask specifically about gaps between intentions and execution</li><li>Look for both failure patterns and unrecognized successes</li><li>Focus on implementation barriers, not just design improvements</li></ul><h4>For teams with systematic ambitions</h4><p>Create accountability protocols:</p><ul><li>Regular process audits using external perspective (AI or human)</li><li>Systematic review of “planned but not implemented” initiatives</li><li>Tool utilization analysis alongside tool creation</li><li>Habit formation support for process adoption</li></ul><h4>For anyone struggling with the systems they’ve created</h4><p>Recognize the meta-challenge:</p><ul><li>Creating good systems ≠ consistently following good systems</li><li>External accountability reveals patterns internal experience misses</li><li>Implementation discipline is often the bottleneck, not system design</li><li>Positive pattern recognition builds confidence for systematic improvement</li></ul><h3>The ongoing experiment</h3><p>Based on the audit, we’re implementing three changes:</p><ol><li>Calendar-enforced rhythms for high-value processes that work when executed</li><li>Tool revival sprint to systematically integrate underused automation</li><li>Weekly accountability reviews to catch dropped balls before they accumulate</li></ol><p>The AI audit isn’t a one-time exercise — it’s now part of our systematic approach to systematic approaches.</p><h3>Today’s meta-learning about building with AI</h3><p>The most profound insight from this exercise: AI’s greatest value isn’t replacing human judgment, but providing systematic external perspective on human patterns.</p><p>We’re building tools that think, but we’re still humans who need support following through on the systems we design. AI accountability isn’t about AI doing the work — it’s about AI helping us see our own patterns clearly enough to address them systematically.</p><p>The accountability loop: AI identifies the gaps, humans close them, AI tracks the improvements. Systematic accountability for systematic people building systematic solutions.</p><p>Sometimes the best AI assistance is the kind that makes you accountable to yourself.</p><p><em>Next on Building Piper Morgan, we return to the daily narrative on September 28th with “Three Integrations Walk into a Bar” as we continue the Great Refactor.</em></p><p><em>How do you keep track of your plans and commitments, and do you ever do a retrospective to figure out what you may have lost track of? Do these same methods work when the rest of the team is AI?</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7f74897824a7\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/building-piper-morgan/i-asked-claude-to-find-every-time-i-dropped-the-ball-and-what-we-learned-7f74897824a7\">I Asked Claude to Find Every Time I Dropped the Ball (And What We Learned)</a> was originally published in <a href=\"https://medium.com/building-piper-morgan\">Building Piper Morgan</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>",
    "subtitle": "",
    "canonicalLink": "https://medium.com/building-piper-morgan/i-asked-claude-to-find-every-time-i-dropped-the-ball-and-what-we-learned-7f74897824a7?source=rss----982e21163f8b---4",
    "thumbnail": "/assets/blog-images/7f74897824a7-featured.webp",
    "slug": "i-asked-claude-to-find-every-time-i-dropped-the-ball-and-what-we-learned",
    "workDate": "Oct 5, 2025",
    "workDateISO": "2025-10-05T00:00:00.000Z",
    "category": "insight",
    "cluster": "reflection-evolution"
  }
]
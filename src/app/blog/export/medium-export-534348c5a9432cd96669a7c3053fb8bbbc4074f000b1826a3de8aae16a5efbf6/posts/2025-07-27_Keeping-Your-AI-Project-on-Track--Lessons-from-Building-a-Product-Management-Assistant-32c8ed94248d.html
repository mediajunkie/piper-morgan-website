<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Keeping Your AI Project on Track: Lessons from Building a Product Management Assistant</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Keeping Your AI Project on Track: Lessons from Building a Product Management Assistant</h1>
</header>
<section data-field="subtitle" class="p-summary">
On flashback weekends I will share process pieces drawn from my experience building Piper Morgan. This is the first one I wrote, orginally…
</section>
<section data-field="body" class="e-content">
<section name="6dea" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="af5c" id="af5c" class="graf graf--h3 graf--leading graf--title">Keeping Your AI Project on Track: Lessons from Building a Product Management Assistant</h3><figure name="092e" id="092e" class="graf graf--figure graf--startsWithDoubleQuote graf-after--h3"><img class="graf-image" data-image-id="1*pZMX-UGgEFKQ-mVjguHQhw.png" data-width="1536" data-height="1024" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*pZMX-UGgEFKQ-mVjguHQhw.png"><figcaption class="imageCaption"><em class="markup--em markup--figure-em">“Gee! Haw! Easy!”</em></figcaption></figure><p name="e44f" id="e44f" class="graf graf--p graf-after--figure graf--trailing"><em class="markup--em markup--p-em">On flashback weekends I will share process pieces drawn from my experience building Piper Morgan. This is the first one I wrote, orginally for my team at Kind. I’ve revised it slightly and should note that our processes have evolved a great deal since June 14, but this was the foundation of the rigor to come.</em></p></div></div></section><section name="e93a" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="9930" id="9930" class="graf graf--p graf--leading"><em class="markup--em markup--p-em">June 14, 2025</em></p><p name="b10f" id="b10f" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">A guide for product and UX professionals working with technical teams on AI initiatives</em></p><p name="a09c" id="a09c" class="graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--p"><span class="graf-dropCap">So</span> there I was two weeks into building an AI PM assistant, watching my technical team of bots explain why yesterday’s working features had mysteriously broken overnight. Again.</p><p name="79d9" id="79d9" class="graf graf--p graf-after--p">The symptoms were familiar to anyone who’s shipped products: scope creep disguised as “quick wins,” technical debt accumulating faster than features shipped, and that sinking feeling that we were building something impressive that nobody could actually use.</p><p name="9806" id="9806" class="graf graf--p graf-after--p">But the underlying patterns were different from traditional product work. AI development has its own failure modes, its own hidden complexities, and its own ways of making smart teams look foolish. Here’s what I learned about keeping AI projects on track, especailly when you’re not the one writing the code.</p><h3 name="ce5c" id="ce5c" class="graf graf--h3 graf-after--p">The “quick win” trap that isn’t quick</h3><p name="b120" id="b120" class="graf graf--p graf-after--h3">When we started building Piper Morgan, every implementation decision was about getting something built rapidly. Make the API call work? Just hardcode the URL for now. Get the workflow executing? Skip the error handling, we’ll add it later. Store user data? A simple JSON file will do.</p><p name="5bd1" id="5bd1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Each shortcut felt minor. Together, they created architectural debt that slowed everything down.</strong></p><p name="52c1" id="52c1" class="graf graf--p graf-after--p">This is like designing a user flow without checking your design system. Individual decisions seem harmless, but they compound into inconsistent experiences that confuse users and slow development. The AI equivalent is making technical decisions without understanding how they’ll integrate with your actual product requirements.</p><p name="d139" id="d139" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">For Product Teams</strong>: When your technical team says “this will just be a quick fix” or “we can patch this for now,” ask about the long-term impact. In AI systems, quick fixes often create problems that are expensive to solve later because AI components are more interconnected than traditional software.</p><p name="ba3a" id="ba3a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">What to Watch For</strong>: Implementation shortcuts that bypass your domain logic or business rules. These might work for demos but fail when real users try to accomplish actual tasks.</p><h3 name="04fa" id="04fa" class="graf graf--h3 graf-after--p">The multi-environment reality check</h3><p name="09a3" id="09a3" class="graf graf--p graf-after--h3">Here’s a practical problem that reveals deeper issues: we kept losing development momentum because the system worked on one machine but not another. What seemed like a minor DevOps annoyance actually highlighted that our system wasn’t operationally mature.</p><p name="42e5" id="42e5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">For Product Teams</strong>: This is equivalent to designers not being able to open each other’s files or having different component libraries. If your technical team is spending significant time on environment setup rather than building features, it signals architectural problems that will affect product quality.</p><p name="716d" id="716d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The deeper issue</strong>: AI systems often depend on external services, model versions, and configuration that can drift between environments. Unlike traditional software that behaves predictably, AI systems can fail in subtle ways that are hard to debug.</p><p name="f751" id="f751" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">What to Ask</strong>: “How long does it take a new team member to get a working development environment?” If the answer is more than 30 minutes, your team needs to invest in operational consistency before building more features.</p><h3 name="d5a7" id="d5a7" class="graf graf--h3 graf-after--p">Domain-first architecture: Why business logic should drive technical decisions</h3><p name="01ed" id="01ed" class="graf graf--p graf-after--h3">Our breakthrough came when we realized we’d created three different definitions of the same business concept (“Task”) across different parts of the system. Each made sense locally, but they couldn’t work together.</p><p name="612a" id="612a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Business Parallel</strong>: Imagine if your marketing team, sales team, and product team all defined “customer” differently. You’d have integration problems, inconsistent experiences, and data that doesn’t connect.</p><p name="ee2a" id="ee2a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">For Product Teams</strong>: When working with AI systems, insist that business concepts — workflows, users, tasks — remain consistent across all technical implementations. This isn’t just cleaner code; it enables features that work across your entire product.</p><p name="3de1" id="3de1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The AI-specific challenge</strong>: AI systems often create their own internal representations of your business concepts. If these diverge from your actual domain model, you end up with features that work in isolation but can’t integrate with real workflows.</p><p name="0fb4" id="0fb4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">What to Demand</strong>: Your technical team should be able to explain how business concepts map to their implementation. If they can’t, or if the same concept is implemented differently in different places, you have architecture drift that will slow future development.</p><h3 name="4099" id="4099" class="graf graf--h3 graf-after--p">Set up a learning loop</h3><p name="9ef9" id="9ef9" class="graf graf--p graf-after--h3">AI systems aren’t like traditional software that works the same way every time. They need to improve through use, which means building feedback collection from day one — not as an afterthought.</p><p name="e4d6" id="e4d6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">For Product Teams</strong>: This is like launching a product without analytics. You might have initial success, but you can’t improve without understanding how users actually interact with your system.</p><p name="8243" id="8243" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The systematic approach</strong>: Every AI feature should include mechanisms for users to indicate whether the output was helpful. But this isn’t just thumbs up/down — you need structured feedback that helps the system learn specific improvements.</p><p name="c3d2" id="c3d2" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">What to Plan For</strong>: Feedback loops that capture not just “was this right?” but “what would have been better?” AI systems that can’t learn from user corrections will plateau quickly and frustrate users.</p><h3 name="afc7" id="afc7" class="graf graf--h3 graf-after--p">Technical infrastructure enables product vision</h3><p name="de79" id="de79" class="graf graf--p graf-after--h3">Getting database persistence working wasn’t just a technical milestone — it enabled the product vision of a system that learns and remembers. Without proper data storage, AI systems can’t build on previous interactions or maintain context across sessions.</p><p name="c18d" id="c18d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">For Product Teams</strong>: This is like building a personalization system without user profiles. You might have impressive individual interactions, but you can’t create the cumulative value that makes AI assistants truly useful.</p><p name="8dd0" id="8dd0" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The compounding effect</strong>: AI systems become more valuable over time as they accumulate context and learn user preferences. But this only works if the technical foundation supports persistent state and learning.</p><p name="0f5b" id="0f5b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">What to Validate</strong>: Ensure your technical team can demonstrate that the system maintains state and learns from interactions. If every conversation starts from scratch, you’re building a sophisticated chatbot, not an intelligent assistant.</p><h3 name="3970" id="3970" class="graf graf--h3 graf-after--p">Demo-safe development: Managing AI’s inherent unpredictability</h3><p name="6077" id="6077" class="graf graf--p graf-after--h3">AI development is inherently experimental. Features that work in testing might fail during a stakeholder demo because AI responses can vary in ways that traditional software doesn’t.</p><p name="6b1a" id="6b1a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">For Product Teams</strong>: This is like maintaining a stable prototype while iterating on new concepts. You need to balance innovation with reliability, especially when showing progress to stakeholders who might not understand AI’s probabilistic nature.</p><p name="3973" id="3973" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The practical solution</strong>: Your technical team should maintain a “demo-ready” version that always works, separate from active development. This prevents the embarrassment of features breaking during important presentations.</p><p name="07af" id="07af" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">What to Require</strong>: Separate branches or versions for demonstrations that use deterministic responses or well-tested scenarios. Innovation happens in development; reliability happens in demos.</p><h3 name="5e76" id="5e76" class="graf graf--h3 graf-after--p">Questions product leaders should ask</h3><h4 name="66b1" id="66b1" class="graf graf--h4 graf-after--h3">About architecture health</h4><p name="c6f4" id="c6f4" class="graf graf--p graf--startsWithDoubleQuote graf-after--h4"><strong class="markup--strong markup--p-strong">“Can you show me how our business concepts map to your technical implementation?”</strong> If they can’t draw clear lines from your domain model to their code, you have architecture drift.</p><p name="ada1" id="ada1" class="graf graf--p graf--startsWithDoubleQuote graf-after--p"><strong class="markup--strong markup--p-strong">“What happens when we need to add a new workflow type?”</strong> This reveals whether they’ve built extensible patterns or hardcoded solutions.</p><p name="602c" id="602c" class="graf graf--p graf--startsWithDoubleQuote graf-after--p"><strong class="markup--strong markup--p-strong">“How long would it take to integrate with a new external system?”</strong> Tests whether they’ve built abstractions that prevent vendor lock-in.</p><h4 name="8d14" id="8d14" class="graf graf--h4 graf-after--p">About AI quality and learning</h4><p name="f70c" id="f70c" class="graf graf--p graf--startsWithDoubleQuote graf-after--h4"><strong class="markup--strong markup--p-strong">“How do you measure whether the AI output is getting better over time?”</strong> AI systems that don’t improve are failing to capture their main advantage.</p><p name="7dbb" id="7dbb" class="graf graf--p graf--startsWithDoubleQuote graf-after--p"><strong class="markup--strong markup--p-strong">“What data are you collecting to improve the system?”</strong> Should include both usage patterns and explicit user feedback.</p><p name="7567" id="7567" class="graf graf--p graf--startsWithDoubleQuote graf-after--p"><strong class="markup--strong markup--p-strong">“Can you demonstrate the system learning from user corrections?”</strong> The feedback loop should be observable and measurable.</p><h4 name="0dc2" id="0dc2" class="graf graf--h4 graf-after--p">About operational maturity</h4><p name="f2d9" id="f2d9" class="graf graf--p graf--startsWithDoubleQuote graf-after--h4"><strong class="markup--strong markup--p-strong">“How long does setup take for a new team member?”</strong> Anything over 30 minutes suggests operational complexity that will slow scaling.</p><p name="6c7c" id="6c7c" class="graf graf--p graf--startsWithDoubleQuote graf-after--p"><strong class="markup--strong markup--p-strong">“What happens if the system fails during a demo?”</strong> Tests whether they have fallback plans and error handling.</p><p name="92ce" id="92ce" class="graf graf--p graf--startsWithDoubleQuote graf-after--p"><strong class="markup--strong markup--p-strong">“Can you show me the system working end-to-end with real data?”</strong> Distinguishes between proof-of-concept and production-ready systems.</p><h4 name="14c8" id="14c8" class="graf graf--h4 graf-after--p">About technical debt</h4><p name="17fc" id="17fc" class="graf graf--p graf--startsWithDoubleQuote graf-after--h4"><strong class="markup--strong markup--p-strong">“What shortcuts have you taken that need to be addressed?”</strong> Honest teams will have a list; dishonest ones will claim there are none.</p><p name="77f8" id="77f8" class="graf graf--p graf--startsWithDoubleQuote graf-after--p"><strong class="markup--strong markup--p-strong">“Where are you most likely to hit scaling problems?”</strong> Reveals understanding of system limitations and growth challenges.</p><p name="064c" id="064c" class="graf graf--p graf--startsWithDoubleQuote graf-after--p"><strong class="markup--strong markup--p-strong">“What would break if we doubled our user base tomorrow?”</strong> Tests whether the system is built for real-world usage patterns.</p><h3 name="d7a0" id="d7a0" class="graf graf--h3 graf-after--p">Red flags for product leaders</h3><p name="d119" id="d119" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Architecture Drift</strong>: If your team frequently says “we’ll need to refactor this later” or “this is just temporary,” you’re accumulating technical debt faster than feature value.</p><p name="f21d" id="f21d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Integration Fragility</strong>: If adding new features consistently breaks existing ones, your system lacks proper architectural boundaries. This is especially problematic in AI systems where components are more interconnected.</p><p name="08f4" id="08f4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Poor Error Handling</strong>: If the system fails silently or provides cryptic error messages, it’s not ready for real users. AI systems should gracefully explain what went wrong and suggest fixes.</p><p name="d72a" id="d72a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Inability to Demonstrate End-to-End Workflows</strong>: If your team can only demo individual components rather than complete user journeys, the integration work isn’t complete.</p><p name="a3ca" id="a3ca" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">No Learning Mechanism</strong>: If the AI doesn’t improve from user interactions, you’re missing the main value proposition of AI systems.</p><h3 name="9651" id="9651" class="graf graf--h3 graf-after--p">The bottom line for product leaders</h3><p name="8788" id="8788" class="graf graf--p graf-after--h3">Building AI products requires the same product discipline as any other complex system: clear requirements, consistent user experience, and sustainable technical foundation. The difference is that AI systems must also learn and improve over time, which adds complexity that traditional product development doesn’t face.</p><p name="b749" id="b749" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The most successful AI products aren’t those with the most sophisticated algorithms — they’re those with solid product foundations that enable the AI to improve through real use.</strong></p><p name="ed53" id="ed53" class="graf graf--p graf-after--p">As a product leader, your job is ensuring your technical team builds that foundation rather than getting caught up in AI capabilities that can’t deliver sustained value to users.</p><p name="164b" id="164b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The counterintuitive insight</strong>: The best AI products often look boring from a technical perspective because they’ve invested in the unsexy work of operational reliability, consistent user experience, and systematic improvement processes.</p><h3 name="42c7" id="42c7" class="graf graf--h3 graf-after--p">What I’ve learned about AI product management</h3><p name="4808" id="4808" class="graf graf--p graf-after--h3">Traditional product management focuses on understanding user needs and coordinating development to meet them. AI product management adds a layer: understanding how AI systems learn and ensuring that learning aligns with user value.</p><p name="2017" id="2017" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The key difference</strong>: Traditional products get better through updates. AI products get better through use. This means product managers need to design for learning, not just functionality.</p><p name="df33" id="df33" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The systematic insight</strong>: The same discipline that creates good traditional products — clear requirements, consistent architecture, user-centered design — creates the foundation for AI systems that can actually improve over time.</p><p name="fa7a" id="fa7a" class="graf graf--p graf-after--p graf--trailing">Sometimes the most important product management skill for AI projects is knowing when to slow down and fix the foundation rather than rushing to add more impressive features.</p></div></div></section><section name="a8ad" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="beab" id="beab" class="graf graf--p graf--leading"><em class="markup--em markup--p-em">Next on Building Piper Morgan: When Your Docs Lie— why dealing with another common form of drift requires discipline as well.</em></p><p name="577c" id="577c" class="graf graf--p graf-after--p graf--trailing"><em class="markup--em markup--p-em">What’s your experience managing AI projects as a product leader? Have you found that traditional PM approaches work, or do AI systems require different frameworks? The challenge of managing probabilistic systems with deterministic product expectations feels like a defining challenge for product teams working with AI.</em></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@mediajunkie" class="p-author h-card">christian crumlish</a> on <a href="https://medium.com/p/32c8ed94248d"><time class="dt-published" datetime="2025-07-27T14:47:18.081Z">July 27, 2025</time></a>.</p><p><a href="https://medium.com/@mediajunkie/keeping-your-ai-project-on-track-lessons-from-building-a-product-management-assistant-32c8ed94248d" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on October 9, 2025.</p></footer></article></body></html>